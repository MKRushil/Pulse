# -*- coding: utf-8 -*-
"""
SCBR LLM è©•åˆ†ç³»çµ± (LLM-based Evaluator)
===========================================
åŠŸèƒ½ï¼š
1. è®€å–å¯¦é©—ç”¢ç”Ÿçš„ CSVã€‚
2. èª¿ç”¨å¤–éƒ¨ LLM (å¦‚ GPT-4, Claude, æˆ–æ‚¨çš„ Llama 3) é€²è¡Œè©•åˆ†ã€‚
3. è©•åˆ†æ¨™æº–ï¼š0.0 (å®Œå…¨éŒ¯èª¤) ~ 1.0 (å®Œå…¨æ­£ç¢º)ã€‚
"""

import pandas as pd
import requests
import json
import time
from concurrent.futures import ThreadPoolExecutor

# ==================== LLM é…ç½® (è«‹å¡«å…¥æ‚¨çš„è³‡è¨Š) ====================
LLM_CONFIG = {
    "api_url": "https://integrate.api.nvidia.com/v1/chat/completions", # æ‚¨çš„ API URL
    "api_key": "nvapi-xxxx", # æ‚¨çš„ Key
    "model_name": "meta/llama-3.3-70b-instruct" # æ‚¨çš„ Model Name
}

INPUT_CSV = "experiment_results_Agentic_v5.csv"
OUTPUT_CSV = "experiment_results_Agentic_Scored.csv"

def get_llm_score(pred, expected):
    """
    å‘¼å« LLM é€²è¡Œè©•åˆ†
    """
    if pd.isna(pred) or not pred: return 0.0
    
    prompt = f"""
    ä½ æ˜¯ä¸­é†«è¨ºæ–·è©•ä¼°å°ˆå®¶ã€‚è«‹è©•ä¼°ä»¥ä¸‹å…©å€‹è¨ºæ–·çµæœçš„èªæ„ç›¸ä¼¼åº¦ã€‚
    
    æ¨™æº–è¨ºæ–·: "{expected}"
    æ¨¡å‹é æ¸¬: "{pred}"
    
    è«‹çµ¦å‡ºä¸€å€‹ 0.0 åˆ° 1.0 ä¹‹é–“çš„åˆ†æ•¸ï¼š
    - 1.0: å®Œå…¨ä¸€è‡´æˆ–åŒç¾©è© (å¦‚ ä¸å¯=å¤±çœ , è‚é¬±æ°£æ»¯=è‚æ°£é¬±çµ)
    - 0.8: é«˜åº¦ç›¸ä¼¼ï¼Œæ ¸å¿ƒè­‰å‹æ­£ç¢ºä½†æœ‰ç´°å¾®å·®ç•° (å¦‚ å¿ƒè„¾å…©è™› vs å¿ƒè„¾ä¸è¶³)
    - 0.5: éƒ¨åˆ†æ­£ç¢ºï¼Œå‘½ä¸­éƒ¨åˆ†é—œéµå­— (å¦‚ è…é™°è™› vs è…è™›)
    - 0.0: å®Œå…¨éŒ¯èª¤æˆ–ä¸ç›¸é—œ
    
    è«‹åªè¼¸å‡ºåˆ†æ•¸æ•¸å­—ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
    """
    
    payload = {
        "model": LLM_CONFIG["model_name"],
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,
        "max_tokens": 10
    }
    
    try:
        resp = requests.post(
            LLM_CONFIG["api_url"], 
            json=payload, 
            headers={"Authorization": f"Bearer {LLM_CONFIG['api_key']}"},
            timeout=10
        )
        content = resp.json()['choices'][0]['message']['content'].strip()
        return float(content)
    except Exception as e:
        print(f"âš ï¸ LLM è©•åˆ†å¤±æ•—: {e}")
        return 0.0

def process_row(row):
    # å¦‚æœæœ‰ Errorï¼Œç›´æ¥çµ¦ 0 åˆ†
    if pd.notna(row.get('Error')) and row['Error']:
        return 0.0
        
    return get_llm_score(row['PredPattern'], row['Expected'])

def main():
    print(f"ğŸ“– è®€å–æª”æ¡ˆ: {INPUT_CSV} ...")
    df = pd.read_csv(INPUT_CSV)
    
    print("ğŸš€ é–‹å§‹ LLM è©•åˆ† (é€™å¯èƒ½éœ€è¦ä¸€é»æ™‚é–“)...")
    
    # ä½¿ç”¨å¤šåŸ·è¡Œç·’åŠ é€Ÿè©•åˆ†
    with ThreadPoolExecutor(max_workers=5) as executor:
        scores = list(executor.map(process_row, [row for _, row in df.iterrows()]))
    
    df['LLM_Score'] = scores
    
    avg_score = df['LLM_Score'].mean()
    print("-" * 40)
    print(f"ğŸ“Š å¹³å‡èªæ„æº–ç¢ºç‡ (LLM Score): {avg_score:.2%}")
    print("-" * 40)
    
    df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    print(f"âœ… è©•åˆ†å®Œæˆï¼Œå·²å„²å­˜è‡³: {OUTPUT_CSV}")

if __name__ == "__main__":
    main()