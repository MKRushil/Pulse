import os
import requests

# åŸºæœ¬è¨­å®š
api_url: str = os.getenv("LLM_API_URL", "https://integrate.api.nvidia.com/v1")
api_key: str = os.getenv("LLM_API_KEY", "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")
model: str = os.getenv("LLM_MODEL", "meta/llama-3.3-70b-instruct")

# å°è©±å‡½å¼
def chat_with_llm(messages):
    url = f"{api_url}/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 512,
    }

    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        raise Exception(f"Error {response.status_code}: {response.text}")

# æ¸¬è©¦å°è©±
if __name__ == "__main__":
    conversation = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹ä¸­é†«å¸«ï¼Œæœƒæ ¹æ“šç—‡ç‹€çµ¦äºˆä¸­é†«åªè¨ºæ–·çµæœèˆ‡ç”Ÿæ´»å»ºè­°ã€‚"},
        {"role": "user", "content": "æˆ‘æœ€è¿‘æ™šä¸Šå¤±çœ ï¼Œè€Œä¸”å¸¸å¸¸å¤šå¤¢ï¼Œæ‡‰è©²æ€éº¼è¾¦ï¼Ÿ  è«‹åƒè€ƒç›¸ä¼¼æ¡ˆä¾‹:ç—‡ç‹€è¡¨ç¾ï¼šå¤±çœ å¤šå¤¢ è¿‘5é€±å…¥ç¡å›°é›£ä¸”å¤šå¤¢æ˜“é†’ï¼Œé†’å¾Œé›£å†å…¥ç¡ï¼Œç™½å¤©å€¦æ€ ä¹åŠ›ï¼Œå¶æœ‰å¥å¿˜èˆ‡é£Ÿæ…¾å·®ã€‚ï¼› è„ˆè±¡ï¼šå·¦å¯¸:ä¸­/é² | å·¦é—œ:ç„¡åŠ›/è»Ÿ | å·¦å°º:ç„¡åŠ›/æµ® | å³å¯¸:ä¸­/æ•¸ | å³é—œ:ç„¡åŠ›/é² | å³å°º:æœ‰åŠ›/æ•¸ï¼› è¼”åŠ©æ¢æ–‡ï¼šé«˜ç†±ã€èƒ¸æ‚¶ã€è¡€ç˜€ç´«æ–‘ã€å¦‚èƒƒç—™æ”£ã€å¿ƒæ‚¸æ°£ä¿ƒã€æ€¥æ€§ç‚ç—‡ç–¼ç—›ã€æ€¥æ€§èƒ¸ç—›"},
    ]
    reply = chat_with_llm(conversation)
    print("ğŸ¤– AI å›è¦†ï¼š", reply)
