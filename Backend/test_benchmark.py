# Backend/test_benchmark.py
"""
æ€§èƒ½èˆ‡æº–ç¢ºæ€§åŸºæº–æ¸¬è©¦
"""

import asyncio
import time
import statistics
from typing import List, Dict
from pathlib import Path
import json

from s_cbr.main import SCBREngine

class BenchmarkTester:
    """åŸºæº–æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.engine = SCBREngine()
        self.results = {
            "performance": [],
            "convergence": [],
            "accuracy": []
        }
    
    # ==================== æ¸¬è©¦æ¡ˆä¾‹åº« ====================
    TEST_CASES = [
        {
            "id": "case_001",
            "rounds": [
                "å¤±çœ å¤šå¤¢ï¼Œå¿ƒæ‚¸å¥å¿˜",
                "èˆŒå°–ç´…ï¼Œå°‘è‹”",
                "å£ä¹¾ï¼Œäº”å¿ƒç…©ç†±"
            ],
            "expected_pattern": "å¿ƒè…ä¸äº¤",
            "expected_rounds": 2
        },
        {
            "id": "case_002",
            "rounds": [
                "å’³å—½ï¼Œç—°ç™½ç¨€è–„",
                "æƒ¡å¯’ï¼Œç„¡æ±—",
                "è„ˆæµ®ç·Š"
            ],
            "expected_pattern": "é¢¨å¯’æŸè‚º",
            "expected_rounds": 2
        },
        {
            "id": "case_003",
            "rounds": [
                "è…¹è„¹ï¼Œé£Ÿæ…¾ä¸æŒ¯",
                "å¤§ä¾¿æºè–„",
                "ç–²å€¦ä¹åŠ›ï¼Œé¢è‰²èé»ƒ"
            ],
            "expected_pattern": "è„¾è™›",
            "expected_rounds": 2
        },
        {
            "id": "case_004",
            "rounds": [
                "é ­ç—›é ­æšˆ",
                "æ€¥èºæ˜“æ€’",
                "è„…ç—›ï¼Œå£è‹¦ï¼Œè„ˆå¼¦"
            ],
            "expected_pattern": "è‚é™½ä¸Šäº¢",
            "expected_rounds": 2
        },
        {
            "id": "case_005",
            "rounds": [
                "è…°è†ç— è»Ÿï¼Œè€³é³´",
                "äº”å¿ƒç…©ç†±ï¼Œç›œæ±—",
                "èˆŒç´…å°‘è‹”"
            ],
            "expected_pattern": "è…é™°è™›",
            "expected_rounds": 2
        },
    ]
    
    async def run_all_benchmarks(self):
        """åŸ·è¡Œæ‰€æœ‰åŸºæº–æ¸¬è©¦"""
        print("\n" + "=" * 80)
        print("ğŸ“Š S-CBR æ€§èƒ½åŸºæº–æ¸¬è©¦")
        print("=" * 80 + "\n")
        
        # 1. æ€§èƒ½æ¸¬è©¦
        await self.test_performance()
        
        # 2. æ”¶æ–‚æ€§æ¸¬è©¦
        await self.test_convergence()
        
        # 3. æº–ç¢ºæ€§æ¸¬è©¦
        await self.test_accuracy()
        
        # 4. ç”Ÿæˆå ±å‘Š
        self.generate_report()
    
    async def test_performance(self):
        """æ€§èƒ½æ¸¬è©¦ï¼šéŸ¿æ‡‰æ™‚é–“ã€ååé‡"""
        print("\n" + "â”€" * 80)
        print("â±ï¸  æ€§èƒ½æ¸¬è©¦")
        print("â”€" * 80)
        
        response_times = []
        
        for i in range(10):
            start = time.time()
            await self.engine.diagnose(f"æ¸¬è©¦ç—‡ç‹€ {i}: å¤±çœ å¿ƒæ‚¸")
            elapsed = time.time() - start
            response_times.append(elapsed)
            print(f"   ç¬¬ {i+1} æ¬¡: {elapsed:.2f}s")
        
        avg_time = statistics.mean(response_times)
        std_time = statistics.stdev(response_times)
        
        print(f"\n   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_time:.2f}s")
        print(f"   æ¨™æº–å·®: {std_time:.2f}s")
        print(f"   æœ€å¿«: {min(response_times):.2f}s")
        print(f"   æœ€æ…¢: {max(response_times):.2f}s")
        
        self.results["performance"] = {
            "avg_time": avg_time,
            "std_time": std_time,
            "min_time": min(response_times),
            "max_time": max(response_times),
            "samples": len(response_times)
        }
        
        # è©•ä¼°
        if avg_time < 5.0:
            print(f"   âœ… æ€§èƒ½è©•ç´š: å„ªç§€ (< 5s)")
        elif avg_time < 10.0:
            print(f"   âœ… æ€§èƒ½è©•ç´š: è‰¯å¥½ (< 10s)")
        else:
            print(f"   âš ï¸  æ€§èƒ½è©•ç´š: éœ€å„ªåŒ– (> 10s)")
    
    async def test_convergence(self):
        """æ”¶æ–‚æ€§æ¸¬è©¦ï¼šæ”¶æ–‚é€Ÿåº¦ã€ç©©å®šæ€§"""
        print("\n" + "â”€" * 80)
        print("ğŸ“ˆ æ”¶æ–‚æ€§æ¸¬è©¦")
        print("â”€" * 80)
        
        convergence_data = []
        
        for case in self.TEST_CASES[:3]:  # æ¸¬è©¦å‰3å€‹æ¡ˆä¾‹
            print(f"\n   æ¸¬è©¦æ¡ˆä¾‹: {case['id']}")
            
            session_id = None
            round_cis = []
            
            for round_num, question in enumerate(case['rounds'], 1):
                if round_num == 1:
                    result = await self.engine.diagnose(question)
                    session_id = result['session_id']
                else:
                    result = await self.engine.diagnose(
                        question,
                        session_id=session_id,
                        continue_spiral=True
                    )
                
                ci = result['convergence_metrics']['Final']
                round_cis.append(ci)
                print(f"      Round {round_num}: CI = {ci:.3f}")
            
            # è¨ˆç®—æ”¶æ–‚é€Ÿåº¦
            if len(round_cis) >= 2:
                convergence_speed = round_cis[-1] - round_cis[0]
                print(f"      æ”¶æ–‚é€Ÿåº¦: {convergence_speed:+.3f}")
                
                convergence_data.append({
                    "case_id": case['id'],
                    "rounds": len(round_cis),
                    "final_ci": round_cis[-1],
                    "speed": convergence_speed,
                    "trajectory": round_cis
                })
        
        # çµ±è¨ˆ
        avg_final_ci = statistics.mean([c['final_ci'] for c in convergence_data])
        avg_speed = statistics.mean([c['speed'] for c in convergence_data])
        
        print(f"\n   å¹³å‡æœ€çµ‚ CI: {avg_final_ci:.3f}")
        print(f"   å¹³å‡æ”¶æ–‚é€Ÿåº¦: {avg_speed:+.3f}")
        
        self.results["convergence"] = {
            "avg_final_ci": avg_final_ci,
            "avg_speed": avg_speed,
            "cases": convergence_data
        }
        
        # è©•ä¼°
        if avg_final_ci >= 0.85:
            print(f"   âœ… æ”¶æ–‚è©•ç´š: å„ªç§€ (â‰¥ 0.85)")
        elif avg_final_ci >= 0.75:
            print(f"   âœ… æ”¶æ–‚è©•ç´š: è‰¯å¥½ (â‰¥ 0.75)")
        else:
            print(f"   âš ï¸  æ”¶æ–‚è©•ç´š: éœ€å„ªåŒ– (< 0.75)")
    
    async def test_accuracy(self):
        """æº–ç¢ºæ€§æ¸¬è©¦ï¼šè¨ºæ–·æº–ç¢ºç‡"""
        print("\n" + "â”€" * 80)
        print("ğŸ¯ æº–ç¢ºæ€§æ¸¬è©¦")
        print("â”€" * 80)
        
        correct = 0
        total = len(self.TEST_CASES)
        
        for case in self.TEST_CASES:
            print(f"\n   æ¸¬è©¦æ¡ˆä¾‹: {case['id']}")
            print(f"   é æœŸè¨ºæ–·: {case['expected_pattern']}")
            
            session_id = None
            
            for round_num, question in enumerate(case['rounds'], 1):
                if round_num == 1:
                    result = await self.engine.diagnose(question)
                    session_id = result['session_id']
                else:
                    result = await self.engine.diagnose(
                        question,
                        session_id=session_id,
                        continue_spiral=True
                    )
            
            # æœ€çµ‚è¨ºæ–·
            final_diagnosis = result['primary'].get('diagnosis', '')
            print(f"   å¯¦éš›è¨ºæ–·: {final_diagnosis}")
            
            # åˆ¤æ–·æ˜¯å¦æ­£ç¢ºï¼ˆåŒ…å«é æœŸè­‰å‹ï¼‰
            if case['expected_pattern'] in final_diagnosis:
                print(f"   âœ… æ­£ç¢º")
                correct += 1
            else:
                print(f"   âŒ ä¸ç¬¦")
        
        accuracy = correct / total * 100
        
        print(f"\n   æº–ç¢ºç‡: {accuracy:.1f}% ({correct}/{total})")
        
        self.results["accuracy"] = {
            "correct": correct,
            "total": total,
            "accuracy": accuracy
        }
        
        # è©•ä¼°
        if accuracy >= 80:
            print(f"   âœ… æº–ç¢ºæ€§è©•ç´š: å„ªç§€ (â‰¥ 80%)")
        elif accuracy >= 70:
            print(f"   âœ… æº–ç¢ºæ€§è©•ç´š: è‰¯å¥½ (â‰¥ 70%)")
        else:
            print(f"   âš ï¸  æº–ç¢ºæ€§è©•ç´š: éœ€å„ªåŒ– (< 70%)")
    
    def generate_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ åŸºæº–æ¸¬è©¦å ±å‘Š")
        print("=" * 80)
        
        # æ€§èƒ½
        perf = self.results["performance"]
        print(f"\nã€æ€§èƒ½æŒ‡æ¨™ã€‘")
        print(f"  å¹³å‡éŸ¿æ‡‰æ™‚é–“: {perf['avg_time']:.2f}s")
        print(f"  æ¨™æº–å·®: {perf['std_time']:.2f}s")
        
        # æ”¶æ–‚æ€§
        conv = self.results["convergence"]
        print(f"\nã€æ”¶æ–‚æ€§æŒ‡æ¨™ã€‘")
        print(f"  å¹³å‡æœ€çµ‚ CI: {conv['avg_final_ci']:.3f}")
        print(f"  å¹³å‡æ”¶æ–‚é€Ÿåº¦: {conv['avg_speed']:+.3f}")
        
        # æº–ç¢ºæ€§
        acc = self.results["accuracy"]
        print(f"\nã€æº–ç¢ºæ€§æŒ‡æ¨™ã€‘")
        print(f"  è¨ºæ–·æº–ç¢ºç‡: {acc['accuracy']:.1f}%")
        print(f"  æ­£ç¢º/ç¸½æ•¸: {acc['correct']}/{acc['total']}")
        
        # ç¸½è©•
        print(f"\nã€ç¸½é«”è©•ä¼°ã€‘")
        
        scores = []
        if perf['avg_time'] < 5.0:
            scores.append(("æ€§èƒ½", 90))
        elif perf['avg_time'] < 10.0:
            scores.append(("æ€§èƒ½", 75))
        else:
            scores.append(("æ€§èƒ½", 60))
        
        if conv['avg_final_ci'] >= 0.85:
            scores.append(("æ”¶æ–‚æ€§", 90))
        elif conv['avg_final_ci'] >= 0.75:
            scores.append(("æ”¶æ–‚æ€§", 75))
        else:
            scores.append(("æ”¶æ–‚æ€§", 60))
        
        if acc['accuracy'] >= 80:
            scores.append(("æº–ç¢ºæ€§", 90))
        elif acc['accuracy'] >= 70:
            scores.append(("æº–ç¢ºæ€§", 75))
        else:
            scores.append(("æº–ç¢ºæ€§", 60))
        
        overall = sum(s[1] for s in scores) / len(scores)
        
        for name, score in scores:
            print(f"  {name}: {score}/100")
        
        print(f"\n  ç¶œåˆè©•åˆ†: {overall:.1f}/100")
        
        if overall >= 85:
            print(f"  ğŸ† è©•ç´š: å„ªç§€ - å¯é€²å…¥ç ”ç©¶éšæ®µ")
        elif overall >= 75:
            print(f"  âœ… è©•ç´š: è‰¯å¥½ - å»ºè­°å„ªåŒ–å¾Œé€²å…¥ç ”ç©¶")
        else:
            print(f"  âš ï¸  è©•ç´š: éœ€æ”¹é€² - å»ºè­°å…ˆå„ªåŒ–ç³»çµ±")
        
        # å„²å­˜å ±å‘Š
        report_path = Path("benchmark_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\n  å ±å‘Šå·²å„²å­˜: {report_path}")

async def main():
    """ä¸»æ¸¬è©¦å…¥å£"""
    tester = BenchmarkTester()
    await tester.run_all_benchmarks()

if __name__ == "__main__":
    asyncio.run(main())