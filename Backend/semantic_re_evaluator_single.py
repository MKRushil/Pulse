# -*- coding: utf-8 -*-
"""
SCBR èªæ„é‡è©•ä¼°å·¥å…· v2.1 (å®Œæ•´è©åº«çœŸå€¼æ¯”å°ç‰ˆ)
===========================================
åŠŸèƒ½ï¼š
1. è®€å–å¯¦é©—çµæœ CSV (experiment_results_Agentic_static.csv)ã€‚
2. è®€å–è€ƒå· YAML (thesis_100_cases.yaml) ç²å–æ¨™æº–ç­”æ¡ˆã€‚
3. âœ… æ•´åˆã€Œå¤§å¹…æ“´å……ç‰ˆä¸­é†«åŒç¾©è©åº«ã€ï¼Œè§£æ±ºå­—é¢ä¸åŒ¹é…å•é¡Œã€‚
4. è¼¸å‡ºæœ€æº–ç¢ºçš„èªæ„æº–ç¢ºç‡ã€‚
"""

import pandas as pd
import yaml
import re
import os

# ================= é…ç½®å€åŸŸ =================
INPUT_CSV = "experiment_results_Agentic_static.csv"
YAML_FILE = "thesis_100_cases.yaml"
OUTPUT_CSV = INPUT_CSV.replace(".csv", "_Semantic_Verified.csv")

# ğŸ§  [æ ¸å¿ƒçŸ¥è­˜åº«] ä¸­é†«åŒç¾©è©åº« (æ‚¨çš„å®Œæ•´æ“´å……ç‰ˆ)
SYNONYMS = {
    # --- ç¡çœ èˆ‡ç²¾ç¥é¡ ---
    "ä¸å¯": "å¤±çœ ",
    "ä¸å¾—çœ ": "å¤±çœ ",
    "ç›®ä¸ç‘": "å¤±çœ ",
    "å¤šå¤¢": "å¤±çœ ",  # å»£ç¾©ä¸Šå¸¸åˆä½µè¨è«–
    "å¥å¿˜": "è¨˜æ†¶åŠ›æ¸›é€€",
    "é¬±è­‰": "æ†‚é¬±",
    "è‡Ÿèº": "ç„¦æ…®",
    "ç…©èº": "å¿ƒç…©",

    # --- ç–¼ç—›é¡ ---
    "è…°ç— ": "è…°ç—›",
    "è…°è†ç— è»Ÿ": "è…°ç—›",
    "è…°è„Šç—›": "è…°ç—›",
    "èƒƒè„˜ç—›": "èƒƒç—›",
    "å¿ƒä¸‹ç—›": "èƒƒç—›",
    "èƒƒè„˜ç¼ç—›": "èƒƒç¼ç†±",
    "å˜ˆé›œ": "èƒƒä¸é©",
    "èƒ¸ç—º": "èƒ¸æ‚¶",
    "èƒ¸æ»¿": "èƒ¸æ‚¶",
    "é ­é¢¨": "é ­ç—›",
    "é¦–é¢¨": "é ­ç—›",
    "è…¦é¢¨": "é ­ç—›",
    "åé ­ç—›": "é ­ç—›",
    "ç—¹è­‰": "é—œç¯€ç—›",
    "æ­·ç¯€": "é—œç¯€ç—›",
    "é¶´è†é¢¨": "è†é—œç¯€ç—›",
    "é …å¼·": "é ¸æ¤ç—…",

    # --- æ¶ˆåŒ–é¡ ---
    "æ³„ç€‰": "è…¹ç€‰",
    "ä¸‹åˆ©": "è…¹ç€‰",
    "é¶©æº": "è…¹ç€‰",
    "ä¾¿æº": "è…¹ç€‰",
    "ä¾¿ç¥•": "ä¾¿ç§˜",
    "å¤§ä¾¿é›£": "ä¾¿ç§˜",
    "è„¾ç´„": "ä¾¿ç§˜",
    "ç´å‘†": "é£Ÿæ…¾ä¸æŒ¯",
    "ç´å°‘": "é£Ÿæ…¾ä¸æŒ¯",
    "ä¸æ€é£²é£Ÿ": "é£Ÿæ…¾ä¸æŒ¯",
    "ç—æ»¿": "æ¶ˆåŒ–ä¸è‰¯",
    "å™¯æ°£": "æ‰“å—",
    "å‘ƒé€†": "æ‰“å—",
    "æ³›é…¸": "èƒƒé£Ÿé“é€†æµ",
    "åé…¸": "èƒƒé£Ÿé“é€†æµ",

    # --- å‘¼å¸é¡ ---
    "å’³å—½": "å’³å–˜", 
    "å“®ç—…": "æ°£å–˜",
    "å–˜è­‰": "æ°£å–˜",
    "è‚ºè„¹": "COPD",
    "è‚ºç™†": "è‚ºçµæ ¸",
    "æ„Ÿå†’": "å‚·é¢¨",

    # --- å©¦ç§‘é¡ ---
    "æœˆç¶“å…ˆæœŸ": "æœˆç¶“æå‰",
    "æœˆç¶“å¾ŒæœŸ": "æœˆç¶“å»¶å¾Œ",
    "ç¶“äº‚": "æœˆç¶“ä¸èª¿",
    "ç—›ç¶“": "ç¶“è¡Œè…¹ç—›",
    "é–‰ç¶“": "ç¶“é–‰",
    "å´©æ¼": "åŠŸèƒ½æ€§å­å®®å‡ºè¡€",
    "å¸¶ä¸‹": "ç™½å¸¶",
    "çµ•ç¶“å‰å¾Œè«¸è­‰": "æ›´å¹´æœŸç¶œåˆç—‡",
    "ä¹³ç™–": "ä¹³è…ºå¢ç”Ÿ",

    # --- äº”å®˜èˆ‡å…¶ä»– ---
    "çœ©æšˆ": "é ­æšˆ",
    "è€³é³´": "é‡è½", 
    "é¼»æ·µ": "é¼»ç«‡ç‚",
    "é¼»é¼½": "éæ•æ€§é¼»ç‚",
    "å£ç˜¡": "å£è…”æ½°ç˜",
    "å–‰ç—º": "å’½ç‚",
    "æ¶ˆæ¸´": "ç³–å°¿ç—…",
    "æ°´è…«": "æµ®è…«",
    "è™›å‹": "æ…¢æ€§ç–²å‹",
    
    # --- è­‰å‹å½¢å®¹è©å°æ‡‰ ---
    "æ°£è™›": ["æ°£å°‘", "æ°£ä¸è¶³", "æ°£æ€¯"],
    "è¡€è™›": ["è¡€å°‘", "è¡€è™§", "è¡€ä¸è¶³", "è¡€æ¯"],
    "é™°è™›": ["é™°è™§", "é™°æ¶²ä¸è¶³", "é™°åˆ†ä¸è¶³"],
    "é™½è™›": ["é™½æ°£ä¸è¶³", "å‘½é–€ç«è¡°", "çœŸé™½ä¸è¶³"],
    "å¯¦ç†±": ["ç«æ—º", "ç†±ç››", "ç«ç†±"],
    "è‚é¬±": ["è‚æ°£é¬±çµ", "è‚æ°£ä¸èˆ’", "æ°£æ»¯"],
    "æ¿•ç†±": ["æ¿•ç†±å…§è˜Š", "æ¿•ç†±ä¸‹æ³¨"],
    "ç—°æ¿•": ["ç—°æ¿", "ç—°é£²"],
    "ç˜€è¡€": ["è¡€ç˜€", "è“„è¡€", "æƒ¡è¡€"],
}

def normalize_text(text):
    """æ¨™æº–åŒ–æ–‡å­—"""
    if pd.isna(text): return ""
    text = str(text)
    # ç§»é™¤æ‹¬è™Ÿå…§å®¹ (å¦‚ "ä¸å¯(å¿ƒè„¾å…©è™›)" -> "ä¸å¯")
    text = re.sub(r"[\(ï¼ˆ].*?[\)ï¼‰]", "", text)
    # ç§»é™¤æ¨™é»
    text = re.sub(r"[ï¼Œã€‚ã€ï¼›ï¼šï¼Ÿï¼]", "", text)
    return text.strip()

def check_match(pred, expected, synonyms_list=None):
    """
    æ ¸å¿ƒæ¯”å°é‚è¼¯ (3å±¤é˜²è­·)
    1. åŒ…å«åŒ¹é… (Inclusion)
    2. æ“´å……è©åº«åŒ¹é… (Dictionary Lookup)
    3. é—œéµå­—å¬å› (Character Recall)
    """
    if not pred or not expected: return False
    
    pred_norm = normalize_text(pred)
    
    # æº–å‚™æ¯”å°ç›®æ¨™æ¸…å–®
    targets = [expected]
    
    # A. åŠ å…¥ YAML ä¸­å®šç¾©çš„åŒç¾©è© (è‹¥æœ‰)
    if synonyms_list:
        targets.extend(synonyms_list)
    
    # B. åŠ å…¥ å…¨åŸŸè©åº« SYNONYMS ä¸­çš„åŒç¾©è©
    # é‚è¼¯ï¼šå¦‚æœæ¨™æº–ç­”æ¡ˆåŒ…å« Key (å¦‚ "ä¸å¯")ï¼Œå°±åŠ å…¥ Value (å¦‚ "å¤±çœ ")
    for key, val in SYNONYMS.items():
        if key in expected:
            if isinstance(val, list):
                targets.extend(val)
            else:
                targets.append(val)

    for target in targets:
        target_norm = normalize_text(target)
        if not target_norm: continue
        
        # 1. ç›´æ¥åŒ…å« (e.g. é æ¸¬ "å¿ƒè„¾å…©è™›è­‰" åŒ…å« "å¿ƒè„¾å…©è™›")
        if target_norm in pred_norm:
            return True
            
        # 2. é—œéµå­—å¬å› (è§£æ±º "å¿ƒè†½æ°£è™›" vs "å¿ƒè™›è†½æ€¯")
        s_pred = set(pred_norm)
        s_target = set(target_norm)
        if not s_target: continue
        
        overlap = len(s_pred.intersection(s_target)) / len(s_target)
        
        # é–€æª» 0.6 (60% å­—å…ƒé‡ç–Šå³ç®—å°)
        if overlap >= 0.6:
            return True
            
    return False

def main():
    if not os.path.exists(INPUT_CSV) or not os.path.exists(YAML_FILE):
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆã€‚è«‹ç¢ºèª {INPUT_CSV} èˆ‡ {YAML_FILE} éƒ½åœ¨ç›®éŒ„ä¸‹ã€‚")
        return

    print(f"ğŸ“– è®€å–è€ƒå·: {YAML_FILE} ...")
    with open(YAML_FILE, 'r', encoding='utf-8') as f:
        yaml_data = yaml.safe_load(f)
    
    # å»ºç«‹ {CaseID: CaseData} çš„å¿«é€ŸæŸ¥è©¢è¡¨
    case_db = {c['id']: c for c in yaml_data.get('test_cases', [])}
    print(f"   - è¼‰å…¥ {len(case_db)} å€‹æ¨™æº–æ¡ˆä¾‹ã€‚")

    print(f"ğŸ“– è®€å–ä½œç­”: {INPUT_CSV} ...")
    df = pd.read_csv(INPUT_CSV)
    
    # åˆå§‹åŒ–çµ±è¨ˆ
    correct_count = 0
    rescued_count = 0
    
    def verify_row(row):
        nonlocal correct_count, rescued_count
        
        case_id = row['CaseID']
        pred = row.get('PredPattern', '')
        original_acc = float(row.get('Accuracy', 0))
        
        # 1. å¦‚æœåŸå§‹å·²ç¶“å°äº†ï¼Œå°±ä¿æŒ
        if original_acc == 1.0:
            correct_count += 1
            return 1.0
            
        # 2. å¦‚æœåŸå§‹éŒ¯äº†ï¼Œå» YAML æ‰¾æ¨™æº–ç­”æ¡ˆé‡åˆ¤
        if case_id in case_db:
            case_info = case_db[case_id]
            expected = case_info['expected_diagnosis'] # é€™è£¡å¯èƒ½æ˜¯å­—ä¸²æˆ–ç‰©ä»¶
            
            # è™•ç† YAML çµæ§‹å·®ç•° (æœ‰äº›æ˜¯å­—ä¸²ï¼Œæœ‰äº›æ˜¯ dict)
            if isinstance(expected, dict):
                primary = expected.get('primary_pattern', '')
                syns = expected.get('synonyms', [])
            else:
                primary = str(expected)
                syns = []
            
            # åŸ·è¡Œæ ¸å¿ƒæ¯”å°
            if check_match(pred, primary, syns):
                correct_count += 1
                rescued_count += 1
                return 1.0 # æ•‘æ´æˆåŠŸ
        
        return 0.0

    print("ğŸ”„ æ­£åœ¨é€²è¡ŒçœŸå€¼æ¯”å° (Ground Truth Verification)...")
    df['Is_Correct_Verified'] = df.apply(verify_row, axis=1)
    
    final_acc = df['Is_Correct_Verified'].mean()
    raw_acc = pd.to_numeric(df['Accuracy'], errors='coerce').fillna(0).mean()
    
    print("-" * 50)
    print(f"ğŸ“Š åˆ†æå ±å‘Š:")
    print(f"   - ç¸½è³‡æ–™ç­†æ•¸: {len(df)}")
    print(f"   - åŸå§‹æº–ç¢ºç‡ (Raw):      {raw_acc:.2%}")
    print(f"   - çœŸå¯¦æº–ç¢ºç‡ (Verified): {final_acc:.2%}")
    print(f"   - è©åº«æ•‘æ´æˆåŠŸç­†æ•¸:      {rescued_count}")
    print("-" * 50)
    
    df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    print(f"âœ… çµæœå·²å„²å­˜: {OUTPUT_CSV}")

if __name__ == "__main__":
    main()