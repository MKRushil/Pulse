# Backend/collect_experiment_data.py
"""
å¯¦é©—æ•¸æ“šæ”¶é›†å™¨
è‡ªå‹•åŒ–åŸ·è¡Œå¯¦é©—ä¸¦è¨˜éŒ„æ‰€æœ‰æ•¸æ“š
"""

import asyncio
import json
import csv
from pathlib import Path
from datetime import datetime
from typing import List, Dict
import time

from s_cbr.main import SCBREngine

class ExperimentDataCollector:
    """å¯¦é©—æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self, output_dir: str = "experiment_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.engine = SCBREngine()
        self.results = []
        
    async def run_experiment(
        self,
        experiment_name: str,
        test_cases: List[Dict],
        config: Dict = None
    ):
        """
        åŸ·è¡Œå¯¦é©—
        
        Args:
            experiment_name: å¯¦é©—åç¨±ï¼ˆå¦‚ "baseline", "full_system"ï¼‰
            test_cases: æ¸¬è©¦æ¡ˆä¾‹åˆ—è¡¨
            config: å¯¦é©—é…ç½®ï¼ˆç”¨æ–¼æ¶ˆèå¯¦é©—ï¼‰
        """
        print(f"\n{'=' * 80}")
        print(f"ğŸ§ª é–‹å§‹å¯¦é©—: {experiment_name}")
        print(f"{'=' * 80}\n")
        
        experiment_results = []
        
        for i, case in enumerate(test_cases, 1):
            print(f"\nè™•ç†æ¡ˆä¾‹ {i}/{len(test_cases)}: {case['id']}")
            
            case_result = await self.process_case(case, experiment_name)
            experiment_results.append(case_result)
            
            # é¡¯ç¤ºé€²åº¦
            accuracy = sum(1 for r in experiment_results if r['is_correct']) / len(experiment_results)
            avg_rounds = sum(r['total_rounds'] for r in experiment_results) / len(experiment_results)
            print(f"   ç•¶å‰æº–ç¢ºç‡: {accuracy:.1%}, å¹³å‡è¼ªæ¬¡: {avg_rounds:.1f}")
        
        # å„²å­˜çµæœ
        self.save_experiment_results(experiment_name, experiment_results)
        
        # ç”Ÿæˆçµ±è¨ˆ
        self.generate_statistics(experiment_name, experiment_results)
        
        return experiment_results
    
    async def process_case(self, case: Dict, experiment_name: str) -> Dict:
        """è™•ç†å–®å€‹æ¡ˆä¾‹"""
        case_id = case['id']
        rounds_data = case['rounds']
        expected_diagnosis = case.get('expected_pattern', '')
        
        session_id = None
        round_results = []
        
        start_time = time.time()
        
        for round_num, question in enumerate(rounds_data, 1):
            print(f"      Round {round_num}: {question[:50]}...")
            
            if round_num == 1:
                result = await self.engine.diagnose(question)
                session_id = result['session_id']
            else:
                result = await self.engine.diagnose(
                    question,
                    session_id=session_id,
                    continue_spiral=True
                )
            
            # è¨˜éŒ„æœ¬è¼ªæ•¸æ“š
            round_result = {
                "round": round_num,
                "question": question,
                "diagnosis": result['primary'].get('diagnosis', ''),
                "convergence_metrics": result['convergence_metrics'],
                "stop_decision": result['stop_decision'],
                "gap_questions": result.get('gap_questions', []),
                "pattern_shift": result.get('pattern_shift', {}),
                "review_info": result.get('review_info', {}),
                "processing_time": result['processing_time']
            }
            
            round_results.append(round_result)
            
            # å¦‚æœå·²æ”¶æ–‚ï¼Œåœæ­¢
            if result['stop_decision']['should_stop']:
                break
        
        total_time = time.time() - start_time
        
        # æœ€çµ‚è¨ºæ–·
        final_diagnosis = round_results[-1]['diagnosis']
        
        # åˆ¤æ–·æ˜¯å¦æ­£ç¢º
        is_correct = expected_diagnosis in final_diagnosis
        
        case_result = {
            "case_id": case_id,
            "experiment": experiment_name,
            "timestamp": datetime.now().isoformat(),
            "total_rounds": len(round_results),
            "total_time": total_time,
            "final_diagnosis": final_diagnosis,
            "expected_diagnosis": expected_diagnosis,
            "is_correct": is_correct,
            "rounds": round_results,
            # èšåˆæŒ‡æ¨™
            "final_ci": round_results[-1]['convergence_metrics']['Final'],
            "avg_processing_time": sum(r['processing_time'] for r in round_results) / len(round_results),
            "gap_questions_count": sum(len(r['gap_questions']) for r in round_results),
            "pattern_shifted": any(r['pattern_shift'].get('shifted', False) for r in round_results),
            "review_revised": any(r['review_info'].get('revised', False) for r in round_results),
        }
        
        print(f"      âœ“ å®Œæˆ - è¨ºæ–·: {final_diagnosis}, æ­£ç¢º: {'âœ…' if is_correct else 'âŒ'}")
        
        return case_result
    
    def save_experiment_results(self, experiment_name: str, results: List[Dict]):
        """å„²å­˜å¯¦é©—çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON æ ¼å¼ï¼ˆå®Œæ•´æ•¸æ“šï¼‰
        json_path = self.output_dir / f"{experiment_name}_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ çµæœå·²å„²å­˜: {json_path}")
        
        # CSV æ ¼å¼ï¼ˆæ‘˜è¦æ•¸æ“šï¼‰
        csv_path = self.output_dir / f"{experiment_name}_{timestamp}.csv"
        
        fieldnames = [
            'case_id', 'total_rounds', 'total_time', 'final_diagnosis',
            'expected_diagnosis', 'is_correct', 'final_ci',
            'gap_questions_count', 'pattern_shifted', 'review_revised'
        ]
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in results:
                row = {k: result[k] for k in fieldnames}
                writer.writerow(row)
        
        print(f"ğŸ’¾ CSV å·²å„²å­˜: {csv_path}")
    
    def generate_statistics(self, experiment_name: str, results: List[Dict]):
        """ç”Ÿæˆçµ±è¨ˆå ±å‘Š"""
        print(f"\n{'=' * 80}")
        print(f"ğŸ“Š å¯¦é©—çµ±è¨ˆ: {experiment_name}")
        print(f"{'=' * 80}\n")
        
        total_cases = len(results)
        correct_cases = sum(1 for r in results if r['is_correct'])
        accuracy = correct_cases / total_cases * 100
        
        avg_rounds = sum(r['total_rounds'] for r in results) / total_cases
        avg_time = sum(r['total_time'] for r in results) / total_cases
        avg_ci = sum(r['final_ci'] for r in results) / total_cases
        
        gap_questions_triggered = sum(1 for r in results if r['gap_questions_count'] > 0)
        pattern_shifted_count = sum(1 for r in results if r['pattern_shifted'])
        review_revised_count = sum(1 for r in results if r['review_revised'])
        
        print(f"ã€æº–ç¢ºæ€§ã€‘")
        print(f"  æ¡ˆä¾‹ç¸½æ•¸: {total_cases}")
        print(f"  æ­£ç¢ºæ¡ˆä¾‹: {correct_cases}")
        print(f"  æº–ç¢ºç‡: {accuracy:.1f}%")
        
        print(f"\nã€æ”¶æ–‚æ€§ã€‘")
        print(f"  å¹³å‡è¼ªæ¬¡: {avg_rounds:.2f}")
        print(f"  å¹³å‡æœ€çµ‚ CI: {avg_ci:.3f}")
        
        print(f"\nã€æ•ˆç‡ã€‘")
        print(f"  å¹³å‡ç¸½æ™‚é–“: {avg_time:.2f}s")
        
        print(f"\nã€æ™ºèƒ½è¼”åŠ©ã€‘")
        print(f"  è£œå•è§¸ç™¼: {gap_questions_triggered}/{total_cases} ({gap_questions_triggered/total_cases*100:.1f}%)")
        print(f"  è­‰å‹è½‰åŒ–: {pattern_shifted_count}/{total_cases} ({pattern_shifted_count/total_cases*100:.1f}%)")
        print(f"  å¯©ç¨¿ä¿®æ­£: {review_revised_count}/{total_cases} ({review_revised_count/total_cases*100:.1f}%)")
        
        # å„²å­˜çµ±è¨ˆæ‘˜è¦
        stats = {
            "experiment": experiment_name,
            "total_cases": total_cases,
            "correct_cases": correct_cases,
            "accuracy": accuracy,
            "avg_rounds": avg_rounds,
            "avg_time": avg_time,
            "avg_ci": avg_ci,
            "gap_questions_triggered": gap_questions_triggered,
            "pattern_shifted_count": pattern_shifted_count,
            "review_revised_count": review_revised_count,
        }
        
        stats_path = self.output_dir / f"{experiment_name}_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)

# ==================== æ¸¬è©¦æ¡ˆä¾‹å®šç¾© ====================

TEST_CASES_FULL = [
    {
        "id": "case_001",
        "category": "simple",
        "rounds": [
            "å¤±çœ å¤šå¤¢ï¼Œå¿ƒæ‚¸å¥å¿˜ï¼Œç–²å€¦ä¹åŠ›",
            "èˆŒå°–ç´…ï¼Œå°‘è‹”",
            "å£ä¹¾ï¼Œäº”å¿ƒç…©ç†±"
        ],
        "expected_pattern": "å¿ƒè…ä¸äº¤"
    },
    {
        "id": "case_002",
        "category": "simple",
        "rounds": [
            "å’³å—½ï¼Œç—°ç™½ç¨€è–„ï¼Œæƒ¡å¯’ç„¡æ±—",
            "é¼»å¡æµæ¸…æ¶•",
            "è„ˆæµ®ç·Š"
        ],
        "expected_pattern": "é¢¨å¯’æŸè‚º"
    },
    {
        "id": "case_003",
        "category": "medium",
        "rounds": [
            "è…¹è„¹ï¼Œé£Ÿæ…¾ä¸æŒ¯ï¼Œç–²å€¦ä¹åŠ›",
            "å¤§ä¾¿æºè–„ï¼Œé¢è‰²èé»ƒ",
            "èˆŒæ·¡è‹”ç™½"
        ],
        "expected_pattern": "è„¾æ°£è™›"
    },
    {
        "id": "case_004",
        "category": "medium",
        "rounds": [
            "é ­ç—›é ­æšˆï¼Œæ€¥èºæ˜“æ€’",
            "è„…ç—›ï¼Œå£è‹¦",
            "è„ˆå¼¦"
        ],
        "expected_pattern": "è‚é™½ä¸Šäº¢"
    },
    {
        "id": "case_005",
        "category": "medium",
        "rounds": [
            "è…°è†ç— è»Ÿï¼Œè€³é³´ï¼Œäº”å¿ƒç…©ç†±",
            "ç›œæ±—ï¼Œéºç²¾",
            "èˆŒç´…å°‘è‹”"
        ],
        "expected_pattern": "è…é™°è™›"
    },
    # å¯ä»¥ç¹¼çºŒæ·»åŠ æ›´å¤šæ¡ˆä¾‹...
]

async def main():
    """ä¸»ç¨‹åº"""
    collector = ExperimentDataCollector()
    
    # åŸ·è¡Œå¯¦é©—
    # é€™è£¡å¯ä»¥é‹è¡Œä¸åŒé…ç½®çš„å¯¦é©—
    
    print("é¸æ“‡å¯¦é©—é¡å‹ï¼š")
    print("1. Full Systemï¼ˆå®Œæ•´ç³»çµ±ï¼‰")
    print("2. Baselineï¼ˆåŸºç·šï¼‰")
    print("3. å…¨éƒ¨å¯¦é©—ï¼ˆæ¶ˆèå¯¦é©—ï¼‰")
    
    choice = input("è«‹é¸æ“‡ (1-3): ").strip()
    
    if choice == "1":
        await collector.run_experiment("full_system", TEST_CASES_FULL)
    
    elif choice == "2":
        # Baseline é…ç½®ï¼ˆéœ€è¦åœ¨ main.py ä¸­æ”¯æŒé…ç½®åˆ‡æ›ï¼‰
        await collector.run_experiment("baseline", TEST_CASES_FULL)
    
    elif choice == "3":
        # é‹è¡Œæ‰€æœ‰å¯¦é©—çµ„
        experiments = ["baseline", "group_a", "group_b", "group_c", "full_system"]
        for exp in experiments:
            await collector.run_experiment(exp, TEST_CASES_FULL)
            print("\nç­‰å¾… 5 ç§’å¾Œç¹¼çºŒä¸‹ä¸€å€‹å¯¦é©—...\n")
            await asyncio.sleep(5)

if __name__ == "__main__":
    asyncio.run(main())