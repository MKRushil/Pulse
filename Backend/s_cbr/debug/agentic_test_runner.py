#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SCBR Agentic NLU æ±ºç­–é‚è¼¯é©—è­‰æ¸¬è©¦æ¡†æ¶
Phase 1.5 ç³»çµ±æ¸¬è©¦èˆ‡é©—è­‰

ç‰ˆæœ¬: v1.0
æ—¥æœŸ: 2025-11-24
ä½ç½®: s_cbr/debug/agentic_test_runner.py

æ¸¬è©¦ç›®æ¨™ï¼š
1. L1 Agentic Gate æ±ºç­–é‚è¼¯é©—è­‰
   - Alpha å€¼é¸æ“‡æ˜¯å¦ç¬¦åˆè¼¸å…¥ç‰¹æ€§
   - ç½®ä¿¡åº¦è©•ä¼°æ˜¯å¦åˆç†
   - è¿½å•æ±ºç­–æ˜¯å¦é©ç•¶

2. L2 Agentic è¨ºæ–·å±¤å·¥å…·èª¿ç”¨é©—è­‰
   - å·¥å…·èª¿ç”¨æ™‚æ©Ÿæ˜¯å¦æ­£ç¢º
   - å·¥å…·é¸æ“‡æ˜¯å¦ç¬¦åˆé‚è¼¯
   - çµæœæ•´åˆæ˜¯å¦æœ‰æ•ˆ

3. æª¢ç´¢å“è³ªèˆ‡ Fallback æ©Ÿåˆ¶é©—è­‰
   - å“è³ªè©•ä¼°æ˜¯å¦æº–ç¢º
   - Fallback è§¸ç™¼æ˜¯å¦åŠæ™‚
   - é‡è©¦ç­–ç•¥æ˜¯å¦æœ‰æ•ˆ

æ¸¬è©¦æŒ‡æ¨™ï¼š
- æ±ºç­–é‚è¼¯ç¬¦åˆç‡ï¼ˆLogic Compliance Rateï¼‰
- å·¥å…·èª¿ç”¨æº–ç¢ºç‡ï¼ˆTool Call Accuracyï¼‰
- æª¢ç´¢å“è³ªç©©å®šæ€§ï¼ˆRetrieval Quality Stabilityï¼‰
"""

import os
import sys
import json
import yaml
import time
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from collections import defaultdict
import statistics

# ============================================
# æ¸¬è©¦é…ç½®
# ============================================

class TestConfig:
    """æ¸¬è©¦é…ç½®ç®¡ç†é¡"""
    
    # API ç«¯é»é…ç½®
    API_BASE_URL = os.environ.get('SCBR_API_URL', 'http://localhost:8000')
    API_HEALTH_ENDPOINT = '/healthz'
    API_DIAGNOSE_ENDPOINT = '/api/scbr/v2/diagnose'
    
    # æª”æ¡ˆè·¯å¾‘é…ç½®ï¼ˆç›¸å°æ–¼ s_cbr/debug ç›®éŒ„ï¼‰
    TEST_CASES_FILE = 'agentic_test_cases.yaml'
    REPORT_DIR = 'test_results/reports'
    LOG_DIR = 'test_results/logs'
    
    # æ¸¬è©¦è¡Œç‚ºé…ç½®
    ENABLE_DEBUG = os.environ.get('SCBR_DEBUG', 'false').lower() == 'true'
    MAX_ROUNDS_PER_CASE = 7
    REQUEST_TIMEOUT = 240
    ROUND_INTERVAL = 0.5
    CASE_INTERVAL = 1.0
    
    # Agentic æ±ºç­–é‚è¼¯è©•ä¼°æ¨™æº–
    ALPHA_LOW_THRESHOLD = 0.4      # é—œéµå­—ç‚ºä¸»ç­–ç•¥çš„ alpha ä¸Šé™
    ALPHA_HIGH_THRESHOLD = 0.6     # å‘é‡ç‚ºä¸»ç­–ç•¥çš„ alpha ä¸‹é™
    CONFIDENCE_LOW_THRESHOLD = 0.55  # éœ€è¦è¿½å•çš„ç½®ä¿¡åº¦é–€æª»
    QUALITY_ACCEPTABLE = 0.65      # å¯æ¥å—çš„æª¢ç´¢å“è³ªé–€æª»
    
    # å·¥å…·èª¿ç”¨è©•ä¼°æ¨™æº–
    KNOWLEDGE_GAP_THRESHOLD = 0.6   # æ¡ˆä¾‹å®Œæ•´åº¦é–€æª»
    VALIDATION_THRESHOLD = 0.7      # è¨ºæ–·ç½®ä¿¡åº¦é–€æª»
    
    @staticmethod
    def ensure_dirs():
        """ç¢ºä¿å¿…è¦çš„ç›®éŒ„å­˜åœ¨"""
        os.makedirs(TestConfig.REPORT_DIR, exist_ok=True)
        os.makedirs(TestConfig.LOG_DIR, exist_ok=True)


# ============================================
# æ¸¬è©¦çµæœæ•¸æ“šçµæ§‹
# ============================================

class AgenticDecisionMetrics:
    """Agentic æ±ºç­–é‚è¼¯è©•ä¼°æŒ‡æ¨™é›†åˆ"""
    
    def __init__(self):
        # L1 æ±ºç­–æŒ‡æ¨™
        self.alpha_decisions = []
        self.confidence_scores = []
        self.strategy_types = []
        self.follow_up_triggered = 0
        self.search_triggered = 0
        
        # L2 å·¥å…·èª¿ç”¨æŒ‡æ¨™
        self.tool_a_calls = 0
        self.tool_b_calls = 0
        self.tool_c_calls = 0
        self.total_tool_calls = 0
        self.validation_status_counts = defaultdict(int)
        self.case_completeness_scores = []
        self.diagnosis_confidence_scores = []
        
        # æª¢ç´¢å“è³ªæŒ‡æ¨™
        self.quality_scores = []
        self.fallback_triggered = 0
        self.fallback_attempts = []
        self.alpha_adjustments = []
        
        # æ±ºç­–é‚è¼¯ç¬¦åˆåº¦è©•ä¼°
        self.logic_checks = {
            'alpha_selection': [],
            'confidence_action': [],
            'tool_decision': [],
            'retrieval_quality': []
        }
        
        # å®‰å…¨æ¸¬è©¦æŒ‡æ¨™
        self.security_blocks = 0
        self.security_passed = 0


class TestCaseResult:
    """å–®ä¸€æ¸¬è©¦æ¡ˆä¾‹çš„çµæœè¨˜éŒ„"""
    
    def __init__(self, case_id: str, case_name: str, case_type: str):
        self.case_id = case_id
        self.case_name = case_name
        self.case_type = case_type
        self.description = ""
        self.rounds = []
        self.success = False
        self.error_message = None
        self.total_time = 0
        self.metrics = AgenticDecisionMetrics()


class RoundResult:
    """å–®è¼ªæ¸¬è©¦çµæœè©³ç´°è¨˜éŒ„"""
    
    def __init__(self, round_num: int, question: str):
        self.round_num = round_num
        self.question = question  # å–®è¼ªå•é¡Œï¼ˆå‘å¾Œå…¼å®¹ï¼‰
        self.accumulated_question = question  # ğŸ†• ç´¯ç©å¾Œçš„å®Œæ•´å•é¡Œ
        self.original_question = question     # ğŸ†• åŸå§‹å–®è¼ªå•é¡Œ
        self.response = None
        self.response_time = 0
        self.http_status = None
        
        # L1 æ±ºç­–è³‡è¨Š
        self.l1_overall_confidence = None
        self.l1_decided_alpha = None
        self.l1_strategy_type = None
        self.l1_next_action = None
        self.l1_expected_quality = None
        
        # L2 Agentic è³‡è¨Š
        self.l2_case_completeness = None
        self.l2_diagnosis_confidence = None
        self.l2_validation_status = None
        self.l2_tool_calls = 0
        self.l2_confidence_boost = 0
        
        # æª¢ç´¢è³‡è¨Š
        self.retrieval_initial_alpha = None
        self.retrieval_final_alpha = None
        self.retrieval_quality_score = None
        self.retrieval_fallback_triggered = False
        self.retrieval_attempts = 0
        
        # å®‰å…¨è³‡è¨Š
        self.security_blocked = False
        self.security_flags = []
        
        # æ±ºç­–é‚è¼¯è©•ä¼°çµæœ
        self.logic_evaluations = {}


# ============================================
# JSONL æ—¥èªŒè¨˜éŒ„å™¨
# ============================================

class JSONLLogger:
    """JSONL æ ¼å¼æ—¥èªŒè¨˜éŒ„å™¨ï¼Œç”¨æ–¼è©³ç´°çš„æ¸¬è©¦æ•¸æ“šè¿½è¹¤"""
    
    def __init__(self):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.detail_log = os.path.join(
            TestConfig.LOG_DIR,
            f'test_details_{timestamp}.jsonl'
        )
        self.decision_log = os.path.join(
            TestConfig.LOG_DIR,
            f'agentic_decisions_{timestamp}.jsonl'
        )
        self.evaluation_log = os.path.join(
            TestConfig.LOG_DIR,
            f'logic_evaluations_{timestamp}.jsonl'
        )
        
    def log_test_detail(self, data: Dict):
        """è¨˜éŒ„æ¸¬è©¦åŸ·è¡Œè©³ç´°è³‡è¨Š"""
        self._append_to_file(self.detail_log, data)
    
    def log_decision(self, data: Dict):
        """è¨˜éŒ„ Agentic æ±ºç­–è³‡è¨Š"""
        self._append_to_file(self.decision_log, data)
    
    def log_evaluation(self, data: Dict):
        """è¨˜éŒ„é‚è¼¯è©•ä¼°çµæœ"""
        self._append_to_file(self.evaluation_log, data)
    
    def _append_to_file(self, filepath: str, data: Dict):
        """è¿½åŠ  JSONL è¨˜éŒ„åˆ°æª”æ¡ˆ"""
        try:
            data['timestamp'] = datetime.now().isoformat()
            with open(filepath, 'a', encoding='utf-8') as f:
                json_line = json.dumps(data, ensure_ascii=False)
                f.write(json_line + '\n')
        except Exception as e:
            print(f"âš ï¸  å¯«å…¥æ—¥èªŒå¤±æ•— {filepath}: {e}")


# ============================================
# æ±ºç­–é‚è¼¯è©•ä¼°å™¨
# ============================================

class DecisionLogicEvaluator:
    """
    Agentic æ±ºç­–é‚è¼¯è©•ä¼°å™¨
    
    è·è²¬ï¼š
    1. è©•ä¼° L1 çš„ alpha å€¼é¸æ“‡æ˜¯å¦åˆç†
    2. è©•ä¼° L1 çš„ç½®ä¿¡åº¦è©•ä¼°èˆ‡å‹•ä½œæ±ºç­–æ˜¯å¦æ­£ç¢º
    3. è©•ä¼° L2 çš„å·¥å…·èª¿ç”¨æ±ºç­–æ˜¯å¦ç¬¦åˆé‚è¼¯
    4. è©•ä¼°æª¢ç´¢å“è³ªèˆ‡ Fallback æ©Ÿåˆ¶æ˜¯å¦æœ‰æ•ˆ
    """
    
    @staticmethod
    def evaluate_l1_alpha_selection(
        input_text: str,
        decided_alpha: float,
        strategy_type: str,
        case_type: str
    ) -> Dict[str, Any]:
        """
        è©•ä¼° L1 çš„ alpha å€¼é¸æ“‡é‚è¼¯
        
        è©•ä¼°æ¨™æº–ï¼š
        - l1_terminology: æ¨™æº–è¡“èªè¼¸å…¥ â†’ æ‡‰é¸æ“‡ä½ alphaï¼ˆâ‰¤0.4ï¼‰
        - l1_oral: å£èªåŒ–æè¿° â†’ æ‡‰é¸æ“‡é«˜ alphaï¼ˆâ‰¥0.6ï¼‰
        - l1_insufficient: è³‡è¨Šä¸è¶³ â†’ alpha å€¼æ¬¡è¦ï¼Œé‡é»æ˜¯è¿½å•
        """
        evaluation = {
            'case_type': case_type,
            'input_style': None,
            'input_length': len(input_text),
            'decided_alpha': decided_alpha,
            'strategy_type': strategy_type,
            'expected_alpha_range': None,
            'is_compliant': False,
            'compliance_score': 0.0,
            'reasoning': '',
            'suggestions': []
        }
        
        # åŸºæ–¼æ¸¬è©¦æ¡ˆä¾‹é¡å‹çš„è©•ä¼°
        if case_type == 'l1_terminology':
            # æ¨™æº–è¡“èªè¼¸å…¥
            tcm_terms = ['å¤±çœ ', 'å¿ƒæ‚¸', 'æ°£çŸ­', 'ä¹åŠ›', 'èˆŒæ·¡', 'è„ˆç´°', 'èˆŒç´…', 'è‹”é»ƒ',
                         'é ­ç—›', 'çœ©æšˆ', 'å’³å—½', 'ç—°å¤š', 'èƒ¸æ‚¶', 'ç´å·®', 'ä¾¿æº', 'èƒ¸è„…',
                         'è„…ç—›', 'å¤ªæ¯', 'å™¯æ°£', 'èƒƒè„˜', 'å†·ç—›', 'å–œæº«', 'å–œæŒ‰', 'äº”å¿ƒ',
                         'ç…©ç†±', 'ç›œæ±—', 'è…°è†', 'ç— è»Ÿ']
            
            term_count = sum(1 for term in tcm_terms if term in input_text)
            
            evaluation['input_style'] = 'terminology'
            evaluation['expected_alpha_range'] = (0.0, TestConfig.ALPHA_LOW_THRESHOLD)
            evaluation['is_compliant'] = decided_alpha <= TestConfig.ALPHA_LOW_THRESHOLD
            evaluation['compliance_score'] = max(0, 1.0 - decided_alpha / TestConfig.ALPHA_LOW_THRESHOLD)
            evaluation['reasoning'] = f"æ¨™æº–è¡“èªè¼¸å…¥ï¼ˆæª¢æ¸¬åˆ° {term_count} å€‹è¡“èªï¼‰ï¼Œæ‡‰é¸æ“‡é—œéµå­—ç‚ºä¸»ç­–ç•¥"
            
            if not evaluation['is_compliant']:
                evaluation['suggestions'].append(
                    f"Alpha å€¼ {decided_alpha:.2f} è¶…éé—œéµå­—ç‚ºä¸»ç­–ç•¥é–¾å€¼ {TestConfig.ALPHA_LOW_THRESHOLD}ï¼Œ"
                    "å»ºè­°é™ä½ä»¥æå‡é—œéµå­—åŒ¹é…æ¬Šé‡"
                )
                
        elif case_type == 'l1_oral':
            # å£èªåŒ–æè¿°
            evaluation['input_style'] = 'oral'
            evaluation['expected_alpha_range'] = (TestConfig.ALPHA_HIGH_THRESHOLD, 1.0)
            evaluation['is_compliant'] = decided_alpha >= TestConfig.ALPHA_HIGH_THRESHOLD
            evaluation['compliance_score'] = min(1.0, decided_alpha / TestConfig.ALPHA_HIGH_THRESHOLD)
            evaluation['reasoning'] = "å£èªåŒ–æè¿°è¼¸å…¥ï¼Œæ‡‰é¸æ“‡å‘é‡ç‚ºä¸»ç­–ç•¥ä»¥ç†è§£èªç¾©"
            
            if not evaluation['is_compliant']:
                evaluation['suggestions'].append(
                    f"Alpha å€¼ {decided_alpha:.2f} ä½æ–¼å‘é‡ç‚ºä¸»ç­–ç•¥é–¾å€¼ {TestConfig.ALPHA_HIGH_THRESHOLD}ï¼Œ"
                    "å»ºè­°æé«˜ä»¥å¢å¼·èªç¾©ç†è§£èƒ½åŠ›"
                )
                
        elif case_type == 'l1_insufficient':
            # è³‡è¨Šä¸è¶³
            evaluation['input_style'] = 'insufficient'
            evaluation['expected_alpha_range'] = (0.0, 1.0)  # alpha å€¼æ¬¡è¦
            evaluation['is_compliant'] = True  # ä¸»è¦çœ‹è¿½å•é‚è¼¯
            evaluation['compliance_score'] = 0.5  # ä¸­æ€§è©•åˆ†
            evaluation['reasoning'] = "è³‡è¨Šä¸è¶³è¼¸å…¥ï¼Œalpha å€¼é¸æ“‡æ¬¡è¦ï¼Œé‡é»åœ¨æ–¼æ˜¯å¦ç”Ÿæˆé©ç•¶è¿½å•"
            
        else:
            # å…¶ä»–é¡å‹ï¼ˆæ··åˆæˆ–æœªåˆ†é¡ï¼‰
            evaluation['input_style'] = 'mixed'
            evaluation['expected_alpha_range'] = (
                TestConfig.ALPHA_LOW_THRESHOLD,
                TestConfig.ALPHA_HIGH_THRESHOLD
            )
            evaluation['is_compliant'] = (
                TestConfig.ALPHA_LOW_THRESHOLD <= decided_alpha <= TestConfig.ALPHA_HIGH_THRESHOLD
            )
            evaluation['compliance_score'] = 0.5 if evaluation['is_compliant'] else 0.0
            evaluation['reasoning'] = "æ··åˆè¼¸å…¥æˆ–æœªæ˜ç¢ºåˆ†é¡ï¼Œæ‡‰é¸æ“‡å‡è¡¡ç­–ç•¥"
        
        return evaluation
    
    @staticmethod
    def evaluate_l1_confidence_action(
        overall_confidence: float,
        next_action: str,
        input_length: int,
        case_type: str
    ) -> Dict[str, Any]:
        """
        è©•ä¼° L1 çš„ç½®ä¿¡åº¦è©•ä¼°èˆ‡ä¸‹ä¸€æ­¥å‹•ä½œæ±ºç­–
        
        è©•ä¼°æ¨™æº–ï¼š
        - ç½®ä¿¡åº¦ < 0.55 ä¸”è¼¸å…¥ç°¡çŸ­ â†’ æ‡‰ç”Ÿæˆè¿½å•ï¼ˆask_moreï¼‰
        - ç½®ä¿¡åº¦ >= 0.55 â†’ æ‡‰åŸ·è¡Œæœç´¢ï¼ˆvector_searchï¼‰
        - è³‡è¨Šä¸è¶³é¡å‹ â†’ ç„¡è«–ç½®ä¿¡åº¦éƒ½å‚¾å‘è¿½å•
        """
        evaluation = {
            'case_type': case_type,
            'confidence_score': overall_confidence,
            'input_length': input_length,
            'next_action': next_action,
            'expected_action': None,
            'is_compliant': False,
            'compliance_score': 0.0,
            'reasoning': '',
            'suggestions': []
        }
        
        # æ ¹æ“šæ¡ˆä¾‹é¡å‹å’Œç½®ä¿¡åº¦åˆ¤æ–·é æœŸå‹•ä½œ
        if case_type == 'l1_insufficient':
            # è³‡è¨Šä¸è¶³é¡å‹ï¼Œæ‡‰å‚¾å‘è¿½å•
            evaluation['expected_action'] = 'ask_more'
            evaluation['is_compliant'] = next_action == 'ask_more'
            evaluation['compliance_score'] = 1.0 if evaluation['is_compliant'] else 0.0
            evaluation['reasoning'] = f"è³‡è¨Šä¸è¶³è¼¸å…¥ï¼ˆé•·åº¦ {input_length}ï¼‰ï¼Œæ‡‰ç”Ÿæˆè¿½å•ä»¥è£œå……è³‡è¨Š"
            
            if not evaluation['is_compliant']:
                evaluation['suggestions'].append(
                    f"ç³»çµ±é¸æ“‡äº† {next_action}ï¼Œä½†å°æ–¼è³‡è¨Šä¸è¶³çš„è¼¸å…¥æ‡‰å„ªå…ˆè¿½å•"
                )
                
        elif overall_confidence < TestConfig.CONFIDENCE_LOW_THRESHOLD:
            # ä½ç½®ä¿¡åº¦
            if input_length < 30:
                evaluation['expected_action'] = 'ask_more'
                evaluation['is_compliant'] = next_action == 'ask_more'
                evaluation['reasoning'] = f"ç½®ä¿¡åº¦ {overall_confidence:.2f} åä½ä¸”è¼¸å…¥ç°¡çŸ­ï¼Œæ‡‰ç”Ÿæˆè¿½å•"
            else:
                evaluation['expected_action'] = 'vector_search'
                evaluation['is_compliant'] = next_action == 'vector_search'
                evaluation['reasoning'] = f"ç½®ä¿¡åº¦ {overall_confidence:.2f} åä½ä½†è¼¸å…¥å……è¶³ï¼Œæ‡‰åŸ·è¡Œæœç´¢"
            
            evaluation['compliance_score'] = 1.0 if evaluation['is_compliant'] else 0.0
            
            if not evaluation['is_compliant']:
                evaluation['suggestions'].append(
                    f"ç½®ä¿¡åº¦ {overall_confidence:.2f} è¼ƒä½ï¼Œå»ºè­° {evaluation['expected_action']}"
                )
                
        else:
            # æ­£å¸¸ç½®ä¿¡åº¦
            evaluation['expected_action'] = 'vector_search'
            evaluation['is_compliant'] = next_action == 'vector_search'
            evaluation['compliance_score'] = 1.0 if evaluation['is_compliant'] else 0.0
            evaluation['reasoning'] = f"ç½®ä¿¡åº¦ {overall_confidence:.2f} è¶³å¤ ï¼Œæ‡‰åŸ·è¡Œæœç´¢"
            
            if not evaluation['is_compliant']:
                evaluation['suggestions'].append(
                    f"ç½®ä¿¡åº¦ {overall_confidence:.2f} å·²é”æ¨™ï¼Œç„¡éœ€è¿½å•"
                )
        
        return evaluation
    
    @staticmethod
    def evaluate_l2_tool_decision(
        case_completeness: float,
        diagnosis_confidence: float,
        tool_calls: int,
        validation_status: str,
        case_type: str
    ) -> Dict[str, Any]:
        """
        è©•ä¼° L2 çš„å·¥å…·èª¿ç”¨æ±ºç­–é‚è¼¯
        
        è©•ä¼°æ¨™æº–ï¼š
        - æ¡ˆä¾‹å®Œæ•´åº¦ < 0.6 â†’ æ‡‰èª¿ç”¨ Tool Bï¼ˆçŸ¥è­˜è£œå……ï¼‰
        - è¨ºæ–·ç½®ä¿¡åº¦ < 0.7 â†’ æ‡‰èª¿ç”¨ Tool Cï¼ˆå¹»è¦ºæ ¡é©—ï¼‰
        - æœ‰æ˜ç¢ºè­‰å‹ â†’ æ‡‰èª¿ç”¨ Tool Aï¼ˆæ¬Šå¨èƒŒæ›¸ï¼‰
        """
        evaluation = {
            'case_type': case_type,
            'case_completeness': case_completeness,
            'diagnosis_confidence': diagnosis_confidence,
            'tool_calls': tool_calls,
            'validation_status': validation_status,
            'expected_min_calls': 0,
            'expected_tools': [],
            'is_compliant': False,
            'compliance_score': 0.0,
            'reasoning': [],
            'suggestions': []
        }
        
        # åŸºæ–¼æ¡ˆä¾‹é¡å‹çš„é æœŸ
        if case_type == 'l2_knowledge_gap':
            # çŸ¥è­˜è£œå……å ´æ™¯ï¼Œæ‡‰èª¿ç”¨ Tool B
            evaluation['expected_min_calls'] = 1
            evaluation['expected_tools'].append('Tool B (A+ç™¾ç§‘)')
            evaluation['reasoning'].append("çŸ¥è­˜è£œå……å ´æ™¯ï¼Œé æœŸèª¿ç”¨ Tool B è£œå……ç—…æ©Ÿåˆ†æ")
            
        elif case_type == 'l2_validation':
            # é©—è­‰å ´æ™¯ï¼Œæ‡‰èª¿ç”¨ Tool C æˆ–å¤šå€‹å·¥å…·
            if diagnosis_confidence is not None and diagnosis_confidence < TestConfig.VALIDATION_THRESHOLD:
                evaluation['expected_min_calls'] = 1
                evaluation['expected_tools'].append('Tool C (ETCM)')
                evaluation['reasoning'].append("è¨ºæ–·ç½®ä¿¡åº¦è¼ƒä½ï¼Œé æœŸèª¿ç”¨ Tool C é€²è¡Œç§‘å­¸é©—è­‰")
        
        # åŸºæ–¼æŒ‡æ¨™çš„å‹•æ…‹è©•ä¼°
        if case_completeness is not None and case_completeness < TestConfig.KNOWLEDGE_GAP_THRESHOLD:
            if 'Tool B' not in ' '.join(evaluation['expected_tools']):
                evaluation['expected_min_calls'] += 1
                evaluation['expected_tools'].append('Tool B (A+ç™¾ç§‘)')
                evaluation['reasoning'].append(
                    f"æ¡ˆä¾‹å®Œæ•´åº¦ {case_completeness:.2f} < {TestConfig.KNOWLEDGE_GAP_THRESHOLD}ï¼Œ"
                    "æ‡‰èª¿ç”¨ Tool B è£œå……çŸ¥è­˜"
                )
        
        if diagnosis_confidence is not None and diagnosis_confidence < TestConfig.VALIDATION_THRESHOLD:
            if 'Tool C' not in ' '.join(evaluation['expected_tools']):
                evaluation['expected_min_calls'] += 1
                evaluation['expected_tools'].append('Tool C (ETCM)')
                evaluation['reasoning'].append(
                    f"è¨ºæ–·ç½®ä¿¡åº¦ {diagnosis_confidence:.2f} < {TestConfig.VALIDATION_THRESHOLD}ï¼Œ"
                    "æ‡‰èª¿ç”¨ Tool C æ ¡é©—"
                )
        
        # åˆ¤æ–·ç¬¦åˆåº¦
        evaluation['is_compliant'] = tool_calls >= evaluation['expected_min_calls']
        
        if evaluation['expected_min_calls'] > 0:
            evaluation['compliance_score'] = min(1.0, tool_calls / evaluation['expected_min_calls'])
        else:
            # ç„¡éœ€èª¿ç”¨å·¥å…·çš„æƒ…æ³
            evaluation['compliance_score'] = 1.0 if tool_calls == 0 else 0.5
            evaluation['reasoning'].append("æ¢ä»¶æœªè§¸ç™¼å·¥å…·èª¿ç”¨é–€æª»ï¼Œç„¡éœ€èª¿ç”¨å·¥å…·")
        
        # ç”Ÿæˆå»ºè­°
        if not evaluation['is_compliant']:
            evaluation['suggestions'].append(
                f"å¯¦éš›èª¿ç”¨ {tool_calls} å€‹å·¥å…·ï¼Œé æœŸè‡³å°‘ {evaluation['expected_min_calls']} å€‹"
            )
            if evaluation['expected_tools']:
                evaluation['suggestions'].append(
                    f"å»ºè­°èª¿ç”¨: {', '.join(evaluation['expected_tools'])}"
                )
        
        return evaluation
    
    @staticmethod
    def evaluate_retrieval_quality(
        quality_score: float,
        fallback_triggered: bool,
        attempts: int,
        final_alpha: float,
        initial_alpha: float
    ) -> Dict[str, Any]:
        """
        è©•ä¼°æª¢ç´¢å“è³ªèˆ‡ Fallback æ©Ÿåˆ¶
        
        è©•ä¼°æ¨™æº–ï¼š
        - å“è³ªè©•åˆ† < 0.65 â†’ æ‡‰è§¸ç™¼ Fallback
        - Fallback æ‡‰å˜—è©¦ä¸åŒçš„ alpha å€¼
        - æœ€çµ‚å“è³ªæ‡‰æå‡
        """
        evaluation = {
            'quality_score': quality_score,
            'fallback_triggered': fallback_triggered,
            'attempts': attempts,
            'initial_alpha': initial_alpha,
            'final_alpha': final_alpha,
            'alpha_adjusted': abs(final_alpha - initial_alpha) > 0.05 if initial_alpha and final_alpha else False,
            'is_compliant': False,
            'compliance_score': 0.0,
            'reasoning': '',
            'suggestions': []
        }
        
        if quality_score is not None:
            if quality_score < TestConfig.QUALITY_ACCEPTABLE:
                # å“è³ªä¸è¶³
                evaluation['is_compliant'] = fallback_triggered
                evaluation['compliance_score'] = 1.0 if fallback_triggered else 0.0
                evaluation['reasoning'] = (
                    f"å“è³ªè©•åˆ† {quality_score:.2f} ä½æ–¼é–€æª» {TestConfig.QUALITY_ACCEPTABLE}ï¼Œ"
                    f"{'å·²' if fallback_triggered else 'æœª'}è§¸ç™¼ Fallback"
                )
                
                if not fallback_triggered:
                    evaluation['suggestions'].append(
                        "å“è³ªä¸è¶³ä½†æœªè§¸ç™¼ Fallbackï¼Œå»ºè­°æª¢æŸ¥å“è³ªè©•ä¼°é–€æª»è¨­å®š"
                    )
                elif attempts == 1:
                    evaluation['suggestions'].append(
                        "Fallback åƒ…å˜—è©¦ 1 æ¬¡ï¼Œå»ºè­°å¢åŠ é‡è©¦æ¬¡æ•¸ä»¥æå‡å“è³ª"
                    )
                    
            else:
                # å“è³ªå¯æ¥å—
                evaluation['is_compliant'] = True
                evaluation['compliance_score'] = min(1.0, quality_score / TestConfig.QUALITY_ACCEPTABLE)
                evaluation['reasoning'] = (
                    f"å“è³ªè©•åˆ† {quality_score:.2f} é”æ¨™ï¼Œ"
                    f"{'ç¶“é' if fallback_triggered else 'æœªç¶“é'} Fallback"
                )
        else:
            # ç„¡å“è³ªè©•åˆ†è³‡è¨Š
            evaluation['is_compliant'] = False
            evaluation['compliance_score'] = 0.0
            evaluation['reasoning'] = "ç¼ºå°‘æª¢ç´¢å“è³ªè©•åˆ†è³‡è¨Š"
            evaluation['suggestions'].append("ç„¡æ³•å–å¾—æª¢ç´¢å…ƒæ•¸æ“šï¼Œè«‹ç¢ºèª Agentic æª¢ç´¢å±¤æ˜¯å¦æ­£å¸¸é‹ä½œ")
        
        return evaluation


# ============================================
# API æ¸¬è©¦åŸ·è¡Œå™¨
# ============================================

class AgenticTestRunner:
    """Agentic NLU æ¸¬è©¦åŸ·è¡Œå™¨ä¸»æ§é¡"""
    
    def __init__(self):
        self.logger = JSONLLogger()
        self.evaluator = DecisionLogicEvaluator()
        TestConfig.ensure_dirs()
        print(f"\nğŸ“ æ¸¬è©¦çµæœå°‡ä¿å­˜è‡³:")
        print(f"   å ±å‘Š: {TestConfig.REPORT_DIR}")
        print(f"   æ—¥èªŒ: {TestConfig.LOG_DIR}")
        
    def _accumulate_questions(self, rounds: List[Dict], current_round: int) -> Tuple[str, str]:
        """
        ç´¯ç©å•é¡Œ - å¯¦ç¾ SCBR èºæ—‹å¼æ”¶æ–‚é‚è¼¯
        
        é€™æ˜¯ SCBRï¼ˆSpiral Case-Based Reasoningï¼‰çš„æ ¸å¿ƒç‰¹æ€§ï¼š
        æ¯ä¸€è¼ªéƒ½ç´¯ç©å‰é¢æ‰€æœ‰è¼ªæ¬¡çš„å•é¡Œï¼Œå¯¦ç¾é€æ­¥æ”¶æ–‚çš„è¨ºæ–·éç¨‹ã€‚
        
        Args:
            rounds: æ‰€æœ‰è¼ªæ¬¡çš„å•é¡Œåˆ—è¡¨
            current_round: ç•¶å‰è¼ªæ¬¡ï¼ˆ1-basedï¼Œå¾ 1 é–‹å§‹ï¼‰
        
        Returns:
            Tuple[ç´¯ç©å¾Œçš„å®Œæ•´å•é¡Œ, åŸå§‹å–®è¼ªå•é¡Œ]
        
        Example:
            rounds = [
                {"question": "å¿ƒæ‚¸æ°£çŸ­ï¼Œå‹•å‰‡åŠ é‡"},
                {"question": "ç¥ç–²ä¹åŠ›ï¼ŒèˆŒæ·¡è„ˆå¼±"},
                {"question": "è‡ªæ±—ï¼Œé¢è‰²æ·¡ç™½"}
            ]
            
            ç¬¬1è¼ª: "å¿ƒæ‚¸æ°£çŸ­ï¼Œå‹•å‰‡åŠ é‡"
            ç¬¬2è¼ª: "å¿ƒæ‚¸æ°£çŸ­ï¼Œå‹•å‰‡åŠ é‡ã€‚è£œå……ï¼šç¥ç–²ä¹åŠ›ï¼ŒèˆŒæ·¡è„ˆå¼±"
            ç¬¬3è¼ª: "å¿ƒæ‚¸æ°£çŸ­ï¼Œå‹•å‰‡åŠ é‡ã€‚è£œå……ï¼šç¥ç–²ä¹åŠ›ï¼ŒèˆŒæ·¡è„ˆå¼±ã€‚å†è£œå……ï¼šè‡ªæ±—ï¼Œé¢è‰²æ·¡ç™½"
        """
        if current_round < 1 or current_round > len(rounds):
            raise ValueError(f"ç„¡æ•ˆçš„è¼ªæ¬¡: {current_round}ï¼Œæœ‰æ•ˆç¯„åœ: 1-{len(rounds)}")
        
        # æå–åŸå§‹å–®è¼ªå•é¡Œ
        original_question = rounds[current_round - 1]['question']
        
        # ç¬¬ä¸€è¼ªï¼šç›´æ¥è¿”å›ç¬¬ä¸€å€‹å•é¡Œ
        if current_round == 1:
            return original_question, original_question
        
        # ç¬¬äºŒè¼ªä»¥å¾Œï¼šç´¯ç©æ‰€æœ‰å‰é¢çš„å•é¡Œ
        accumulated = rounds[0]['question']
        
        for i in range(1, current_round):
            # [ä¿®æ”¹] ä½¿ç”¨é€—è™Ÿè‡ªç„¶é€£æ¥ï¼Œè€Œé "è£œå……ï¼š" æ¨™ç±¤
            # é€™èƒ½æ¸›å°‘ Token æ¶ˆè€—ï¼Œä¸¦è®“èªæ„æ›´é€£è²«
            accumulated += f"ï¼Œ{rounds[i]['question']}"
        
        return accumulated, original_question
    
    def check_api_health(self) -> bool:
        """æª¢æŸ¥ API å¥åº·ç‹€æ…‹"""
        try:
            url = TestConfig.API_BASE_URL + TestConfig.API_HEALTH_ENDPOINT
            print(f"\nğŸ” æª¢æŸ¥ API å¥åº·ç‹€æ…‹: {url}")
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                health_data = response.json()
                print("âœ… API å¥åº·æª¢æŸ¥é€šé")
                print(f"   ç‰ˆæœ¬: {health_data.get('version', 'N/A')}")
                print(f"   æœå‹™: {health_data.get('service', 'N/A')}")
                return True
            else:
                print(f"âŒ API å¥åº·æª¢æŸ¥å¤±æ•—: HTTP {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ API é€£æ¥å¤±æ•—: {e}")
            return False
    
    def run_test_case(self, test_case: Dict) -> TestCaseResult:
        """åŸ·è¡Œå–®ä¸€æ¸¬è©¦æ¡ˆä¾‹"""
        case_id = test_case['id']
        case_name = test_case['name']
        case_type = test_case['type']
        description = test_case.get('description', '')
        rounds = test_case['rounds']
        
        print(f"\n{'='*70}")
        print(f"ğŸ“‹ æ¸¬è©¦æ¡ˆä¾‹: {case_id}")
        print(f"åç¨±: {case_name}")
        print(f"é¡å‹: {case_type}")
        if description:
            print(f"èªªæ˜: {description}")
        print(f"è¼ªæ•¸: {len(rounds)}")
        print(f"{'='*70}")
        
        result = TestCaseResult(case_id, case_name, case_type)
        result.description = description
        session_id = None
        start_time = time.time()
        
        try:
            for round_num, round_data in enumerate(rounds, 1):
                # ğŸ†• èºæ—‹ç´¯ç©é‚è¼¯ï¼šç´¯ç©æ‰€æœ‰å‰é¢è¼ªæ¬¡çš„å•é¡Œ
                accumulated_question, original_question = self._accumulate_questions(rounds, round_num)
                
                print(f"\n[è¼ªæ¬¡ {round_num}/{len(rounds)}]")
                # é¡¯ç¤ºç´¯ç©å¾Œçš„å•é¡Œï¼ˆå¦‚æœå¤ªé•·å‰‡æˆªæ–·é¡¯ç¤ºï¼‰
                display_question = accumulated_question[:80] + '...' if len(accumulated_question) > 80 else accumulated_question
                print(f"å•é¡Œ: {display_question}")
                if len(accumulated_question) > 80:
                    print(f"      (å®Œæ•´å•é¡Œé•·åº¦: {len(accumulated_question)} å­—å…ƒ)")
                if round_num > 1:
                    print(f"      æœ¬è¼ªæ–°å¢: {original_question}")
                
                round_result = self._execute_round(
                    question=accumulated_question,  # ğŸ†• ç™¼é€ç´¯ç©å¾Œçš„å®Œæ•´å•é¡Œ
                    session_id=session_id,
                    round_num=round_num,
                    case_type=case_type
                )
                
                result.rounds.append(round_result)
                
                # æ›´æ–° session_id
                if round_result.response and round_result.http_status == 200:
                    session_id = round_result.response.get('session_id')
                    
                    # è¨˜éŒ„æ±ºç­–è³‡è¨Š
                    self._log_decision_info(case_id, round_num, round_result)
                    
                    # åŸ·è¡Œé‚è¼¯è©•ä¼°
                    self._evaluate_round_logic(round_result, case_type)
                    
                    # è¨˜éŒ„è©•ä¼°çµæœ
                    self._log_evaluation_info(case_id, round_num, round_result)
                    
                    # è¼¸å‡ºé—œéµæŒ‡æ¨™
                    self._print_round_metrics(round_result)
                
                # è¼ªæ¬¡é–“éš”
                if round_num < len(rounds):
                    time.sleep(TestConfig.ROUND_INTERVAL)
            
            result.success = True
            result.total_time = time.time() - start_time
            
            # èšåˆçµ±è¨ˆæŒ‡æ¨™
            self._aggregate_metrics(result)
            
            print(f"\nâœ… æ¸¬è©¦æ¡ˆä¾‹å®Œæˆ")
            print(f"   ç¸½è€—æ™‚: {result.total_time:.2f}ç§’")
            print(f"   æˆåŠŸè¼ªæ¬¡: {sum(1 for r in result.rounds if r.response is not None)}/{len(rounds)}")
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
            result.success = False
            result.error_message = "ç”¨æˆ¶ä¸­æ–·æ¸¬è©¦"
            result.total_time = time.time() - start_time
            raise
        except Exception as e:
            result.success = False
            result.error_message = str(e)
            result.total_time = time.time() - start_time
            print(f"\nâŒ æ¸¬è©¦æ¡ˆä¾‹å¤±æ•—")
            print(f"   éŒ¯èª¤: {e}")
        
        return result
    
    def _execute_round(
        self,
        question: str,
        session_id: Optional[str],
        round_num: int,
        case_type: str
    ) -> RoundResult:
        """åŸ·è¡Œå–®è¼ªè¨ºæ–·è«‹æ±‚"""
        round_result = RoundResult(round_num, question)
        
        url = TestConfig.API_BASE_URL + TestConfig.API_DIAGNOSE_ENDPOINT
        payload = {
            'question': question,
            'session_id': session_id,
            'continue_spiral': session_id is not None
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                url,
                json=payload,
                timeout=TestConfig.REQUEST_TIMEOUT
            )
            round_result.response_time = time.time() - start_time
            round_result.http_status = response.status_code
            
            if response.status_code == 200:
                round_result.response = response.json()
                self._extract_metrics_from_response(round_result)
                print(f"   âœ… HTTP 200 | éŸ¿æ‡‰æ™‚é–“: {round_result.response_time:.2f}s")
            elif response.status_code == 422:
                # å®‰å…¨æ””æˆª
                round_result.security_blocked = True
                error_detail = response.json().get('detail', {})
                if isinstance(error_detail, dict):
                    round_result.security_flags = error_detail.get('l1_flags', [])
                print(f"   ğŸ›¡ï¸  HTTP 422 | å®‰å…¨æ””æˆª: {error_detail.get('error', 'SECURITY_BLOCK')}")
            else:
                print(f"   âš ï¸  HTTP {response.status_code}: {response.text[:100]}")
                
        except requests.Timeout:
            print(f"   âš ï¸  è«‹æ±‚è¶…æ™‚ï¼ˆ{TestConfig.REQUEST_TIMEOUT}ç§’ï¼‰")
        except Exception as e:
            print(f"   âš ï¸  è«‹æ±‚éŒ¯èª¤: {e}")
        
        return round_result
    
    def _extract_metrics_from_response(self, round_result: RoundResult):
        """å¾ API å›æ‡‰ä¸­æå– Agentic æ±ºç­–æŒ‡æ¨™"""
        response = round_result.response
        
        # æå– L1 æ±ºç­–è³‡è¨Š
        l1 = response.get('l1', {})
        round_result.l1_overall_confidence = l1.get('overall_confidence')
        round_result.l1_next_action = l1.get('next_action')
        
        retrieval_strategy = l1.get('retrieval_strategy', {})
        round_result.l1_decided_alpha = retrieval_strategy.get('decided_alpha')
        round_result.l1_strategy_type = retrieval_strategy.get('strategy_type')
        round_result.l1_expected_quality = retrieval_strategy.get('expected_quality')
        
        # æå– L2 Agentic è³‡è¨Š
        l2_agentic = response.get('l2_agentic_metadata', {})
        if l2_agentic:
            round_result.l2_case_completeness = l2_agentic.get('case_completeness')
            round_result.l2_diagnosis_confidence = l2_agentic.get('diagnosis_confidence')
            round_result.l2_validation_status = l2_agentic.get('validation_status')
            round_result.l2_tool_calls = l2_agentic.get('tool_calls', 0)
            round_result.l2_confidence_boost = l2_agentic.get('confidence_boost', 0)
        
        # æå–æª¢ç´¢å…ƒæ•¸æ“š
        retrieval_meta = response.get('retrieval_metadata', {})
        if retrieval_meta:
            round_result.retrieval_initial_alpha = retrieval_meta.get('initial_alpha')
            round_result.retrieval_final_alpha = retrieval_meta.get('final_alpha')
            round_result.retrieval_quality_score = retrieval_meta.get('quality_score')
            round_result.retrieval_fallback_triggered = retrieval_meta.get('fallback_triggered', False)
            round_result.retrieval_attempts = retrieval_meta.get('attempts', 1)
    
    def _evaluate_round_logic(self, round_result: RoundResult, case_type: str):
        """åŸ·è¡Œæœ¬è¼ªçš„æ±ºç­–é‚è¼¯è©•ä¼°"""
        # è©•ä¼° L1 Alpha é¸æ“‡
        if round_result.l1_decided_alpha is not None:
            round_result.logic_evaluations['alpha_selection'] = (
                self.evaluator.evaluate_l1_alpha_selection(
                    round_result.question,
                    round_result.l1_decided_alpha,
                    round_result.l1_strategy_type,
                    case_type
                )
            )
        
        # è©•ä¼° L1 ç½®ä¿¡åº¦èˆ‡å‹•ä½œ
        if round_result.l1_overall_confidence is not None:
            round_result.logic_evaluations['confidence_action'] = (
                self.evaluator.evaluate_l1_confidence_action(
                    round_result.l1_overall_confidence,
                    round_result.l1_next_action,
                    len(round_result.question),
                    case_type
                )
            )
        
        # è©•ä¼° L2 å·¥å…·æ±ºç­–
        if round_result.l2_case_completeness is not None or round_result.l2_diagnosis_confidence is not None:
            round_result.logic_evaluations['tool_decision'] = (
                self.evaluator.evaluate_l2_tool_decision(
                    round_result.l2_case_completeness,
                    round_result.l2_diagnosis_confidence,
                    round_result.l2_tool_calls,
                    round_result.l2_validation_status,
                    case_type
                )
            )
        
        # è©•ä¼°æª¢ç´¢å“è³ª
        if round_result.retrieval_quality_score is not None:
            round_result.logic_evaluations['retrieval_quality'] = (
                self.evaluator.evaluate_retrieval_quality(
                    round_result.retrieval_quality_score,
                    round_result.retrieval_fallback_triggered,
                    round_result.retrieval_attempts,
                    round_result.retrieval_final_alpha,
                    round_result.retrieval_initial_alpha
                )
            )
    
    def _log_decision_info(self, case_id: str, round_num: int, round_result: RoundResult):
        """è¨˜éŒ„æ±ºç­–è³‡è¨Šåˆ° JSONL"""
        decision_data = {
            'case_id': case_id,
            'round_num': round_num,
            'question': round_result.accumulated_question,  # ğŸ†• è¨˜éŒ„ç´¯ç©å•é¡Œ
            'original_question': round_result.original_question,  # ğŸ†• è¨˜éŒ„åŸå§‹å•é¡Œ
            'response_time': round_result.response_time,
            'l1': {
                'confidence': round_result.l1_overall_confidence,
                'alpha': round_result.l1_decided_alpha,
                'strategy': round_result.l1_strategy_type,
                'action': round_result.l1_next_action,
                'expected_quality': round_result.l1_expected_quality
            },
            'l2': {
                'case_completeness': round_result.l2_case_completeness,
                'diagnosis_confidence': round_result.l2_diagnosis_confidence,
                'tool_calls': round_result.l2_tool_calls,
                'validation_status': round_result.l2_validation_status,
                'confidence_boost': round_result.l2_confidence_boost
            },
            'retrieval': {
                'initial_alpha': round_result.retrieval_initial_alpha,
                'final_alpha': round_result.retrieval_final_alpha,
                'quality': round_result.retrieval_quality_score,
                'fallback': round_result.retrieval_fallback_triggered,
                'attempts': round_result.retrieval_attempts
            },
            'security': {
                'blocked': round_result.security_blocked,
                'flags': round_result.security_flags
            }
        }
        
        self.logger.log_decision(decision_data)
    
    def _log_evaluation_info(self, case_id: str, round_num: int, round_result: RoundResult):
        """è¨˜éŒ„é‚è¼¯è©•ä¼°çµæœåˆ° JSONL"""
        evaluation_data = {
            'case_id': case_id,
            'round_num': round_num,
            'evaluations': round_result.logic_evaluations
        }
        
        self.logger.log_evaluation(evaluation_data)
    
    def _print_round_metrics(self, round_result: RoundResult):
        """è¼¸å‡ºè¼ªæ¬¡é—œéµæŒ‡æ¨™ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        # L1 æŒ‡æ¨™
        if round_result.l1_decided_alpha is not None:
            print(f"\n   ğŸ¯ L1 æ±ºç­–:")
            print(f"      Alpha: {round_result.l1_decided_alpha:.2f} | "
                  f"ç­–ç•¥: {round_result.l1_strategy_type} | "
                  f"ç½®ä¿¡åº¦: {round_result.l1_overall_confidence:.2f}")
            
            # Alpha è©•ä¼°çµæœ
            if 'alpha_selection' in round_result.logic_evaluations:
                eval_result = round_result.logic_evaluations['alpha_selection']
                status = "âœ…" if eval_result['is_compliant'] else "âŒ"
                score = eval_result['compliance_score']
                print(f"      è©•ä¼°: {status} ç¬¦åˆåº¦ {score:.0%} - {eval_result['reasoning']}")
        
        # L2 æŒ‡æ¨™
        if round_result.l2_tool_calls > 0:
            print(f"\n   ğŸ”§ L2 å·¥å…·:")
            print(f"      èª¿ç”¨æ•¸: {round_result.l2_tool_calls} | "
                  f"é©—è­‰: {round_result.l2_validation_status} | "
                  f"æå‡: +{round_result.l2_confidence_boost:.2f}")
            
            # å·¥å…·æ±ºç­–è©•ä¼°çµæœ
            if 'tool_decision' in round_result.logic_evaluations:
                eval_result = round_result.logic_evaluations['tool_decision']
                status = "âœ…" if eval_result['is_compliant'] else "âŒ"
                score = eval_result['compliance_score']
                print(f"      è©•ä¼°: {status} ç¬¦åˆåº¦ {score:.0%}")
        
        # æª¢ç´¢ Fallback
        if round_result.retrieval_fallback_triggered:
            print(f"\n   ğŸ”„ æª¢ç´¢ Fallback:")
            print(f"      å˜—è©¦: {round_result.retrieval_attempts} æ¬¡ | "
                  f"å“è³ª: {round_result.retrieval_quality_score:.2f}")
    
    def _aggregate_metrics(self, result: TestCaseResult):
        """èšåˆæ¸¬è©¦æ¡ˆä¾‹çš„çµ±è¨ˆæŒ‡æ¨™"""
        metrics = result.metrics
        
        for round_result in result.rounds:
            # å®‰å…¨æŒ‡æ¨™
            if round_result.security_blocked:
                metrics.security_blocks += 1
            elif round_result.response is not None:
                metrics.security_passed += 1
            
            # L1 æŒ‡æ¨™
            if round_result.l1_decided_alpha is not None:
                metrics.alpha_decisions.append(round_result.l1_decided_alpha)
            if round_result.l1_overall_confidence is not None:
                metrics.confidence_scores.append(round_result.l1_overall_confidence)
            if round_result.l1_strategy_type:
                metrics.strategy_types.append(round_result.l1_strategy_type)
            if round_result.l1_next_action == 'ask_more':
                metrics.follow_up_triggered += 1
            elif round_result.l1_next_action == 'vector_search':
                metrics.search_triggered += 1
            
            # L2 æŒ‡æ¨™
            if round_result.l2_tool_calls > 0:
                metrics.total_tool_calls += round_result.l2_tool_calls
                # ç°¡åŒ–çµ±è¨ˆï¼ˆå¯¦éš›æ‡‰å¾è©³ç´°æ—¥èªŒè§£æï¼‰
                metrics.tool_b_calls += 1
            if round_result.l2_validation_status:
                metrics.validation_status_counts[round_result.l2_validation_status] += 1
            if round_result.l2_case_completeness is not None:
                metrics.case_completeness_scores.append(round_result.l2_case_completeness)
            if round_result.l2_diagnosis_confidence is not None:
                metrics.diagnosis_confidence_scores.append(round_result.l2_diagnosis_confidence)
            
            # æª¢ç´¢æŒ‡æ¨™
            if round_result.retrieval_quality_score is not None:
                metrics.quality_scores.append(round_result.retrieval_quality_score)
            if round_result.retrieval_fallback_triggered:
                metrics.fallback_triggered += 1
                metrics.fallback_attempts.append(round_result.retrieval_attempts)
            if round_result.retrieval_initial_alpha and round_result.retrieval_final_alpha:
                adjustment = abs(round_result.retrieval_final_alpha - round_result.retrieval_initial_alpha)
                metrics.alpha_adjustments.append(adjustment)
            
            # é‚è¼¯ç¬¦åˆåº¦
            for eval_type, eval_result in round_result.logic_evaluations.items():
                metrics.logic_checks[eval_type].append({
                    'is_compliant': eval_result.get('is_compliant', False),
                    'compliance_score': eval_result.get('compliance_score', 0.0)
                })


# ============================================
# æ¸¬è©¦å ±å‘Šç”Ÿæˆå™¨
# ============================================

class AgenticTestReporter:
    """Agentic NLU æ¸¬è©¦å ±å‘Šç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_summary_report(all_results: List[TestCaseResult]) -> str:
        """ç”Ÿæˆæ¸¬è©¦æ‘˜è¦å ±å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # åŸºæœ¬çµ±è¨ˆ
        total_cases = len(all_results)
        success_cases = sum(1 for r in all_results if r.success)
        total_rounds = sum(len(r.rounds) for r in all_results)
        total_time = sum(r.total_time for r in all_results)
        
        # æ¡ˆä¾‹é¡å‹åˆ†ä½ˆ
        type_counts = defaultdict(int)
        for result in all_results:
            type_counts[result.case_type] += 1
        
        # èšåˆæ‰€æœ‰æŒ‡æ¨™
        all_metrics = AgenticDecisionMetrics()
        for result in all_results:
            m = result.metrics
            all_metrics.alpha_decisions.extend(m.alpha_decisions)
            all_metrics.confidence_scores.extend(m.confidence_scores)
            all_metrics.strategy_types.extend(m.strategy_types)
            all_metrics.follow_up_triggered += m.follow_up_triggered
            all_metrics.search_triggered += m.search_triggered
            all_metrics.tool_a_calls += m.tool_a_calls
            all_metrics.tool_b_calls += m.tool_b_calls
            all_metrics.tool_c_calls += m.tool_c_calls
            all_metrics.total_tool_calls += m.total_tool_calls
            for status, count in m.validation_status_counts.items():
                all_metrics.validation_status_counts[status] += count
            all_metrics.case_completeness_scores.extend(m.case_completeness_scores)
            all_metrics.diagnosis_confidence_scores.extend(m.diagnosis_confidence_scores)
            all_metrics.quality_scores.extend(m.quality_scores)
            all_metrics.fallback_triggered += m.fallback_triggered
            all_metrics.fallback_attempts.extend(m.fallback_attempts)
            all_metrics.alpha_adjustments.extend(m.alpha_adjustments)
            all_metrics.security_blocks += m.security_blocks
            all_metrics.security_passed += m.security_passed
            for check_type, checks in m.logic_checks.items():
                all_metrics.logic_checks[check_type].extend(checks)
        
        # è¨ˆç®—çµ±è¨ˆå€¼
        def safe_mean(values):
            return statistics.mean(values) if values else 0
        
        def safe_stdev(values):
            return statistics.stdev(values) if len(values) > 1 else 0
        
        avg_alpha = safe_mean(all_metrics.alpha_decisions)
        std_alpha = safe_stdev(all_metrics.alpha_decisions)
        avg_confidence = safe_mean(all_metrics.confidence_scores)
        std_confidence = safe_stdev(all_metrics.confidence_scores)
        avg_quality = safe_mean(all_metrics.quality_scores)
        std_quality = safe_stdev(all_metrics.quality_scores)
        
        # è¨ˆç®—æ±ºç­–é‚è¼¯ç¬¦åˆç‡
        total_checks = sum(len(checks) for checks in all_metrics.logic_checks.values())
        total_compliant = sum(
            sum(1 for c in checks if c['is_compliant'])
            for checks in all_metrics.logic_checks.values()
        )
        overall_compliance_rate = (total_compliant / total_checks * 100) if total_checks > 0 else 0
        
        # è¨ˆç®—å¹³å‡ç¬¦åˆåº¦è©•åˆ†
        total_score = sum(
            sum(c['compliance_score'] for c in checks)
            for checks in all_metrics.logic_checks.values()
        )
        avg_compliance_score = (total_score / total_checks) if total_checks > 0 else 0
        
        # ç”Ÿæˆå ±å‘Š
        report = f"""
{'='*80}
SCBR Agentic NLU æ±ºç­–é‚è¼¯é©—è­‰æ¸¬è©¦å ±å‘Š
Phase 1.5 ç³»çµ±æ¸¬è©¦èˆ‡é©—è­‰
{'='*80}

æ¸¬è©¦æ™‚é–“: {timestamp}
æ¸¬è©¦ç‰ˆæœ¬: v1.0

{'='*80}
ä¸€ã€æ¸¬è©¦åŸ·è¡Œæ‘˜è¦
{'='*80}

1.1 åŸºæœ¬çµ±è¨ˆ
--------------------------------------------------
ç¸½æ¸¬è©¦æ¡ˆä¾‹æ•¸: {total_cases}
æˆåŠŸåŸ·è¡Œæ¡ˆä¾‹: {success_cases} ({success_cases/total_cases*100:.1f}%)
å¤±æ•—æ¡ˆä¾‹æ•¸: {total_cases - success_cases}
ç¸½æ¸¬è©¦è¼ªæ¬¡: {total_rounds}
ç¸½æ¸¬è©¦æ™‚é–“: {total_time:.2f}ç§’
å¹³å‡æ¯æ¡ˆä¾‹æ™‚é–“: {total_time/total_cases:.2f}ç§’

1.2 æ¡ˆä¾‹é¡å‹åˆ†ä½ˆ
--------------------------------------------------
"""
        
        for case_type, count in sorted(type_counts.items()):
            report += f"{case_type}: {count} å€‹æ¡ˆä¾‹\n"
        
        report += f"""
{'='*80}
äºŒã€L1 Agentic Gate æ±ºç­–åˆ†æ
{'='*80}

2.1 Alpha å€¼é¸æ“‡çµ±è¨ˆ
--------------------------------------------------
å¹³å‡ Alpha: {avg_alpha:.3f} Â± {std_alpha:.3f}
Alpha ç¯„åœ: [{min(all_metrics.alpha_decisions) if all_metrics.alpha_decisions else 0:.2f}, {max(all_metrics.alpha_decisions) if all_metrics.alpha_decisions else 0:.2f}]

Alpha åˆ†ä½ˆ:
  ä½å€¼ (â‰¤{TestConfig.ALPHA_LOW_THRESHOLD}, é—œéµå­—ç‚ºä¸»): {sum(1 for a in all_metrics.alpha_decisions if a <= TestConfig.ALPHA_LOW_THRESHOLD)} æ¬¡ ({sum(1 for a in all_metrics.alpha_decisions if a <= TestConfig.ALPHA_LOW_THRESHOLD)/len(all_metrics.alpha_decisions)*100 if all_metrics.alpha_decisions else 0:.1f}%)
  ä¸­å€¼ ({TestConfig.ALPHA_LOW_THRESHOLD}-{TestConfig.ALPHA_HIGH_THRESHOLD}, å‡è¡¡): {sum(1 for a in all_metrics.alpha_decisions if TestConfig.ALPHA_LOW_THRESHOLD < a < TestConfig.ALPHA_HIGH_THRESHOLD)} æ¬¡ ({sum(1 for a in all_metrics.alpha_decisions if TestConfig.ALPHA_LOW_THRESHOLD < a < TestConfig.ALPHA_HIGH_THRESHOLD)/len(all_metrics.alpha_decisions)*100 if all_metrics.alpha_decisions else 0:.1f}%)
  é«˜å€¼ (â‰¥{TestConfig.ALPHA_HIGH_THRESHOLD}, å‘é‡ç‚ºä¸»): {sum(1 for a in all_metrics.alpha_decisions if a >= TestConfig.ALPHA_HIGH_THRESHOLD)} æ¬¡ ({sum(1 for a in all_metrics.alpha_decisions if a >= TestConfig.ALPHA_HIGH_THRESHOLD)/len(all_metrics.alpha_decisions)*100 if all_metrics.alpha_decisions else 0:.1f}%)

2.2 ç½®ä¿¡åº¦è©•ä¼°çµ±è¨ˆ
--------------------------------------------------
å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f} Â± {std_confidence:.3f}
ç½®ä¿¡åº¦ç¯„åœ: [{min(all_metrics.confidence_scores) if all_metrics.confidence_scores else 0:.2f}, {max(all_metrics.confidence_scores) if all_metrics.confidence_scores else 0:.2f}]

ç½®ä¿¡åº¦åˆ†ä½ˆ:
  é«˜ (â‰¥0.75): {sum(1 for c in all_metrics.confidence_scores if c >= 0.75)} æ¬¡ ({sum(1 for c in all_metrics.confidence_scores if c >= 0.75)/len(all_metrics.confidence_scores)*100 if all_metrics.confidence_scores else 0:.1f}%)
  ä¸­ (0.55-0.75): {sum(1 for c in all_metrics.confidence_scores if 0.55 <= c < 0.75)} æ¬¡ ({sum(1 for c in all_metrics.confidence_scores if 0.55 <= c < 0.75)/len(all_metrics.confidence_scores)*100 if all_metrics.confidence_scores else 0:.1f}%)
  ä½ (<{TestConfig.CONFIDENCE_LOW_THRESHOLD}): {sum(1 for c in all_metrics.confidence_scores if c < TestConfig.CONFIDENCE_LOW_THRESHOLD)} æ¬¡ ({sum(1 for c in all_metrics.confidence_scores if c < TestConfig.CONFIDENCE_LOW_THRESHOLD)/len(all_metrics.confidence_scores)*100 if all_metrics.confidence_scores else 0:.1f}%)

2.3 æ±ºç­–å‹•ä½œçµ±è¨ˆ
--------------------------------------------------
åŸ·è¡Œæœç´¢æ¬¡æ•¸: {all_metrics.search_triggered} ({all_metrics.search_triggered/total_rounds*100:.1f}%)
ç”Ÿæˆè¿½å•æ¬¡æ•¸: {all_metrics.follow_up_triggered} ({all_metrics.follow_up_triggered/total_rounds*100:.1f}%)

{'='*80}
ä¸‰ã€L2 Agentic è¨ºæ–·å±¤åˆ†æ
{'='*80}

3.1 å·¥å…·èª¿ç”¨çµ±è¨ˆ
--------------------------------------------------
Tool A (ICD-11) èª¿ç”¨: {all_metrics.tool_a_calls} æ¬¡
Tool B (A+ç™¾ç§‘) èª¿ç”¨: {all_metrics.tool_b_calls} æ¬¡
Tool C (ETCM) èª¿ç”¨: {all_metrics.tool_c_calls} æ¬¡
ç¸½å·¥å…·èª¿ç”¨æ¬¡æ•¸: {all_metrics.total_tool_calls}
å¹³å‡æ¯æ¡ˆä¾‹èª¿ç”¨: {all_metrics.total_tool_calls/total_cases:.2f} æ¬¡

3.2 é©—è­‰ç‹€æ…‹åˆ†ä½ˆ
--------------------------------------------------
"""
        
        if all_metrics.validation_status_counts:
            for status, count in sorted(all_metrics.validation_status_counts.items()):
                report += f"{status}: {count} æ¬¡ ({count/sum(all_metrics.validation_status_counts.values())*100:.1f}%)\n"
        else:
            report += "ç„¡é©—è­‰ç‹€æ…‹æ•¸æ“š\n"
        
        report += f"""
3.3 æ¡ˆä¾‹å®Œæ•´åº¦èˆ‡è¨ºæ–·ç½®ä¿¡åº¦
--------------------------------------------------
å¹³å‡æ¡ˆä¾‹å®Œæ•´åº¦: {safe_mean(all_metrics.case_completeness_scores):.3f}
å¹³å‡è¨ºæ–·ç½®ä¿¡åº¦: {safe_mean(all_metrics.diagnosis_confidence_scores):.3f}

{'='*80}
å››ã€æª¢ç´¢å“è³ªèˆ‡ Fallback æ©Ÿåˆ¶åˆ†æ
{'='*80}

4.1 æª¢ç´¢å“è³ªçµ±è¨ˆ
--------------------------------------------------
å¹³å‡å“è³ªè©•åˆ†: {avg_quality:.3f} Â± {std_quality:.3f}
å“è³ªç¯„åœ: [{min(all_metrics.quality_scores) if all_metrics.quality_scores else 0:.2f}, {max(all_metrics.quality_scores) if all_metrics.quality_scores else 0:.2f}]

å“è³ªåˆ†ä½ˆ:
  å„ªç§€ (â‰¥0.80): {sum(1 for q in all_metrics.quality_scores if q >= 0.80)} æ¬¡ ({sum(1 for q in all_metrics.quality_scores if q >= 0.80)/len(all_metrics.quality_scores)*100 if all_metrics.quality_scores else 0:.1f}%)
  è‰¯å¥½ (0.65-0.80): {sum(1 for q in all_metrics.quality_scores if 0.65 <= q < 0.80)} æ¬¡ ({sum(1 for q in all_metrics.quality_scores if 0.65 <= q < 0.80)/len(all_metrics.quality_scores)*100 if all_metrics.quality_scores else 0:.1f}%)
  ä¸è¶³ (<{TestConfig.QUALITY_ACCEPTABLE}): {sum(1 for q in all_metrics.quality_scores if q < TestConfig.QUALITY_ACCEPTABLE)} æ¬¡ ({sum(1 for q in all_metrics.quality_scores if q < TestConfig.QUALITY_ACCEPTABLE)/len(all_metrics.quality_scores)*100 if all_metrics.quality_scores else 0:.1f}%)

4.2 Fallback æ©Ÿåˆ¶çµ±è¨ˆ
--------------------------------------------------
Fallback è§¸ç™¼æ¬¡æ•¸: {all_metrics.fallback_triggered}
Fallback è§¸ç™¼ç‡: {all_metrics.fallback_triggered/total_rounds*100:.1f}%
å¹³å‡ Fallback å˜—è©¦: {safe_mean(all_metrics.fallback_attempts):.1f} æ¬¡
å¹³å‡ Alpha èª¿æ•´å¹…åº¦: {safe_mean(all_metrics.alpha_adjustments):.3f}

{'='*80}
äº”ã€æ±ºç­–é‚è¼¯ç¬¦åˆåº¦è©•ä¼°ï¼ˆæ ¸å¿ƒæŒ‡æ¨™ï¼‰
{'='*80}

5.1 æ•´é«”ç¬¦åˆåº¦
--------------------------------------------------
ç¸½é‚è¼¯æª¢æŸ¥æ¬¡æ•¸: {total_checks}
ç¬¦åˆé æœŸæ¬¡æ•¸: {total_compliant}
æ±ºç­–é‚è¼¯ç¬¦åˆç‡: {overall_compliance_rate:.1f}%
å¹³å‡ç¬¦åˆåº¦è©•åˆ†: {avg_compliance_score:.2f} / 1.00

5.2 åˆ†é …ç¬¦åˆåº¦çµ±è¨ˆ
--------------------------------------------------
"""
        
        for check_type, checks in sorted(all_metrics.logic_checks.items()):
            if checks:
                compliant_count = sum(1 for c in checks if c['is_compliant'])
                total_count = len(checks)
                avg_score = safe_mean([c['compliance_score'] for c in checks])
                report += f"{check_type}:\n"
                report += f"  ç¬¦åˆç‡: {compliant_count}/{total_count} ({compliant_count/total_count*100:.1f}%)\n"
                report += f"  å¹³å‡è©•åˆ†: {avg_score:.2f}\n"
        
        report += f"""
{'='*80}
å…­ã€å®‰å…¨æ¸¬è©¦çµæœ
{'='*80}

å®‰å…¨æ””æˆªæ¬¡æ•¸: {all_metrics.security_blocks}
æ­£å¸¸é€šéæ¬¡æ•¸: {all_metrics.security_passed}
æ””æˆªç‡: {all_metrics.security_blocks/(all_metrics.security_blocks + all_metrics.security_passed)*100 if (all_metrics.security_blocks + all_metrics.security_passed) > 0 else 0:.1f}%

{'='*80}
ä¸ƒã€æ¸¬è©¦çµè«–èˆ‡å»ºè­°
{'='*80}

7.1 æ¸¬è©¦çµè«–
--------------------------------------------------
"""
        
        # æ ¹æ“šç¬¦åˆç‡çµ¦å‡ºçµè«–
        if overall_compliance_rate >= 85:
            report += """
âœ… å„ªç§€ï¼šAgentic æ±ºç­–é‚è¼¯é‹ä½œè‰¯å¥½ï¼Œé«˜åº¦ç¬¦åˆé æœŸè¨­è¨ˆ

æ ¸å¿ƒå„ªå‹¢ï¼š
- L1 æ±ºç­–é‚è¼¯æº–ç¢ºï¼Œèƒ½å¤ æ ¹æ“šè¼¸å…¥ç‰¹æ€§é¸æ“‡åˆé©ç­–ç•¥
- L2 å·¥å…·èª¿ç”¨æ™‚æ©Ÿæ°ç•¶ï¼Œæœ‰æ•ˆæå‡è¨ºæ–·å“è³ª
- æª¢ç´¢å“è³ªç©©å®šï¼ŒFallback æ©Ÿåˆ¶é‹ä½œæ­£å¸¸
"""
        elif overall_compliance_rate >= 70:
            report += """
âš ï¸  è‰¯å¥½ï¼šAgentic æ±ºç­–é‚è¼¯åŸºæœ¬ç¬¦åˆé æœŸï¼Œå­˜åœ¨æ”¹é€²ç©ºé–“

éœ€è¦é—œæ³¨ï¼š
- éƒ¨åˆ†æ±ºç­–é‚è¼¯èˆ‡é æœŸå­˜åœ¨åå·®
- å»ºè­°é€²è¡Œåƒæ•¸å¾®èª¿å’Œå„ªåŒ–
"""
        else:
            report += """
âŒ å¾…æ”¹é€²ï¼šAgentic æ±ºç­–é‚è¼¯èˆ‡é æœŸå­˜åœ¨è¼ƒå¤§å·®è·

ä¸»è¦å•é¡Œï¼š
- æ±ºç­–é‚è¼¯ç¬¦åˆç‡åä½
- éœ€è¦æ·±å…¥åˆ†æå•é¡Œæ ¹æºä¸¦é€²è¡Œé‡å¤§èª¿æ•´
"""
        
        report += """
7.2 åƒæ•¸èª¿æ•´å»ºè­°
--------------------------------------------------
"""
        
        # åŸºæ–¼çµ±è¨ˆæ•¸æ“šçµ¦å‡ºåƒæ•¸èª¿æ•´å»ºè­°
        if avg_alpha < 0.3:
            report += f"- Alpha å¹³å‡å€¼ {avg_alpha:.2f} åä½ï¼Œå»ºè­°æª¢æŸ¥æ˜¯å¦éåº¦åå¥½é—œéµå­—åŒ¹é…\n"
        elif avg_alpha > 0.7:
            report += f"- Alpha å¹³å‡å€¼ {avg_alpha:.2f} åé«˜ï¼Œå»ºè­°æª¢æŸ¥æ˜¯å¦éåº¦ä¾è³´å‘é‡ç›¸ä¼¼åº¦\n"
        
        if all_metrics.fallback_triggered > total_rounds * 0.3:
            report += f"- Fallback è§¸ç™¼ç‡ {all_metrics.fallback_triggered/total_rounds*100:.1f}% åé«˜ï¼Œå»ºè­°é™ä½å“è³ªé–€æª»æˆ–æ”¹å–„æª¢ç´¢ç­–ç•¥\n"
        elif all_metrics.fallback_triggered < total_rounds * 0.05:
            report += f"- Fallback è§¸ç™¼ç‡ {all_metrics.fallback_triggered/total_rounds*100:.1f}% åä½ï¼Œå»ºè­°æª¢æŸ¥å“è³ªè©•ä¼°æ˜¯å¦éæ–¼å¯¬é¬†\n"
        
        if all_metrics.total_tool_calls > 0:
            tool_call_rate = all_metrics.total_tool_calls / total_rounds
            if tool_call_rate > 0.5:
                report += f"- å·¥å…·èª¿ç”¨é »ç‡ {tool_call_rate:.2f} æ¬¡/è¼ª åé«˜ï¼Œå»ºè­°æé«˜èª¿ç”¨é–€æª»ä»¥æ¸›å°‘é–‹éŠ·\n"
            elif tool_call_rate < 0.1:
                report += f"- å·¥å…·èª¿ç”¨é »ç‡ {tool_call_rate:.2f} æ¬¡/è¼ª åä½ï¼Œå»ºè­°æª¢æŸ¥é–€æª»è¨­å®šæ˜¯å¦éæ–¼åš´æ ¼\n"
        
        report += """
7.3 ä¸‹ä¸€æ­¥è¡Œå‹•å»ºè­°
--------------------------------------------------
"""
        
        if overall_compliance_rate >= 85:
            report += """
âœ… å»ºè­°é€²å…¥ä¸‹ä¸€éšæ®µï¼š
   1. éƒ¨ç½²å°ç¯„åœè©¦é‹è¡Œï¼ˆProduction Pilotï¼‰
   2. æ”¶é›†çœŸå¯¦ç”¨æˆ¶ä½¿ç”¨æ•¸æ“š
   3. æŒçºŒç›£æ§ Agentic æ±ºç­–å“è³ª
   4. å»ºç«‹é•·æœŸå„ªåŒ–æ©Ÿåˆ¶
"""
        elif overall_compliance_rate >= 70:
            report += """
âš ï¸  å»ºè­°å…ˆé€²è¡Œå„ªåŒ–ï¼š
   1. é‡å°ç¬¦åˆç‡è¼ƒä½çš„æ±ºç­–é‚è¼¯é€²è¡Œèª¿æ•´
   2. å¾®èª¿ç›¸é—œé–€æª»åƒæ•¸ï¼ˆåƒè€ƒä¸Šè¿°å»ºè­°ï¼‰
   3. é€²è¡Œé‡é»æ¡ˆä¾‹çš„æ·±å…¥åˆ†æ
   4. å„ªåŒ–å¾Œé‡æ–°åŸ·è¡Œæ¸¬è©¦é©—è­‰
"""
        else:
            report += """
âŒ å»ºè­°æ·±å…¥åˆ†æå•é¡Œï¼š
   1. è©³ç´°å¯©æŸ¥æ±ºç­–é‚è¼¯ä¸ç¬¦åˆçš„å…·é«”æ¡ˆä¾‹
   2. åˆ†æ Prompt è¨­è¨ˆæ˜¯å¦éœ€è¦æ”¹é€²
   3. æª¢æŸ¥é…ç½®åƒæ•¸æ˜¯å¦åˆç†
   4. è€ƒæ…®èª¿æ•´ Agentic æ±ºç­–çš„æ ¸å¿ƒé‚è¼¯
   5. å®Œæˆæ”¹é€²å¾Œé€²è¡Œå…¨é¢é‡æ¸¬
"""
        
        report += f"""
{'='*80}
å…«ã€æ¸¬è©¦æ•¸æ“šèªªæ˜
{'='*80}

8.1 æ•¸æ“šä¾†æº
--------------------------------------------------
- æ¸¬è©¦æ¡ˆä¾‹: {TestConfig.TEST_CASES_FILE}
- æ¸¬è©¦æ™‚é–“: {timestamp}
- æ¸¬è©¦ç’°å¢ƒ: {TestConfig.API_BASE_URL}

8.2 è©³ç´°æ—¥èªŒä½ç½®
--------------------------------------------------
- æ¸¬è©¦è©³æƒ…: {TestConfig.LOG_DIR}/test_details_*.jsonl
- æ±ºç­–è¨˜éŒ„: {TestConfig.LOG_DIR}/agentic_decisions_*.jsonl
- è©•ä¼°çµæœ: {TestConfig.LOG_DIR}/logic_evaluations_*.jsonl

{'='*80}
å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*80}
"""
        
        return report
    
    @staticmethod
    def save_report(report: str, filename: str):
        """ä¿å­˜å ±å‘Šåˆ°æª”æ¡ˆ"""
        filepath = os.path.join(TestConfig.REPORT_DIR, filename)
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\nğŸ“„ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {filepath}")
        except Exception as e:
            print(f"\nâš ï¸  ä¿å­˜å ±å‘Šå¤±æ•—: {e}")


# ============================================
# ä¸»æ¸¬è©¦æµç¨‹
# ============================================

def main():
    """ä¸»æ¸¬è©¦æµç¨‹æ§åˆ¶"""
    print("\n" + "="*80)
    print("SCBR Agentic NLU æ±ºç­–é‚è¼¯é©—è­‰æ¸¬è©¦")
    print("Phase 1.5 ç³»çµ±æ¸¬è©¦èˆ‡é©—è­‰")
    print("="*80 + "\n")
    
    # åˆå§‹åŒ–æ¸¬è©¦åŸ·è¡Œå™¨
    runner = AgenticTestRunner()
    
    # æª¢æŸ¥ API å¥åº·ç‹€æ…‹
    if not runner.check_api_health():
        print("\nâŒ API å¥åº·æª¢æŸ¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒæ¸¬è©¦")
        print("   è«‹ç¢ºèª:")
        print("   1. å¾Œç«¯æœå‹™å·²å•Ÿå‹•")
        print("   2. API ç«¯é»é…ç½®æ­£ç¢º")
        print("   3. ç¶²è·¯é€£æ¥æ­£å¸¸")
        return 1
    
    # è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹
    test_cases_file = TestConfig.TEST_CASES_FILE
    if not os.path.exists(test_cases_file):
        print(f"\nâŒ æ¸¬è©¦æ¡ˆä¾‹æª”æ¡ˆä¸å­˜åœ¨: {test_cases_file}")
        print(f"   è«‹ç¢ºèªæª”æ¡ˆä½æ–¼ç•¶å‰ç›®éŒ„: {os.getcwd()}")
        return 1
    
    try:
        with open(test_cases_file, 'r', encoding='utf-8') as f:
            test_data = yaml.safe_load(f)
            test_cases = test_data.get('test_cases', [])
    except Exception as e:
        print(f"\nâŒ è¼‰å…¥æ¸¬è©¦æ¡ˆä¾‹å¤±æ•—: {e}")
        return 1
    
    if not test_cases:
        print("\nâŒ æ²’æœ‰æ‰¾åˆ°æ¸¬è©¦æ¡ˆä¾‹")
        return 1
    
    print(f"\nğŸ“‹ è¼‰å…¥äº† {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
    print(f"   æ¡ˆä¾‹æª”æ¡ˆ: {test_cases_file}")
    
    # è©¢å•æ˜¯å¦ç¹¼çºŒ
    try:
        user_input = input("\næ˜¯å¦é–‹å§‹æ¸¬è©¦ï¼Ÿ[Y/n]: ").strip().lower()
        if user_input and user_input != 'y':
            print("æ¸¬è©¦å·²å–æ¶ˆ")
            return 0
    except KeyboardInterrupt:
        print("\næ¸¬è©¦å·²å–æ¶ˆ")
        return 0
    
    # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹
    all_results = []
    start_time = time.time()
    
    try:
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n{'#'*80}")
            print(f"é€²åº¦: [{i}/{len(test_cases)}]")
            print(f"{'#'*80}")
            
            result = runner.run_test_case(test_case)
            all_results.append(result)
            
            # æ¸¬è©¦æ¡ˆä¾‹é–“éš”
            if i < len(test_cases):
                time.sleep(TestConfig.CASE_INTERVAL)
                
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        print(f"å·²å®Œæˆ {len(all_results)}/{len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
    
    total_test_time = time.time() - start_time
    
    # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
    print("\n" + "="*80)
    print("ç”Ÿæˆæ¸¬è©¦å ±å‘Š...")
    print("="*80)
    
    report = AgenticTestReporter.generate_summary_report(all_results)
    print(report)
    
    # ä¿å­˜å ±å‘Š
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_filename = f'agentic_test_report_{timestamp}.txt'
    AgenticTestReporter.save_report(report, report_filename)
    
    print(f"\n{'='*80}")
    print("æ¸¬è©¦å®Œæˆ")
    print(f"{'='*80}")
    print(f"ç¸½æ¸¬è©¦æ™‚é–“: {total_test_time:.2f}ç§’")
    print(f"æˆåŠŸæ¡ˆä¾‹: {sum(1 for r in all_results if r.success)}/{len(all_results)}")
    print(f"{'='*80}\n")
    
    return 0


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\næ¸¬è©¦å·²ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)