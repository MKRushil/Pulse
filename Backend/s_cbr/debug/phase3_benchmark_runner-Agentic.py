#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SCBR Phase 3: è«–æ–‡å¯¦é©—æ•¸æ“šè‡ªå‹•åŒ–æ”¶é›†è…³æœ¬ (Benchmark Runner)

ç”¨é€”ï¼š
1. æ‰¹é‡åŸ·è¡ŒçœŸå¯¦è‡¨åºŠæ¡ˆä¾‹ (Real-world Cases)
2. è‡ªå‹•æ¯”å°æ¨¡å‹è¨ºæ–·èˆ‡æ¨™æº–ç­”æ¡ˆ (Ground Truth Evaluation)
3. æ”¶é›†é—œéµå¯¦é©—æŒ‡æ¨™ (Accuracy, Latency, Tool Usage, Alpha)
4. ç”Ÿæˆ CSV æ ¼å¼çš„åŸå§‹æ•¸æ“šä¾›è«–æ–‡ç¹ªåœ–ä½¿ç”¨

è¼¸å‡ºï¼š
- test_results/thesis_experiment_data.csv (åŸå§‹æ•¸æ“š)
- test_results/phase3_summary_report.txt (çµ±è¨ˆå ±å‘Š)
"""

import os
import sys
import json
import yaml
import time
import csv
import requests
from datetime import datetime
from typing import Dict, List, Any, Tuple

# ==================== é…ç½®å€åŸŸ ====================
class BenchmarkConfig:
    # API è¨­å®š
    API_URL = "http://localhost:8000/api/scbr/v2/diagnose"
    TIMEOUT = 240  # 4åˆ†é˜è¶…æ™‚ï¼Œå®¹è¨±å®Œæ•´ Agentic æ¨ç†
    
    # æª”æ¡ˆè·¯å¾‘
    INPUT_FILE = "benchmark_cases_spiral.yaml"  # æ‚¨çš„çœŸå¯¦é†«æ¡ˆåº«
    OUTPUT_CSV = "test_results/thesis_experiment_data-Agentic.csv"
    OUTPUT_REPORT = "test_results/phase3_summary_report-Agentic.txt"
    
    # å¯¦é©—æ¨™ç±¤ (æ¯æ¬¡åŸ·è¡Œå‰è«‹ä¿®æ”¹æ­¤è™•ä»¥å€åˆ†å¯¦é©—çµ„/å°ç…§çµ„)
    # ä¾‹å¦‚: "Agentic_V1.5" æˆ– "Baseline_Traditional"
    EXPERIMENT_TAG = "Agentic_Spiral" 

    @staticmethod
    def ensure_dirs():
        os.makedirs("test_results", exist_ok=True)

# ==================== è©•ä¼°é‚è¼¯ ====================
class DiagnosisEvaluator:
    """è¨ºæ–·æº–ç¢ºåº¦è©•ä¼°å™¨"""
    
    @staticmethod
    def check_correctness(predicted: str, ground_truth: str) -> float:
        """
        è¨ˆç®—è¨ºæ–·æº–ç¢ºåº¦ (0.0 - 1.0)
        æ¡ç”¨é—œéµå­—æ¨¡ç³ŠåŒ¹é…ï¼šåªè¦ Ground Truth çš„æ ¸å¿ƒè­‰å‹å‡ºç¾åœ¨é æ¸¬ä¸­å°±ç®—æ­£ç¢º
        """
        if not predicted or not ground_truth:
            return 0.0
            
        pred_clean = predicted.replace("ï¼ˆ", "(").replace("ï¼‰", ")").strip()
        truth_clean = ground_truth.replace("ï¼ˆ", "(").replace("ï¼‰", ")").strip()
        
        # 1. å®Œå…¨åŒ¹é…
        if pred_clean == truth_clean:
            return 1.0
            
        # 2. æ ¸å¿ƒè­‰å‹åŒ…å«åŒ¹é… (ä¾‹å¦‚ Truth="å¿ƒè„¾å…©è™›", Pred="å¤±çœ (å¿ƒè„¾å…©è™›)")
        # æå–æ‹¬è™Ÿå…§çš„è­‰å‹ï¼Œæˆ–ç›´æ¥æ¯”å°å­—ä¸²
        if truth_clean in pred_clean:
            return 1.0
            
        # 3. éƒ¨åˆ†é—œéµå­—é‡ç–Š (Fuzzy Match)
        # å°‡ Ground Truth æ‹†è§£ç‚ºé—œéµè© (æ’é™¤æ¨™é»)
        keywords = [k for k in truth_clean if k not in "(),ï¼ˆï¼‰ "]
        if not keywords: return 0.0
        
        hit_count = sum(1 for k in keywords if k in pred_clean)
        match_ratio = hit_count / len(keywords)
        
        # é–€æª»ï¼šé‡ç–Šåº¦è¶…é 80% è¦–ç‚ºæ­£ç¢º
        return 1.0 if match_ratio >= 0.8 else 0.0

# ==================== åŸ·è¡Œæ ¸å¿ƒ ====================
class Phase3Runner:
    def __init__(self):
        BenchmarkConfig.ensure_dirs()
        self.results = []
        self.total_cases = 0
        
    def load_cases(self) -> List[Dict]:
        if not os.path.exists(BenchmarkConfig.INPUT_FILE):
            print(f"âŒ æ‰¾ä¸åˆ°æ¸¬è©¦æª”æ¡ˆ: {BenchmarkConfig.INPUT_FILE}")
            sys.exit(1)
            
        with open(BenchmarkConfig.INPUT_FILE, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
            cases = data.get('test_cases', [])
            self.total_cases = len(cases)
            print(f"ğŸ“– å·²è¼‰å…¥ {self.total_cases} å€‹çœŸå¯¦æ¡ˆä¾‹")
            return cases

    def run_diagnosis(self, question: str, session_id: str = None) -> Dict:
        """å‘¼å« API é€²è¡Œè¨ºæ–·"""
        payload = {
            "question": question,
            "session_id": session_id, # [ä¿®æ”¹] ä½¿ç”¨å‚³å…¥çš„ session_id
            "continue_spiral": session_id is not None # [ä¿®æ”¹] è‡ªå‹•åˆ¤æ–·æ˜¯å¦å»¶çºŒå°è©±
        }
        
        start_time = time.time()
        try:
            resp = requests.post(BenchmarkConfig.API_URL, json=payload, timeout=BenchmarkConfig.TIMEOUT)
            latency = time.time() - start_time
            
            if resp.status_code == 200:
                return {"success": True, "data": resp.json(), "latency": latency}
            else:
                return {"success": False, "error": f"HTTP {resp.status_code}", "latency": latency}
                
        except Exception as e:
            return {"success": False, "error": str(e), "latency": time.time() - start_time}

    def execute(self):
        cases = self.load_cases()
        
        # åˆå§‹åŒ– CSV å¯«å…¥
        file_exists = os.path.exists(BenchmarkConfig.OUTPUT_CSV)
        # ä½¿ç”¨ utf-8-sig ä»¥ä¾¿ Excel æ­£ç¢ºé–‹å•Ÿä¸­æ–‡
        csv_file = open(BenchmarkConfig.OUTPUT_CSV, 'a', newline='', encoding='utf-8-sig')
        
        # å®šç¾© CSV æ¬„ä½ (å¢åŠ äº† Session_ID, Question)
        fieldnames = [
            'Experiment_Tag', 'Case_ID', 'Session_ID', 'Round', 'Time', 
            'Question', 'Ground_Truth', 'Predicted_Pattern', 'Is_Correct', 
            'L1_Alpha', 'L1_Strategy', 'L1_Confidence', 
            'Retrieval_Quality', 'Fallback_Triggered',
            'L2_Tool_Calls', 'L2_Confidence_Boost', 'Completeness_Score',
            'Latency_Total', 'Error_Msg'
        ]
        
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        
        if not file_exists:
            writer.writeheader()
            
        print(f"\nğŸš€ é–‹å§‹åŸ·è¡Œ Phase 3 Benchmark ({BenchmarkConfig.EXPERIMENT_TAG})...")
        print(f"   ç›®æ¨™ï¼š{self.total_cases} æ¡ˆä¾‹ (èºæ—‹å¤šè¼ªæ¨¡å¼) | è¼¸å‡º: {BenchmarkConfig.OUTPUT_CSV}")
        print("-" * 80)

        total_correct_cases = 0 # çµ±è¨ˆæœ€çµ‚æ­£ç¢ºçš„æ¡ˆä¾‹æ•¸
        
        for idx, case in enumerate(cases, 1):
            case_id = case['id']
            ground_truth = case['expected_diagnosis']
            session_id = None # âš ï¸ é‡è¦ï¼šæ¯å€‹æ–°æ¡ˆä¾‹é–‹å§‹å‰é‡ç½® Session
            
            print(f"\n[{idx}/{self.total_cases}] æ¡ˆä¾‹ {case_id}: ", end="", flush=True)
            
            case_final_correct = False # è¿½è¹¤æ­¤æ¡ˆä¾‹æœ€å¾Œä¸€è¼ªæ˜¯å¦æ­£ç¢º
            
            # --- èºæ—‹è¼ªæ¬¡è¿´åœˆ ---
            rounds = case.get('rounds', [])
            for round_idx, round_data in enumerate(rounds, 1):
                question = round_data['question']
                
                # åŸ·è¡Œè¨ºæ–· (å‚³å…¥ session_id ä»¥ç¶­æŒä¸Šä¸‹æ–‡)
                result = self.run_diagnosis(question, session_id)
                
                # æº–å‚™åŸºç¤æ•¸æ“š
                row_data = {
                    'Experiment_Tag': BenchmarkConfig.EXPERIMENT_TAG,
                    'Case_ID': case_id,
                    'Session_ID': 'N/A',
                    'Round': round_idx,
                    'Time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'Question': question[:30].replace("\n", " ") + "...",
                    'Ground_Truth': ground_truth,
                    'Latency_Total': round(result['latency'], 2)
                }
                
                if result['success']:
                    data = result['data']
                    
                    # 1. æ›´æ–° Session ID (ä¾›ä¸‹ä¸€è¼ªä½¿ç”¨)
                    session_id = data.get('session_id')
                    row_data['Session_ID'] = session_id
                    
                    # 2. æå–é—œéµæŒ‡æ¨™
                    l1 = data.get('l1', {})
                    l2_meta = data.get('l2_agentic_metadata', {})
                    l2_infer = data.get('l2', {}).get('tcm_inference', {})
                    ret_meta = data.get('retrieval_metadata', {})
                    
                    # 3. ç²å–é æ¸¬çµæœ
                    # å„ªå…ˆå¾ L2 inference æ‹¿ï¼Œå¦‚æœæ²’æœ‰å‰‡æ‹¿ l2 root çš„ primary_pattern
                    predicted = l2_infer.get('primary_pattern') or data.get('l2', {}).get('primary_pattern', 'N/A')
                    
                    # 4. åˆ¤æ–·æº–ç¢ºåº¦
                    is_correct = DiagnosisEvaluator.check_correctness(predicted, ground_truth)
                    
                    # è¨˜éŒ„æœ¬è¼ªæ•¸æ“š
                    row_data.update({
                        'Predicted_Pattern': predicted,
                        'Is_Correct': 1 if is_correct else 0,
                        'L1_Alpha': l1.get('retrieval_strategy', {}).get('decided_alpha'),
                        'L1_Strategy': l1.get('retrieval_strategy', {}).get('strategy_type'),
                        'L1_Confidence': l1.get('overall_confidence'),
                        'Retrieval_Quality': ret_meta.get('quality_score'),
                        'Fallback_Triggered': 1 if ret_meta.get('fallback_triggered') else 0,
                        'L2_Tool_Calls': l2_meta.get('tool_calls', 0),
                        'L2_Confidence_Boost': l2_meta.get('confidence_boost', 0),
                        'Completeness_Score': l2_meta.get('case_completeness', 0),
                        'Error_Msg': ''
                    })
                    
                    # å°å‡ºé€²åº¦ (R1âœ… R2âš ï¸)
                    status_icon = "âœ…" if is_correct else "âš ï¸"
                    print(f"R{round_idx}{status_icon} ", end="", flush=True)
                    
                    # å¦‚æœæ˜¯æœ€å¾Œä¸€è¼ªä¸”æ­£ç¢ºï¼Œå‰‡æ¨™è¨˜æ­¤æ¡ˆä¾‹ç‚ºæˆåŠŸ
                    if round_idx == len(rounds) and is_correct:
                        case_final_correct = True

                else:
                    # è«‹æ±‚å¤±æ•—è™•ç†
                    print(f"R{round_idx}âŒ ", end="", flush=True)
                    row_data['Error_Msg'] = result['error']
                    row_data['Is_Correct'] = 0
                    writer.writerow(row_data)
                    csv_file.flush()
                    break # é€™ä¸€è¼ªå¤±æ•—å°±è·³å‡ºé€™å€‹æ¡ˆä¾‹ï¼Œä¸ç¹¼çºŒè·‘ä¸‹ä¸€è¼ª

                # å¯«å…¥ CSV ä¸¦åˆ·æ–°ç·©è¡å€
                writer.writerow(row_data)
                csv_file.flush()
            
            # è©²æ¡ˆä¾‹æ‰€æœ‰è¼ªæ¬¡çµæŸå¾Œçš„çµ±è¨ˆ
            if case_final_correct:
                total_correct_cases += 1

        # é—œé–‰æª”æ¡ˆä¸¦ç”Ÿæˆå ±å‘Š
        csv_file.close()
        print("\n" + "-" * 80)
        self._generate_report(total_correct_cases)

    def _generate_report(self, correct_count):
        """ç”Ÿæˆæœ¬æ¬¡åŸ·è¡Œçš„çµ±è¨ˆæ‘˜è¦"""
        accuracy = (correct_count / self.total_cases) * 100 if self.total_cases > 0 else 0
        
        report = f"""
================================================
Phase 3 Benchmark åŸ·è¡Œå ±å‘Š
================================================
å¯¦é©—æ¨™ç±¤: {BenchmarkConfig.EXPERIMENT_TAG}
åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç¸½æ¡ˆä¾‹æ•¸: {self.total_cases}
æˆåŠŸè¨ºæ–·: {correct_count} (æº–ç¢ºç‡: {accuracy:.2f}%)
æ•¸æ“šä½ç½®: {BenchmarkConfig.OUTPUT_CSV}
================================================
"""
        print(report)
        with open(BenchmarkConfig.OUTPUT_REPORT, 'a', encoding='utf-8') as f:
            f.write(report + "\n")

if __name__ == "__main__":
    try:
        runner = Phase3Runner()
        runner.execute()
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¸¬è©¦å·²ç”±ä½¿ç”¨è€…ä¸­æ–·")