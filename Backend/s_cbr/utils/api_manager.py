"""
S-CBR API ç®¡ç†å™¨ v1.1 - å–®ä¾‹æ¨¡å¼ä¿®å¾©ç‰ˆ

å®Œæ•´åŠŸèƒ½ï¼š
- å–®ä¾‹æ¨¡å¼é˜²æ­¢é‡è¤‡åˆå§‹åŒ–
- æ™ºèƒ½é€Ÿç‡é™åˆ¶ç®¡ç† (25 RPM ä¿å®ˆè¨­å®š)
- å„ªé›…é™ç´šç­–ç•¥
- å®Œæ•´éŒ¯èª¤è™•ç†
- æ··åˆå‘é‡æœå°‹æ”¯æŒ

ç‰ˆæœ¬ï¼šv1.1-Singleton
"""

import asyncio
import logging
import time
import random
import threading
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
import json
import hashlib
import math
from collections import deque
from enum import Enum

try:
    from openai import AsyncOpenAI
    import weaviate
    from weaviate.auth import AuthApiKey
    import httpx
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    DEPENDENCIES_AVAILABLE = False
    IMPORT_ERROR = str(e)

from s_cbr.config.scbr_config import SCBRConfig

class RequestPriority(Enum):
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

class SCBRAPIManager:
    """S-CBR API ç®¡ç†å™¨ v1.1 - å–®ä¾‹æ¨¡å¼"""
    
    _instance = None
    _lock = threading.Lock()
    _initialized = False
    
    def __new__(cls):
        """ğŸ”¥ é—œéµä¿®å¾©ï¼šå¯¦ç¾å–®ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(SCBRAPIManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ– API ç®¡ç†å™¨ - åªåŸ·è¡Œä¸€æ¬¡"""
        
        # ğŸ”¥ é—œéµä¿®å¾©ï¼šé˜²æ­¢é‡è¤‡åˆå§‹åŒ–
        if self._initialized:
            if hasattr(self, 'logger'):
                self.logger.debug(f"â™»ï¸  API ç®¡ç†å™¨å·²å­˜åœ¨ï¼Œè¤‡ç”¨å¯¦ä¾‹ (ID: {id(self)})")
            return
        
        if not DEPENDENCIES_AVAILABLE:
            raise ImportError(f"ç¼ºå°‘å¿…è¦ä¾è³´: {IMPORT_ERROR}")
        
        # ğŸ”¥ é—œéµä¿®å¾©ï¼šä½¿ç”¨ä¿®å¾©å¾Œçš„æ—¥èªŒå™¨
        from s_cbr.utils.spiral_logger import SpiralLogger
        self.logger = SpiralLogger.get_logger("api_manager")
        
        self.config = SCBRConfig()
        self.version = "1.1-singleton"
        self.instance_id = id(self)
        
        self.logger.info(f"ğŸš€ å‰µå»ºæ–°çš„ API ç®¡ç†å™¨å¯¦ä¾‹ (ID: {self.instance_id})")
        
        # åˆå§‹åŒ–æ‰€æœ‰ç®¡ç†å™¨çµ„ä»¶
        self._init_rate_limiter()
        self._init_batch_processor()
        self._init_priority_manager()
        self._init_retry_config()
        self._init_degradation_manager()
        self._init_health_monitor()
        self._init_statistics()
        
        # åˆå§‹åŒ–å®¢æˆ¶ç«¯
        self._initialize_clients()
        
        # å•Ÿå‹•å¾Œå°æœå‹™
        self._background_tasks = []
        self._start_background_services()
        
        # ğŸ”¥ é—œéµä¿®å¾©ï¼šè¨­ç½®åˆå§‹åŒ–æ¨™è¨˜
        SCBRAPIManager._initialized = True
        
        self.logger.info(f"âœ… API ç®¡ç†å™¨å–®ä¾‹åˆå§‹åŒ–å®Œæˆ (ID: {self.instance_id})")
    
    def _init_rate_limiter(self):
        """åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨"""
        self.rate_limiter = {
            'requests_per_minute': 25,      # é€²ä¸€æ­¥ä¿å®ˆè¨­å®š
            'request_timestamps': deque(),
            'min_interval': 3.0,            # å¢åŠ åˆ°3ç§’é–“éš”
            'last_request_time': 0,
            'burst_allowance': 2,           # æ¸›å°‘çªç™¼å…è¨±
            'current_burst': 0,
            'daily_quota': 800,             # æ¸›å°‘æ¯æ—¥é…é¡
            'daily_used': 0,
            'quota_reset_time': 0
        }
    
    def _init_batch_processor(self):
        """åˆå§‹åŒ–æ‰¹é‡è™•ç†å™¨"""
        self.batch_processor = {
            'pending_tasks': [],
            'batch_size': 2,                # æ¸›å°‘æ‰¹é‡å¤§å°
            'batch_timeout': 10,            # å¢åŠ è¶…æ™‚
            'last_batch_time': 0,
            'batch_stats': {
                'total_batches': 0,
                'total_tasks_processed': 0,
                'api_calls_saved': 0
            }
        }
    
    def _init_priority_manager(self):
        """åˆå§‹åŒ–å„ªå…ˆç´šç®¡ç†å™¨"""
        self.priority_manager = {
            'queues': {
                RequestPriority.HIGH: deque(),
                RequestPriority.MEDIUM: deque(),
                RequestPriority.LOW: deque()
            },
            'processing': False,
            'processor_lock': asyncio.Lock(),
            'stats': {
                'high_processed': 0,
                'medium_processed': 0,
                'low_processed': 0
            }
        }
    
    def _init_retry_config(self):
        """åˆå§‹åŒ–é‡è©¦é…ç½®"""
        self.retry_config = {
            'max_retries': 4,               # æ¸›å°‘é‡è©¦æ¬¡æ•¸
            'base_delay': 5,               # å¢åŠ åŸºç¤å»¶é²
            'max_delay': 120,              # å¢åŠ æœ€å¤§å»¶é²
            'exponential_base': 2.0,
            'jitter': True,
            'jitter_range': 0.3
        }
    
    def _init_degradation_manager(self):
        """åˆå§‹åŒ–é™ç´šç®¡ç†å™¨"""
        self.degradation_manager = {
            'fallback_enabled': True,
            'quality_levels': ['full', 'optimized', 'basic', 'minimal'],
            'current_level': 'full',
            'performance_metrics': {
                'success_rate': 1.0,
                'avg_response_time': 0.0,
                'error_count': 0,
                'last_success_time': time.time()
            }
        }
    
    def _init_health_monitor(self):
        """åˆå§‹åŒ–å¥åº·ç›£æ§"""
        self.health_monitor = {
            'last_health_check': 0,
            'health_check_interval': 600,  # å¢åŠ åˆ°10åˆ†é˜
            'api_health_status': 'unknown',
            'consecutive_failures': 0,
            'service_alerts': []
        }
    
    def _init_statistics(self):
        """åˆå§‹åŒ–çµ±è¨ˆç³»çµ±"""
        self.statistics = {
            'session_start_time': time.time(),
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_wait_time': 0.0,
            'api_quota_efficiency': 0.0,
            'feature_usage': {
                'batch_processing': 0,
                'priority_scheduling': 0,
                'fallback_responses': 0,
                'rate_limit_waits': 0
            }
        }
    
    def _initialize_clients(self):
        """åˆå§‹åŒ–æ‰€æœ‰ API å®¢æˆ¶ç«¯ - å–®ä¾‹ç‰ˆæœ¬"""
        try:
            # æª¢æŸ¥æ˜¯å¦å·²ç¶“åˆå§‹åŒ–éå®¢æˆ¶ç«¯
            if hasattr(self, 'llm_client') and self.llm_client is not None:
                self.logger.info(f"â™»ï¸  API å®¢æˆ¶ç«¯å·²å­˜åœ¨ï¼Œè·³éé‡è¤‡åˆå§‹åŒ– (ID: {self.instance_id})")
                return
            
            self.logger.info(f"ğŸ”§ åˆå§‹åŒ– API å®¢æˆ¶ç«¯ (ID: {self.instance_id})")
            
            # ä½¿ç”¨ä¿®å¾©å¾Œçš„ HTTP å‚³è¼¸å±¤
            transport = httpx.AsyncHTTPTransport(retries=1)
            
            timeout_config = httpx.Timeout(
                connect=45.0,
                read=180.0,
                write=45.0,
                pool=240.0
            )
            
            connection_limits = httpx.Limits(
                max_keepalive_connections=5,   # æ¸›å°‘é€£æ¥æ•¸
                max_connections=10,
                keepalive_expiry=600
            )
            
            # å‰µå»º HTTP å®¢æˆ¶ç«¯
            self.http_client = httpx.AsyncClient(
                transport=transport,
                timeout=timeout_config,
                limits=connection_limits,
                headers={
                    'User-Agent': f'S-CBR-API-Manager-Singleton/{self.version}',
                    'Accept': 'application/json'
                }
            )
            
            # åˆå§‹åŒ– LLM å®¢æˆ¶ç«¯
            self.llm_client = AsyncOpenAI(
                api_key=self.config.LLM_API_KEY,
                base_url=self.config.LLM_API_URL,
                http_client=self.http_client,
                max_retries=0  # æˆ‘å€‘æœ‰è‡ªå·±çš„é‡è©¦é‚è¼¯
            )
            self.llm_model_name = self.config.LLM_MODEL_NAME
            
            # åˆå§‹åŒ– Embedding å®¢æˆ¶ç«¯
            self.embedding_client = AsyncOpenAI(
                api_key=self.config.EMBEDDING_API_KEY,
                base_url=self.config.EMBEDDING_BASE_URL,
                http_client=self.http_client,
                max_retries=0
            )
            self.embedding_model_name = self.config.EMBEDDING_MODEL_NAME
            
            # åˆå§‹åŒ– Weaviate å®¢æˆ¶ç«¯
            auth_config = AuthApiKey(api_key=self.config.WV_API_KEY)
            self.weaviate_client = weaviate.Client(
                url=self.config.WEAVIATE_URL,
                auth_client_secret=auth_config
            )
            
            self.logger.info(f"âœ… API å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆ (å–®ä¾‹æ¨¡å¼ - ID: {self.instance_id})")
            
        except Exception as e:
            self.logger.error(f"âŒ API å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise
    
    # ==================== é€Ÿç‡é™åˆ¶ç®¡ç† ====================
    
    async def _wait_for_rate_limit(self):
        """æ™ºèƒ½é€Ÿç‡é™åˆ¶ç­‰å¾…"""
        current_time = time.time()
        
        # æ¯æ—¥é…é¡é‡ç½®æª¢æŸ¥
        if current_time - self.rate_limiter['quota_reset_time'] > 86400:
            self.rate_limiter['daily_used'] = 0
            self.rate_limiter['quota_reset_time'] = current_time
            self.logger.info("ğŸ”„ æ¯æ—¥APIé…é¡å·²é‡ç½®")
        
        # æª¢æŸ¥æ¯æ—¥é…é¡
        if self.rate_limiter['daily_used'] >= self.rate_limiter['daily_quota']:
            self.logger.error("ğŸš« æ¯æ—¥APIé…é¡å·²ç”¨ç›¡ï¼Œå•Ÿç”¨é™ç´šæ¨¡å¼")
            return
        
        # æ¸…ç†éæœŸçš„è«‹æ±‚è¨˜éŒ„
        cutoff_time = current_time - 60
        while (self.rate_limiter['request_timestamps'] and 
               self.rate_limiter['request_timestamps'][0] <= cutoff_time):
            self.rate_limiter['request_timestamps'].popleft()
        
        # çªç™¼é™åˆ¶æª¢æŸ¥
        if self.rate_limiter['current_burst'] >= self.rate_limiter['burst_allowance']:
            burst_cooldown = 10
            self.logger.warning(f"âš¡ çªç™¼é™åˆ¶è§¸ç™¼ï¼Œå†·å» {burst_cooldown} ç§’")
            await asyncio.sleep(burst_cooldown)
            self.rate_limiter['current_burst'] = 0
            self.statistics['feature_usage']['rate_limit_waits'] += 1
        
        # æ¯åˆ†é˜é™åˆ¶æª¢æŸ¥
        recent_requests = len(self.rate_limiter['request_timestamps'])
        if recent_requests >= self.rate_limiter['requests_per_minute']:
            oldest_request = self.rate_limiter['request_timestamps'][0]
            wait_time = 62 - (current_time - oldest_request)
            
            if wait_time > 0:
                self.logger.warning(f"â³ æ¯åˆ†é˜é™åˆ¶ï¼šç­‰å¾… {wait_time:.1f} ç§’ ({recent_requests}/{self.rate_limiter['requests_per_minute']})")
                self.statistics['total_wait_time'] += wait_time
                await asyncio.sleep(wait_time)
                current_time = time.time()
        
        # æœ€å°é–“éš”æª¢æŸ¥
        time_since_last = current_time - self.rate_limiter['last_request_time']
        if time_since_last < self.rate_limiter['min_interval']:
            wait_time = self.rate_limiter['min_interval'] - time_since_last
            self.logger.debug(f"â±ï¸  é–“éš”æ§åˆ¶ï¼šç­‰å¾… {wait_time:.1f} ç§’")
            await asyncio.sleep(wait_time)
            current_time = time.time()
        
        # è¨˜éŒ„è«‹æ±‚
        self.rate_limiter['request_timestamps'].append(current_time)
        self.rate_limiter['last_request_time'] = current_time
        self.rate_limiter['current_burst'] += 1
        self.rate_limiter['daily_used'] += 1
    
    # ==================== æ ¸å¿ƒ API æ–¹æ³• ====================
    
    async def generate_llm_response(self, prompt: str, agent_config: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆ LLM å›æ‡‰ - å–®ä¾‹ç‰ˆæœ¬"""
        return await self._execute_llm_request_with_retry(prompt, agent_config)
    
    async def generate_high_priority_response(self, prompt: str, agent_config: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆé«˜å„ªå…ˆç´š LLM å›æ‡‰"""
        return await self._execute_llm_request_with_retry(prompt, agent_config)
    
    async def _execute_llm_request_with_retry(self, prompt: str, agent_config: Dict[str, Any] = None) -> str:
        """åŸ·è¡Œ LLM è«‹æ±‚ - å¸¶é‡è©¦æ©Ÿåˆ¶"""
        
        for attempt in range(self.retry_config['max_retries']):
            try:
                # é€Ÿç‡é™åˆ¶æª¢æŸ¥
                await self._wait_for_rate_limit()
                
                config = agent_config or self.config.get_llm_config()
                
                self.logger.info(f"ğŸš€ åŸ·è¡ŒLLMè«‹æ±‚ (å˜—è©¦ {attempt + 1}/{self.retry_config['max_retries']}) - ID: {self.instance_id}")
                
                start_time = time.time()
                
                response = await asyncio.wait_for(
                    self.llm_client.chat.completions.create(
                        model=config.get('model', self.llm_model_name),
                        messages=[
                            {"role": "system", "content": config.get('system_prompt', "ä½ æ˜¯å°ˆæ¥­çš„ä¸­é†«AIåŠ©ç†ã€‚")},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=config.get('temperature', 0.7),
                        max_tokens=config.get('max_tokens', 2000)
                    ),
                    timeout=180  # 3åˆ†é˜è¶…æ™‚
                )
                
                response_time = time.time() - start_time
                
                # æ›´æ–°çµ±è¨ˆ
                self.statistics['total_requests'] += 1
                self.statistics['successful_requests'] += 1
                
                self.logger.info(f"âœ… LLMè«‹æ±‚æˆåŠŸ (è€—æ™‚: {response_time:.2f}s) - ID: {self.instance_id}")
                
                return response.choices[0].message.content
                
            except Exception as e:
                error_str = str(e).lower()
                
                self.statistics['total_requests'] += 1
                self.statistics['failed_requests'] += 1
                
                if attempt == self.retry_config['max_retries'] - 1:
                    self.logger.error(f"âŒ LLMè«‹æ±‚æœ€çµ‚å¤±æ•—: {str(e)}")
                    self.statistics['feature_usage']['fallback_responses'] += 1
                    return self._generate_fallback_response(prompt, f"APIèª¿ç”¨å¤±æ•—: {str(e)[:100]}")
                
                # è¨ˆç®—é‡è©¦å»¶é²
                delay = self._calculate_retry_delay(attempt)
                
                self.logger.warning(f"âš ï¸ LLMè«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}): {str(e)[:100]}")
                self.logger.info(f"â³ {delay:.1f}ç§’å¾Œé‡è©¦...")
                
                await asyncio.sleep(delay)
        
        return self._generate_fallback_response(prompt, "é‡è©¦æ©Ÿåˆ¶ç•°å¸¸")
    
    def _calculate_retry_delay(self, attempt: int) -> float:
        """è¨ˆç®—é‡è©¦å»¶é²"""
        base_delay = self.retry_config['base_delay']
        exponential_delay = base_delay * (self.retry_config['exponential_base'] ** attempt)
        delay = min(exponential_delay, self.retry_config['max_delay'])
        
        if self.retry_config['jitter']:
            jitter = random.uniform(-self.retry_config['jitter_range'], self.retry_config['jitter_range']) * delay
            delay = max(2.0, delay + jitter)
        
        return delay
    
    # ==================== å‘é‡æœå°‹æ–¹æ³• ====================
    
    def _deterministic_vector(self, seed: str, dim: int = 384) -> List[float]:
        """ç”Ÿæˆç¢ºå®šæ€§å‘é‡"""
        out: List[float] = []
        i = 0
        while len(out) < dim:
            h = hashlib.sha256(f"{seed}:{i}".encode("utf-8")).digest()
            for j in range(0, len(h), 4):
                chunk = h[j:j+4]
                if len(chunk) < 4:
                    break
                n = int.from_bytes(chunk, byteorder="big", signed=False)
                out.append((n % 2000000) / 1000000.0 - 1.0)
                if len(out) >= dim:
                    break
            i += 1
        return out

    def _generate_case_compatible_embedding(self, text: str, input_type: str = "passage") -> List[float]:
        """ç”Ÿæˆèˆ‡Caseä¸Šå‚³æ™‚ç›¸å®¹çš„384ç¶­å‘é‡"""
        seed = f"{input_type}|{text}"
        return self._deterministic_vector(seed, dim=384)

    async def search_cases(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æœå°‹CaseçŸ¥è­˜åº«"""
        try:
            query_vector = self._generate_case_compatible_embedding(query, "passage")
            self.logger.info(f"ğŸ” Caseæœå°‹ï¼Œå‘é‡ç¶­åº¦ï¼š{len(query_vector)} - ID: {self.instance_id}")
            
            result = self.weaviate_client.query.get(
                "Case",
                ["case_id", "chief_complaint", "summary_text", "diagnosis_main", 
                 "age", "gender", "pulse_text", "present_illness", "provisional_dx"]
            ).with_near_vector({
                "vector": query_vector
            }).with_limit(limit).with_additional(['certainty']).do()
            
            cases = []
            if result.get("data", {}).get("Get", {}).get("Case"):
                for item in result["data"]["Get"]["Case"]:
                    case_data = {
                        'case_id': item.get('case_id', ''),
                        'chief_complaint': item.get('chief_complaint', ''),
                        'summary_text': item.get('summary_text', ''),
                        'diagnosis_main': item.get('diagnosis_main', ''),
                        'age': item.get('age', ''),
                        'gender': item.get('gender', ''),
                        'pulse_text': item.get('pulse_text', ''),
                        'present_illness': item.get('present_illness', ''),
                        'provisional_dx': item.get('provisional_dx', ''),
                        'similarity': item.get('_additional', {}).get('certainty', 0.0)
                    }
                    cases.append(case_data)
            
            self.logger.info(f"âœ… Caseæœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(cases)} å€‹ç›¸é—œæ¡ˆä¾‹")
            return cases
            
        except Exception as e:
            self.logger.error(f"Caseæœå°‹å¤±æ•—: {str(e)}")
            return []

    async def get_embeddings_v1(self, text: str, input_type: str = "query") -> List[float]:
        """ç²å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            if not text or not text.strip():
                return self._get_default_embedding()
            
            text = text[:8192] if len(text) > 8192 else text
            
            response = await self.embedding_client.embeddings.create(
                input=[text],
                model=self.embedding_model_name,
                encoding_format="float",
                extra_body={
                    "modality": ["text"],
                    "input_type": input_type,
                    "truncate": "NONE"
                }
            )
            
            embedding = response.data[0].embedding
            self.logger.debug(f"âœ… ç²å– {input_type} embeddingæˆåŠŸï¼Œç¶­åº¦ï¼š{len(embedding)}")
            return embedding
            
        except Exception as e:
            self.logger.error(f"Embeddingèª¿ç”¨å¤±æ•—: {str(e)}")
            return self._generate_text_based_embedding(text)

    async def search_pulse_knowledge(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """æœå°‹PulsePJçŸ¥è­˜åº«"""
        try:
            query_vector = await self.get_embeddings_v1(query, input_type="query")
            
            result = self.weaviate_client.query.get(
                "PulsePJ",
                ["name", "description", "main_disease", "symptoms", "category"]
            ).with_near_vector({
                "vector": query_vector
            }).with_limit(limit).with_additional(['certainty']).do()
            
            pulse_knowledge = []
            if result.get("data", {}).get("Get", {}).get("PulsePJ"):
                for item in result["data"]["Get"]["PulsePJ"]:
                    pulse_data = {
                        'name': item.get('name', ''),
                        'description': item.get('description', ''),
                        'main_disease': item.get('main_disease', ''),
                        'symptoms': item.get('symptoms', ''),
                        'category': item.get('category', ''),
                        'similarity': item.get('_additional', {}).get('certainty', 0.0)
                    }
                    pulse_knowledge.append(pulse_data)
            
            self.logger.info(f"PulsePJæœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(pulse_knowledge)} å€‹ç›¸é—œè„ˆè¨ºçŸ¥è­˜")
            return pulse_knowledge
            
        except Exception as e:
            self.logger.error(f"PulsePJæœå°‹å¤±æ•—: {str(e)}")
            return []

    async def comprehensive_search(self, query: str, search_options: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç¶œåˆæœå°‹"""
        try:
            search_options = search_options or {}
            case_limit = search_options.get('case_limit', 10)
            pulse_limit = search_options.get('pulse_limit', 5)
            
            case_results, pulse_results = await asyncio.gather(
                self.search_cases(query, case_limit),
                self.search_pulse_knowledge(query, pulse_limit)
            )
            
            all_similarities = []
            if case_results:
                all_similarities.extend([c.get('similarity', 0) for c in case_results])
            if pulse_results:
                all_similarities.extend([p.get('similarity', 0) for p in pulse_results])
            
            overall_similarity = max(all_similarities) if all_similarities else 0.0
            
            return {
                'cases': case_results,
                'pulse_knowledge': pulse_results,
                'total_cases_found': len(case_results),
                'total_pulse_found': len(pulse_results),
                'best_case': case_results[0] if case_results else None,
                'best_pulse': pulse_results[0] if pulse_results else None,
                'overall_similarity': overall_similarity,
                'search_success': True,
                'api_manager_id': self.instance_id,
                'query': query,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ç¶œåˆæœå°‹å¤±æ•—: {str(e)}")
            return {
                'cases': [], 'pulse_knowledge': [], 'total_cases_found': 0,
                'total_pulse_found': 0, 'best_case': None, 'best_pulse': None,
                'overall_similarity': 0.0, 'search_success': False,
                'error': str(e), 'query': query, 'timestamp': datetime.now().isoformat()
            }

    # ==================== å¾Œå°æœå‹™ ====================
    
    def _start_background_services(self):
        """å•Ÿå‹•å¾Œå°æœå‹™ - é˜²æ­¢é‡è¤‡å•Ÿå‹•"""
        
        # ğŸ”¥ é—œéµä¿®å¾©ï¼šæª¢æŸ¥æ˜¯å¦å·²å•Ÿå‹•å¾Œå°æœå‹™
        if hasattr(self, '_services_started') and self._services_started:
            self.logger.info(f"â™»ï¸  å¾Œå°æœå‹™å·²å•Ÿå‹•ï¼Œè·³éé‡è¤‡å•Ÿå‹• (ID: {self.instance_id})")
            return
        
        # å•Ÿå‹•å¾Œå°æœå‹™ï¼ˆé »ç‡èª¿æ•´ï¼‰
        self._background_tasks.append(
            asyncio.create_task(self._statistics_reporter())
        )
        
        self._services_started = True
        self.logger.info(f"ğŸ”„ å¾Œå°æœå‹™å·²å•Ÿå‹• (å–®ä¾‹æ¨¡å¼ - ID: {self.instance_id})")
    
    async def _statistics_reporter(self):
        """çµ±è¨ˆå ±å‘Šæœå‹™ - é™ä½é »ç‡"""
        while True:
            try:
                await asyncio.sleep(300)  # æ”¹ç‚º5åˆ†é˜å ±å‘Šä¸€æ¬¡
                await self._generate_statistics_report()
            except Exception as e:
                self.logger.error(f"çµ±è¨ˆå ±å‘Šæœå‹™ç•°å¸¸: {str(e)}")
                await asyncio.sleep(600)
    
    async def _generate_statistics_report(self):
        """ç”Ÿæˆçµ±è¨ˆå ±å‘Š"""
        uptime = time.time() - self.statistics['session_start_time']
        
        total_requests = self.statistics['total_requests']
        success_rate = (
            (self.statistics['successful_requests'] / total_requests * 100) 
            if total_requests > 0 else 100
        )
        
        self.logger.info("ğŸ“Š === S-CBR API ç®¡ç†å™¨çµ±è¨ˆå ±å‘Š (å–®ä¾‹æ¨¡å¼) ===")
        self.logger.info(f"ğŸ†” å¯¦ä¾‹ID: {self.instance_id}")
        self.logger.info(f"â±ï¸  é‹è¡Œæ™‚é–“: {uptime/3600:.1f}å°æ™‚")
        self.logger.info(f"ğŸ“ˆ è«‹æ±‚çµ±è¨ˆ: ç¸½è¨ˆ{total_requests}, æˆåŠŸç‡{success_rate:.1f}%")
        self.logger.info(f"ğŸ¯ æ¯æ—¥é…é¡: {self.rate_limiter['daily_used']}/{self.rate_limiter['daily_quota']}")
    
    # ==================== å·¥å…·æ–¹æ³• ====================
    
    def _generate_fallback_response(self, prompt: str, reason: str = "APIèª¿ç”¨å¤±æ•—") -> str:
        """æ™ºèƒ½å‚™ç”¨å›æ‡‰ç”Ÿæˆ"""
        keywords = prompt.lower()
        
        if any(word in keywords for word in ['å¤±çœ ', 'ç¡çœ ', 'å¤šå¤¢', 'å…¥ç¡å›°é›£', 'é ­ç—›']):
            return f"""ğŸ”„ **S-CBR v1.1 å–®ä¾‹æ¨¡å¼åˆ†æ** (ID: {self.instance_id})

ğŸ¯ **åŸºæ–¼ç—‡ç‹€çš„ä¸­é†«åˆ†æ**ï¼š

**å¤±çœ é ­ç—›ç¶œåˆåˆ†æ**ï¼š
- å¸¸è¦‹è­‰å‹ï¼šè‚é™½ä¸Šäº¢ã€å¿ƒè…ä¸äº¤ã€æ°£è¡€ä¸è¶³
- ç—…æ©Ÿç‰¹é»ï¼šæƒ…å¿—ä¸é‚ã€å‹ç´¯éåº¦ã€æ€æ…®éå¤š
- ç™¼ç—…è¦å¾‹ï¼šå¤šèˆ‡å·¥ä½œå£“åŠ›ã€ç”Ÿæ´»ç¯€å¥ç›¸é—œ

**æ²»ç™‚å»ºè­°**ï¼š
- æ²»æ³•ï¼šå¹³è‚æ½›é™½ï¼Œé¤Šå¿ƒå®‰ç¥
- æ–¹è—¥ï¼šå¤©éº»é‰¤è—¤é£²ã€ç”˜éº¥å¤§æ£—æ¹¯åŠ æ¸›
- é‡ç¸ï¼šç™¾æœƒã€å››ç¥è°ã€ç¥é–€ã€å¤ªè¡ç­‰ç©´

**ç”Ÿæ´»èª¿ç†**ï¼š
- ä½œæ¯è¦å¾‹ï¼Œé¿å…ç†¬å¤œ
- é©åº¦é‹å‹•ï¼Œå¦‚æ•£æ­¥ã€å¤ªæ¥µ
- æƒ…ç·’ç®¡ç†ï¼Œå­¸æœƒæ”¾é¬†

âš ï¸ **é‡è¦æé†’**ï¼šè«‹è«®è©¢å°ˆæ¥­ä¸­é†«å¸«é€²è¡Œè©³ç´°è¨ºæ–·

ğŸ”„ **ç³»çµ±ç‹€æ…‹**ï¼š{reason} | å–®ä¾‹IDï¼š{self.instance_id} | å»ºè­°ç¨å¾Œé‡è©¦ç²å¾—å®Œæ•´åˆ†æ"""
        
        return f"""ğŸ”„ **S-CBR v1.1 å–®ä¾‹æ¨¡å¼** (ID: {self.instance_id})

ğŸ¯ **ä¸­é†«è¾¨è­‰è¦é»**ï¼š
1. è©³ç´°è¨˜éŒ„ç—‡ç‹€çš„æ™‚é–“ã€ç¨‹åº¦ã€èª˜å› 
2. æ³¨æ„è§€å¯ŸèˆŒè±¡ã€è„ˆè±¡è®ŠåŒ–
3. çµåˆæ—¢å¾€ç—…å²å’Œç”¨è—¥æƒ…æ³
4. è€ƒæ…®æƒ…ç·’ã€é£²é£Ÿã€ä½œæ¯ç­‰å› ç´ 

ğŸ’¡ **ç³»çµ±å„ªå‹¢**ï¼š
- å–®ä¾‹æ¨¡å¼ç¢ºä¿è³‡æºçµ±ä¸€ç®¡ç†
- æ™ºèƒ½é€Ÿç‡é™åˆ¶ä¿è­‰æœå‹™ç©©å®š
- 25 RPMä¿å®ˆé…é¡è¨­è¨ˆ
- å„ªé›…é™ç´šç­–ç•¥ä¿è­‰æœå‹™é€£çºŒæ€§

ğŸ”„ **ç³»çµ±ç‹€æ…‹**ï¼š{reason}
ğŸ†” **å¯¦ä¾‹ID**ï¼š{self.instance_id}
â±ï¸  **å»ºè­°æ“ä½œ**ï¼šç¨å¾Œé‡è©¦ç²å¾—å®Œæ•´èºæ—‹æ¨ç†åˆ†æ

âš ï¸ **é‡è¦**ï¼šè«‹è«®è©¢å°ˆæ¥­ä¸­é†«å¸«é€²è¡Œæº–ç¢ºè¨ºæ–·

*S-CBR v1.1 å–®ä¾‹å„ªåŒ–ç‰ˆ - ç©©å®šé«˜æ•ˆçš„æ™ºèƒ½æœå‹™*"""
    
    def _get_default_embedding(self) -> List[float]:
        return [0.01] * 1024
    
    def _generate_text_based_embedding(self, text: str) -> List[float]:
        hash_obj = hashlib.md5(text.encode('utf-8'))
        hash_hex = hash_obj.hexdigest()
        embedding = []
        for i in range(1024):
            hash_val = int(hash_hex[i % len(hash_hex)], 16) / 15.0 - 0.5
            embedding.append(hash_val)
        return embedding
    
    # ==================== å–®ä¾‹ç®¡ç†æ–¹æ³• ====================
    
    @classmethod
    def get_instance(cls):
        """ç²å–å–®ä¾‹å¯¦ä¾‹"""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    @classmethod
    def reset_instance(cls):
        """é‡ç½®å–®ä¾‹å¯¦ä¾‹ (åƒ…ç”¨æ–¼æ¸¬è©¦)"""
        with cls._lock:
            if cls._instance is not None:
                # æ¸…ç†è³‡æº
                if hasattr(cls._instance, 'http_client'):
                    try:
                        asyncio.create_task(cls._instance.http_client.aclose())
                    except:
                        pass
            cls._instance = None
            cls._initialized = False
    
    def get_client_info(self) -> Dict[str, Any]:
        """ç²å–å®¢æˆ¶ç«¯è³‡è¨Š"""
        return {
            "version": self.version,
            "instance_id": self.instance_id,
            "optimization_level": "singleton_mode",
            "rate_limit": f"{self.rate_limiter['requests_per_minute']} RPM",
            "daily_quota": f"{self.rate_limiter['daily_used']}/{self.rate_limiter['daily_quota']}",
            "initialized_at": datetime.now().isoformat()
        }

# ==================== ä¾¿æ·å‡½æ•¸ ====================

def get_api_manager():
    """ç²å– API ç®¡ç†å™¨å–®ä¾‹å¯¦ä¾‹"""
    return SCBRAPIManager.get_instance()

# åŒ¯å‡º
__all__ = ["SCBRAPIManager", "get_api_manager"]
