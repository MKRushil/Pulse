# -*- coding: utf-8 -*-
"""
å››å±¤æ¨ç†å¼•æ“æ ¸å¿ƒç®¡ç·š (FourLayerSCBR)

è·è²¬ï¼šå”èª¿ L1 (Gate) -> æª¢ç´¢ -> L2 (Diagnosis) -> L3 (Review) -> L4 (Presentation)
çš„æ•¸æ“šæµå’Œé‚è¼¯åˆ¤æ–·ã€‚

æ ¸å¿ƒä¿®å¾©ï¼š
1. ç¢ºä¿ L1 Gate çš„æ‹’çµ•ç‹€æ…‹èƒ½å¤ æ­£ç¢ºè¿”å›çµ¦ main.py é€²è¡Œ 422 è™•ç†ã€‚
2. å°‡ L2, L3, L4 çš„ LLM èª¿ç”¨å¤±æ•—æ”¹ç‚ºæ‹‹å‡ºå—æ§ç•°å¸¸ï¼Œè®“ä¸» Engine è™•ç†ç‚º 500 Internal Server Errorã€‚
3. ğŸš¨ æ–¹æ¡ˆä¸‰å¯¦è£ï¼šL1 éšæ®µå¼•å…¥ TCMTools é€²è¡ŒçœŸæ­£çš„å¤–éƒ¨å·¥å…·æŸ¥è©¢å¢å¼·ã€‚
"""

from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
import json
import asyncio 
import re # å¼•å…¥ re è™•ç† JSON å®¹éŒ¯

# å‡è¨­å­˜åœ¨é€™äº›æ¨¡çµ„
from ..llm.client import LLMClient
from ..config import SCBRConfig
from ..utils.logger import get_logger
from ..security.owasp_mapper import OWASPMapper 
from ..llm.embedding import EmbedClient
from .search_engine import SearchEngine 
from .agentic_retrieval import AgenticRetrieval
from .l2_agentic_diagnosis import L2AgenticDiagnosis
from ..utils.terminology_manager import TerminologyManager

# [MODIFIED] å¼•å…¥å·¥å…·åº« (æ–¹æ¡ˆä¸‰å¿…è¦)
from ..tools.tcm_tools import TCMTools

logger = get_logger("FourLayerPipeline")

# æ˜¯å¦å•Ÿç”¨æª¢ç´¢çµæœç˜¦èº«ï¼ˆé è¨­åœç”¨ï¼Œèµ° raw å„ªå…ˆï¼‰
USE_RETRIEVAL_SLIMMING = False


def _read_prompt(path: Path) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def _classify_domain(text: str) -> str:
    """æ¥µç°¡é ˜åŸŸåˆ†é¡ï¼šdigestive / gyne / generalã€‚"""
    if not text:
        return "general"
    d_words = ["èƒƒ", "è„˜", "è„¹", "å™¯æ°£", "å—³æ°£", "æ—©é£½", "è„¾èƒƒ", "é£Ÿæ…¾ä¸æŒ¯"]
    g_words = ["å¸¶ä¸‹", "ç™½å¸¶", "é™°é“", "æœˆç¶“", "ç¶“æœŸ", "å©¦ç§‘"]
    for w in d_words:
        if w in text:
            return "digestive"
    for w in g_words:
        if w in text:
            return "gyne"
    return "general"


async def call_llm_with_prompt(llm: LLMClient, prompt_path: Path, payload: Dict[str, Any], temperature: float = 0.0) -> Dict[str, Any]:
    """
    è¼‰å…¥å°æ‡‰ .txt promptï¼Œå½¢æˆ system æŒ‡ç¤º + user payloadï¼Œå‘¼å« LLMã€‚
    """
    system_prompt = _read_prompt(prompt_path)
    
    resp = await llm.complete_json(system_prompt=system_prompt, user_prompt=payload, temperature=temperature) 

    if isinstance(resp, dict):
        return resp
    # ç°¡åŒ– JSON å®¹éŒ¯è™•ç†
    if isinstance(resp, str):
        try:
            return json.loads(resp)
        except Exception:
            import re
            m = re.search(r"\{.*\}", resp, re.DOTALL)
            if not m:
                raise ValueError("LLM éŸ¿æ‡‰ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸”ç„¡æ³•æå– JSON å€å¡Š") 
            return json.loads(m.group(0))
    raise TypeError(f"LLM éŸ¿æ‡‰é¡å‹éŒ¯èª¤: {type(resp)}")


# ç‚ºäº†åœ¨æ—¥èªŒä¸­æå–åˆ†æ•¸ï¼Œå®šç¾© _score_of å‡½å¼
def _score_of(hit: Dict[str, Any]) -> float:
    """å¾æª¢ç´¢çµæœä¸­æå–åˆ†æ•¸ï¼Œå…¼å®¹ _additional å’Œ score çµæ§‹"""
    try:
        add = hit.get("_additional", {}) if isinstance(hit.get("_additional"), dict) else {}
        # å„ªå…ˆä½¿ç”¨ score/distanceï¼Œå…¶æ¬¡ä½¿ç”¨ _final_score (SearchEngine æœƒæ­£è¦åŒ–)
        return float(add.get("score") or hit.get("_final_score") or 0.0)
    except Exception:
        return 0.0


class FourLayerSCBR:
    """å››å±¤é †åºåŸ·è¡Œæ§åˆ¶å™¨ã€‚"""

    def __init__(self, llm: LLMClient, config: Optional[SCBRConfig] = None, search_engine: Optional[SearchEngine] = None, embed_client: Optional[EmbedClient] = None):
        self.llm = llm
        self.cfg = config
        self.SE = search_engine or (SearchEngine(self.cfg) if self.cfg else None)
        self.embed = embed_client or (EmbedClient(self.cfg) if self.cfg else None)
        # ğŸ†• åˆå§‹åŒ– Agentic æª¢ç´¢å™¨
        self.agentic_enabled = (
            self.cfg.agentic_nlu.enabled 
            if self.cfg and hasattr(self.cfg, 'agentic_nlu') 
            else False
        )
        if self.agentic_enabled and self.SE and self.embed:
            self.agentic_retrieval = AgenticRetrieval(
                search_engine=self.SE,
                embed_client=self.embed,
                config=self.cfg
            )
        else:
            self.agentic_retrieval = None
        
        # [MODIFIED] åˆå§‹åŒ– TCMTools å·¥å…·åº« (ç”¨æ–¼ L1 å¢å¼·)
        self.tools = TCMTools() 
        logger.info("[FourLayerPipeline] TCMTools å·¥å…·åº«å·²æ›è¼‰")
        
        # ğŸ†• åˆå§‹åŒ– L2 Agentic è¨ºæ–·å™¨
        if self.agentic_enabled and self.cfg:
            try:
                self.l2_agentic = L2AgenticDiagnosis(config=self.cfg, search_engine=self.SE,embed_client=self.embed)
                logger.info("[L2Agentic] åˆå§‹åŒ–å®Œæˆ (å«å…§éƒ¨çŸ¥è­˜åº«é€£ç·š)")
            except Exception as e:
                logger.warning(f"[L2Agentic] åˆå§‹åŒ–å¤±æ•—: {e}ï¼Œå°‡é™ç´šç‚ºå‚³çµ± L2 æ¨¡å¼")
                self.l2_agentic = None
        else:
            self.l2_agentic = None
            if not self.agentic_enabled:
                logger.info("[L2] Agentic æ¨¡å¼æœªå•Ÿç”¨ï¼Œä½¿ç”¨å‚³çµ± L2 æ¨¡å¼")
        
        self.term_manager = TerminologyManager()
        self.base_dir = Path(__file__).resolve().parents[1]
        self.prompts_dir = self.base_dir / "prompts"

    async def run_once(
        self, 
        user_query: str, 
        history_summary: str | None = None, 
        disable_case_slimming: Optional[bool] = None, 
        round_count: int = 1, 
        max_rounds: int = 7,
        previous_diagnosis: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        debug_notes: List[str] = []
        
        # 1. åˆå§‹åŒ– Result çµæ§‹
        result = {
            "l1": {}, "l2": {}, "l3": {}, "l4": {}, 
            "diagnosis": {}, "converged": False, "security_checks": {}, "is_forced_convergence": False
        }
        
        # ==================== L1: é–€ç¦å±¤ (Gate Layer) ====================
        # ğŸ†• æ ¹æ“šé…ç½®é¸æ“‡ L1 Prompt
        if self.agentic_enabled:
            l1_prompt_file = "l1_gate_agentic_prompt.txt"
            l1_payload = {
                "layer": "L1_AGENTIC_GATE",
                "input": {"user_query": user_query, "history_summary": history_summary or ""}
            }
            logger.info("[L1] ä½¿ç”¨ Agentic NLU æ¨¡å¼")
        else:
            l1_prompt_file = "l1_gate_prompt.txt"
            l1_payload = {
                "layer": "L1_GATE",
                "input": {"user_query": user_query, "history_summary": history_summary or ""}
            }
            logger.info("[L1] ä½¿ç”¨å‚³çµ±æ¨¡å¼")
        
        # ğŸš¨ L1 å¯¦éš› LLM èª¿ç”¨ (ä½¿ç”¨æº«åº¦ 0.0 æˆ– Agentic æº«åº¦)
        l1_temperature = (
            self.cfg.agentic_nlu.llm_temperature 
            if self.agentic_enabled and self.cfg 
            else 0.0
        )
        l1 = await call_llm_with_prompt(
            self.llm, 
            self.prompts_dir / l1_prompt_file, 
            l1_payload, 
            temperature=l1_temperature
        )
        result['l1'] = l1
        
        logger.info(f"[L1 FINAL RESULT] L1 ç‹€æ…‹: {l1.get('status', 'N/A')}")
        
        # ğŸ†• è¨˜éŒ„ Agentic æ±ºç­–ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.agentic_enabled:
            logger.info(
                f"[L1 AGENTIC DECISION]\n"
                f"  Overall Confidence: {l1.get('overall_confidence', 0.0):.3f}\n"
                f"  Decided Alpha: {l1.get('retrieval_strategy', {}).get('decided_alpha', 0.5)}\n"
                f"  Strategy Type: {l1.get('retrieval_strategy', {}).get('strategy_type', 'N/A')}\n"
                f"  Expected Quality: {l1.get('retrieval_strategy', {}).get('expected_quality', 'N/A')}"
            )
        else:
            # å‚³çµ±æ¨¡å¼è¨˜éŒ„
            logger.info(f"[L1 KEYWORD PLAN]\n{json.dumps(l1.get('keyword_plan', {}), indent=2, ensure_ascii=False)}")

        # èˆŠç‰ˆæ—¥èªŒçš„ L1 BEFORE/AFTER FILTER é‚è¼¯ï¼ˆä¿ç•™ï¼‰
        try:
            raw = getattr(self.llm, "_last_raw_output", None)
            flt = getattr(self.llm, "_last_filtered_output", None)
            is_l1 = getattr(self.llm, "_last_is_l1", False)
            if is_l1 and raw and flt:
                def _pp(s: str) -> str:
                    try:
                        return json.dumps(json.loads(s), ensure_ascii=False, indent=2)
                    except Exception:
                        return s
                logger.info("[L1 BEFORE FILTER]\n%s", _pp(raw))
                logger.info("[L1 AFTER  FILTER]\n%s", _pp(flt))
        except Exception:
            pass

        # =================================================================
        # ğŸ†• [æ–°å¢] L1 ç­–ç•¥å¾®èª¿ (åŸºæ–¼æœ¬åœ°è©åº«çš„ Hybrid ä¿®æ­£)
        # =================================================================
        if self.agentic_enabled and l1.get("status") == "ok":
            try:
                # 1. æ”¶é›† L1 æå–çš„æ‰€æœ‰é—œéµå­—
                extracted_terms = []
                kw_data = l1.get("keyword_extraction", {})
                extracted_terms.extend(kw_data.get("symptom_terms", []))
                extracted_terms.extend(kw_data.get("tongue_pulse_terms", []))
                
                # 2. è¨ˆç®—ã€Œè¡“èªå¯†åº¦ã€ (æœ‰å¤šå°‘æ¯”ä¾‹æ˜¯å·²çŸ¥æ¨™æº–è©)
                density = self.term_manager.get_density(extracted_terms)
                
                # 3. ç­–ç•¥è‡ªå‹•ä¿®æ­£ (Auto-Correction)
                # è¦å‰‡ï¼šå¦‚æœ 50% ä»¥ä¸Šæ˜¯æ¨™æº–è¡“èªï¼Œä¸”ç›®å‰ Alpha > 0.4 (é Keyword Focus)ï¼Œå¼·åˆ¶é™è½‰
                current_strategy = l1.get("retrieval_strategy", {})
                current_alpha = current_strategy.get("decided_alpha", 0.5)
                
                if density >= 0.5 and current_alpha > 0.4:
                    logger.info(f"ğŸ”§ [L1 Correction] æª¢æ¸¬åˆ°é«˜å¯†åº¦æ¨™æº–è¡“èª ({density:.0%})ï¼Œå¼·åˆ¶èª¿æ•´ Alpha: {current_alpha} -> 0.3")
                    
                    # ä¿®æ”¹ L1 çš„æ±ºç­–çµæœ (In-place modification)
                    if "retrieval_strategy" not in l1: l1["retrieval_strategy"] = {}
                    
                    l1["retrieval_strategy"]["decided_alpha"] = 0.3
                    l1["retrieval_strategy"]["strategy_type"] = "keyword_focus_forced"
                    
                    # è¨˜éŒ„ä¿®æ­£åŸå› ï¼Œæ–¹ä¾¿å¾ŒçºŒé™¤éŒ¯
                    original_reason = l1["retrieval_strategy"].get("reasoning", "")
                    l1["retrieval_strategy"]["reasoning"] = (
                        f"{original_reason} (ç³»çµ±æª¢æ¸¬åˆ° {density:.0%} æ¨™æº–è¡“èªï¼Œå·²ç”±æœ¬åœ°è©åº«å¼·åˆ¶ä¿®æ­£ç­–ç•¥)"
                    )
            except Exception as e:
                logger.warning(f"âš ï¸ L1 ç­–ç•¥ä¿®æ­£åŸ·è¡Œå¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {e}")
        
        # ğŸš¨ L1 æª¢æŸ¥é» (é—œéµé»ï¼šå°‡æ‹’çµ•é‚è¼¯è¿”å›çµ¦ main.py è™•ç†)
        if l1.get("status") == "reject" or l1.get("next_action") == "reject":
            logger.warning(f"ğŸ›¡ï¸ L1 é–€ç¦æª¢æ¸¬åˆ°å¨è„…ï¼Œé˜»æ­¢å¾ŒçºŒæ¨ç†ã€‚ç‹€æ…‹: {l1.get('status')}")
            result['security_checks']['l1_flags'] = l1.get('owasp_screening', {}).get('flags', [])
            return result # è¿”å›çµ¦ main.py æ‹‹å‡º 422 HTTPException

        # =================================================================
        # ğŸ†• [æ–¹æ¡ˆä¸‰ä¿®æ­£ç‰ˆ] L1 å¤–éƒ¨å·¥å…·ä»‹å…¥ (Tool-Assisted Query Enrichment)
        # =================================================================
        # æ ¸å¿ƒé‚è¼¯ï¼šå¦‚æœ L1 ä¿¡å¿ƒä¸è¶³ (< 0.4)ï¼Œå…ˆç”¨ LLM è½‰è­¯ï¼Œå†èª¿ç”¨å·¥å…·
        l1_conf = l1.get("overall_confidence", 0.0)
        user_query_text = user_query
        
        if self.agentic_enabled and l1_conf < 0.4:
            logger.info(f"ğŸ”§ [L1 Enhancement] æª¢æ¸¬åˆ°ç›´æ•˜å¥/ä¿¡å¿ƒä¸è¶³ ({l1_conf})ï¼Œå•Ÿå‹•å¤–éƒ¨å·¥å…·å¢å¼·æ¨¡å¼...")
            
            try:
                # [FIX] æ­¥é©Ÿ 1: å…ˆè®“ LLM æ‰®æ¼”ã€Œç¿»è­¯å®˜ã€ï¼Œå°‡é•·å¥è½‰ç‚º 1-2 å€‹æ ¸å¿ƒæœå°‹è©
                # é€™è§£æ±ºäº† "å¤–éƒ¨å·¥å…·æŸ¥è©¢ç„¡çµæœ" çš„å•é¡Œ
                extraction_prompt = (
                    f"è«‹å¾ä»¥ä¸‹æ‚£è€…æè¿°ä¸­ï¼Œæå–æœ€æ ¸å¿ƒçš„ä¸€å€‹ã€Œä¸­é†«ç—…åã€æˆ–ã€Œä¸»ç—‡è¡“èªã€ç”¨æ–¼æª¢ç´¢ç™¾ç§‘ã€‚\n"
                    f"æ‚£è€…æè¿°ï¼š{user_query}\n"
                    f"è¦æ±‚ï¼šåªè¼¸å‡ºä¸€å€‹è©ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚ç¯„ä¾‹ï¼šã€Œç”¢å¾Œç¼ºä¹³ã€ã€ã€Œå¤±çœ ã€ã€‚"
                )
                search_term = await self.llm.chat_complete(
                    system_prompt="ä½ æ˜¯ä¸€å€‹ç²¾æº–çš„ä¸­é†«é—œéµè©æå–å™¨ã€‚",
                    user_prompt=extraction_prompt
                )
                search_term = search_term.strip().replace("ã€‚", "")
                logger.info(f"ğŸ”§ [L1 Translation] é•·å¥è½‰è­¯ -> æœå°‹è©: {search_term}")

                # [FIX] æ­¥é©Ÿ 2: ä½¿ç”¨è½‰è­¯å¾Œçš„é—œéµè©å»æŸ¥å·¥å…· (A+ç™¾ç§‘)
                loop = asyncio.get_event_loop()
                tool_content = await loop.run_in_executor(
                    None, 
                    self.tools.tool_b_syndrome_logic, 
                    search_term # é€™è£¡å‚³å…¥çŸ­è©ï¼Œå·¥å…·å°±èƒ½æ‰¾åˆ°äº†ï¼
                )
                
                # æ­¥é©Ÿ 3: å¾å·¥å…·å›å‚³çš„è±å¯ŒçŸ¥è­˜ä¸­ï¼Œæå–æ›´å¤šæ“´å……é—œéµå­—
                if tool_content and "æœªæ‰¾åˆ°" not in tool_content:
                    enrichment_prompt = (
                        f"åƒè€ƒä»¥ä¸‹ä¸­é†«çŸ¥è­˜ï¼Œç‚ºç—‡ç‹€ '{search_term}' æå– 3-5 å€‹ç›¸é—œçš„ä¸­é†«è¾¨è­‰é—œéµå­—(å¦‚è­‰å‹ã€ç—…æ©Ÿ)ã€‚"
                        f"åªè¼¸å‡ºé—œéµå­—ï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚\n\nçŸ¥è­˜å…§å®¹ï¼š{tool_content[:500]}"
                    )
                    enriched_terms = await self.llm.chat_complete(
                        system_prompt="ä½ æ˜¯ä¸€å€‹ä¸­é†«è¡“èªæ“´å……å™¨ã€‚",
                        user_prompt=enrichment_prompt
                    )
                    
                    logger.info(f"ğŸ”§ [Tool Result] çŸ¥è­˜åº«æ“´å……æˆåŠŸ -> å¢å¼·è¡“èª: {enriched_terms}")
                    user_query_text = f"{user_query} {enriched_terms}"
                    
                    # æ¨™è¨˜å¢å¼·
                    if "retrieval_strategy" in l1:
                        l1["retrieval_strategy"]["reasoning"] += " (å·²ç”± A+ç™¾ç§‘å·¥å…·å¢å¼·è¡“èª)"
                    # =====================================================
                    # ğŸš¨ [CRITICAL FIX] å¼·åˆ¶è¦†è“‹ L1 çš„ä¸‹ä¸€æ­¥æ±ºç­–
                    # =====================================================
                    # åŸæœ¬ L1 å› ç‚ºä¿¡å¿ƒä½å¯èƒ½å›å‚³ "ask_more"ï¼Œå°è‡´å¾Œé¢æª¢ç´¢å€å¡Šè¢«è·³éã€‚
                    # ç¾åœ¨æ—¢ç„¶å·²ç¶“å¢å¼·äº†é—œéµå­—ï¼Œæˆ‘å€‘å°±å¼·åˆ¶ç³»çµ±é€²è¡Œå‘é‡æª¢ç´¢ã€‚
                    l1["next_action"] = "vector_search" 
                    logger.info("ğŸ”§ [L1 Override] å·²å¼·åˆ¶å°‡ next_action ä¿®æ”¹ç‚º 'vector_search'")
                                    
            except Exception as e:
                logger.warning(f"âš ï¸ å·¥å…·å¢å¼·åŸ·è¡Œå¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {e}")

        # ------------------- æ­£å¸¸æµç¨‹ -------------------
        
        # 2. æª¢ç´¢å±¤ (Retrieval Layer)
        cases: List[Dict] = []
        retrieval_metadata = {}
        
        if l1.get("next_action") == "vector_search":
            if not self.SE or not self.embed:
                logger.error("âŒ SearchEngine æˆ– EmbedClient æœªåˆå§‹åŒ–ï¼Œç„¡æ³•é€²è¡Œæª¢ç´¢ã€‚")
                return result 
            
            # [MODIFIED] ä½¿ç”¨ç¶“éå·¥å…·å¢å¼·çš„ user_query_text
            text_query = user_query_text
            
            # ğŸ†• æ ¹æ“šæ¨¡å¼é¸æ“‡æª¢ç´¢æ–¹å¼
            if self.agentic_enabled and self.agentic_retrieval:
                # === Agentic æ™ºèƒ½æª¢ç´¢æ¨¡å¼ ===
                logger.info("[RETRIEVAL] ä½¿ç”¨ Agentic æ™ºèƒ½æª¢ç´¢")
                
                try:
                    # åŸ·è¡Œæ™ºèƒ½æª¢ç´¢ï¼ˆåŒ…å«å‹•æ…‹ alphaã€å“è³ªè©•ä¼°ã€è‡ªå‹• fallbackï¼‰
                    retrieval_result = await self.agentic_retrieval.intelligent_search(
                        index="TCMCase",
                        text=text_query,
                        l1_strategy=l1.get("retrieval_strategy", {}),
                        limit=3
                    )
                    
                    cases = retrieval_result.get("cases", [])
                    retrieval_metadata = retrieval_result.get("metadata", {})
                    
                    # è¨˜éŒ„ Agentic æª¢ç´¢æ±ºç­–
                    logger.info(
                        f"[AGENTIC RETRIEVAL]\n"
                        f"  åˆå§‹ Alpha: {retrieval_metadata.get('initial_alpha', 0.0):.2f}\n"
                        f"  æœ€çµ‚ Alpha: {retrieval_metadata.get('final_alpha', 0.0):.2f}\n"
                        f"  å˜—è©¦æ¬¡æ•¸: {retrieval_metadata.get('attempts', 0)}\n"
                        f"  å“è³ªè©•åˆ†: {retrieval_metadata.get('quality_score', 0.0):.3f}\n"
                        f"  Fallback: {'æ˜¯' if retrieval_metadata.get('fallback_triggered') else 'å¦'}"
                    )
                    
                except Exception as e:
                    logger.error(f"âŒ Agentic æª¢ç´¢å¤±æ•—: {e}", exc_info=True)
                    cases = []
                    
            else:
                # === å‚³çµ±æª¢ç´¢æ¨¡å¼ ===
                logger.info("[RETRIEVAL] ä½¿ç”¨å‚³çµ±æª¢ç´¢æ¨¡å¼")
                
                # 1. ç²å–æŸ¥è©¢å‘é‡
                try:
                    vector = await self.embed.embed(text_query) 
                except Exception as e:
                     logger.warning(f"âš ï¸ å‘é‡ç”Ÿæˆå¤±æ•—ï¼Œå˜—è©¦ç´” BM25: {e}")
                     vector = None
                    
                # 2. åŸ·è¡Œæ··åˆæª¢ç´¢
                try:
                    cases = await self.SE.hybrid_search(
                        index="TCMCase", 
                        text=text_query, 
                        vector=vector, 
                        alpha=0.55 if vector else 1.0, 
                        limit=3,
                        search_fields=["full_text"] 
                    )
                except Exception as e:
                    logger.error(f"âŒ æª¢ç´¢å¤±æ•—: {e}", exc_info=True)
                    cases = []

        # ğŸš¨ æ—¥èªŒé» 2ï¼šæª¢ç´¢çµæœæ‘˜è¦
        log_samples = []
        if cases:
            log_samples = [
                {"case_id": c.get("case_id", "N/A"), "score": f"{_score_of(c):.4f}"}
                for c in cases[:3] 
            ]
        logger.info(f"[RETRIEVAL RESULT] æˆåŠŸæ‰¾åˆ° {len(cases)} å€‹æ¡ˆä¾‹. Top 3 ç¯„ä¾‹: {log_samples}")

        # ğŸ†• å°‡æª¢ç´¢å…ƒæ•¸æ“šæ·»åŠ åˆ°çµæœä¸­
        if retrieval_metadata:
            result['retrieval_metadata'] = retrieval_metadata

        if not cases:
            debug_notes.append("Retrieval returned zero cases.")
            result["debug_note"] = "; ".join(debug_notes)
            # return result 

        
        # 3. L2: ç”Ÿæˆå±¤ (Diagnosis Layer)
        l2_raw_result = {}
        
        # [MODIFIED] æ ¹æ“šæ¨¡å¼é¸æ“‡åŸ·è¡Œè·¯å¾‘
        if self.agentic_enabled and self.l2_agentic:
            logger.info("[L2] ä½¿ç”¨ Agentic å¢å¼·æ¨¡å¼ (v2.3 å…¨è¨—ç®¡æµç¨‹)")
            
            # åŸ·è¡Œå…¨è¨—ç®¡è¨ºæ–· (åŒ…å« é–å®šéŒ¨å®š -> æ¨ç† -> å…§éƒ¨çŸ¥è­˜æª¢ç´¢ -> å·¥å…·èª¿ç”¨ -> ç¶œåˆ)
            # é€™è£¡å‘¼å«çš„æ˜¯æˆ‘å€‘å‰›åœ¨ l2_agentic_diagnosis.py ä¸­æ›´æ–°çš„ diagnose_with_tools
            agentic_result = await self.l2_agentic.diagnose_with_tools(
                user_query=user_query,
                retrieved_cases=cases,
                l1_decision=l1
            )
            
            # [é—œéµ] å°‡ Agentic çš„æœ€çµ‚è¨ºæ–· (Final Diagnosis) é‡æ§‹ç‚ºç³»çµ±é€šç”¨çš„ l2_raw_result æ ¼å¼
            # é€™æ¨£ L3 (å®‰å…¨å¯©æ ¸) å’Œ L4 (å‘ˆç¾) æ‰èƒ½çœ‹åˆ°è¢« Agentic ä¿®æ­£éçš„é«˜å“è³ªå…§å®¹
            final_diag = agentic_result.get("final_diagnosis", {})
            metrics = agentic_result.get("metrics", {})
            tool_outputs = agentic_result.get("tool_outputs", {})
            
            # é‡å»º l2_raw_result çµæ§‹
            l2_raw_result = {
                "tcm_inference": {
                    "primary_pattern": final_diag.get("primary_syndrome", "æœªå®š"),
                    "pathogenesis": final_diag.get("pathogenesis", ""),
                    "treatment_principle": final_diag.get("treatment_principle", ""),
                    # é€™è£¡å°‡åŒ…å« 'ç™¼ç¾ç–‘é»...' çš„ reasoning æ³¨å…¥ï¼Œè®“ L4 å‘ˆç¾çµ¦ç”¨æˆ¶çœ‹
                    "syndrome_analysis": final_diag.get("reasoning", "") 
                },
                "coverage_evaluation": {
                    "coverage_ratio": metrics.get("case_completeness", 0.0),
                    "missing_info": []
                },
                "selected_case": {
                    # å˜—è©¦å¾ initial_diagnosis æ‹¿å›éŒ¨å®šè³‡è¨Šï¼Œè‹¥ç„¡å‰‡æ¨™è¨˜ç‚º Agentic åˆæˆ
                    "case_id": agentic_result.get("initial_diagnosis", {}).get("anchored_case_id", "Agentic_Synthesized"),
                    "diagnosis": "Agentic Optimization"
                },
                "knowledge_supplements": final_diag.get("knowledge_supplements", [])
            }

            # å¡«å…… result çµæ§‹
            result['l2'] = l2_raw_result
            result['l2_agentic_metadata'] = {
                "validation_status": "validated" if tool_outputs else "unvalidated",
                "tool_calls": len(tool_outputs),
                "confidence_boost": 0.15 if tool_outputs else 0.0,
                "case_completeness": metrics.get("case_completeness", 0.0),
                "diagnosis_confidence": metrics.get("final_confidence", 0.0)
            }
            
            # å°‡å·¥å…·è¼¸å‡ºå‚³éçµ¦ result (ä¾›å‰ç«¯æˆ–é™¤éŒ¯ä½¿ç”¨)
            if tool_outputs:
                result['l2']['tool_outputs'] = tool_outputs

            logger.info(
                f"[L2 AGENTIC COMPLETE]\n"
                f"  æœ€çµ‚è¨ºæ–·: {l2_raw_result['tcm_inference']['primary_pattern']}\n"
                f"  å·¥å…·èª¿ç”¨: {len(tool_outputs)}\n"
                f"  åŒ…å«ç–‘é»åˆ†æ: {'æ˜¯' if 'ç–‘é»' in l2_raw_result['tcm_inference']['syndrome_analysis'] else 'å¦'}"
            )

        else:
            # === å‚³çµ±æ¨¡å¼ ===
            logger.info("[L2] ä½¿ç”¨å‚³çµ±æ¨¡å¼ (ç„¡ Agentic)")
            l2_payload = {
                "layer": "L2_CASE_ANCHORED_DIAGNOSIS",
                "input": {
                    "user_accumulated_query": user_query,
                    "retrieved_cases": cases,
                    "round_count": round_count,
                    "previous_diagnosis": previous_diagnosis if previous_diagnosis else {}
                }
            }
            l2_raw_result = await call_llm_with_prompt(
                self.llm, 
                self.prompts_dir / "l2_case_anchored_diagnosis_prompt.txt", 
                l2_payload, 
                temperature=0.1
            )
            result['l2'] = l2_raw_result

        # ğŸš¨ [æ—¥èªŒé» 3: L2 æ¡ˆä¾‹éŒ¨å®šæ‘˜è¦]
        selected_case_id = l2_raw_result.get("selected_case", {}).get("case_id", "æœªéŒ¨å®š")
        coverage = l2_raw_result.get("coverage_evaluation", {}).get("coverage_ratio", 0.0)
        primary_pattern = l2_raw_result.get('tcm_inference', {}).get('primary_pattern', 'N/A')
        
        logger.info(
            f"[L2 DIAGNOSIS SUMMARY] éŒ¨å®š ID: {selected_case_id}, è­‰å‹: {primary_pattern}, "
            f"è¦†è“‹åº¦: {coverage:.2f}"
        )

        # 4. L3: å¯©æ ¸å±¤ (Safety Review Layer)
        l3_payload = {"layer": "L3_SAFETY_REVIEW", "input": {"diagnosis_payload": l2_raw_result}}
        # ğŸš¨ L3 å¯¦éš› LLM èª¿ç”¨ (ä½¿ç”¨æº«åº¦ 0.0)
        l3_result = await call_llm_with_prompt(self.llm, self.prompts_dir / "l3_safety_review_prompt.txt", l3_payload, temperature=0.0)
        result['l3'] = l3_result
        
        # ğŸš¨ [æ—¥èªŒé» 4: L3 å®‰å…¨å¯©æ ¸çµæœ]
        logger.info(f"[L3 REVIEW STATUS] å¯©æ ¸çµæœ: {l3_result.get('status', 'N/A')}")
            
        # ğŸš¨ L3 æª¢æŸ¥é»
        if l3_result.get('status') == 'rejected':
            logger.warning("ğŸ›¡ï¸ L3 å¯©æ ¸æ‹’çµ•è¼¸å‡ºã€‚")
            result['security_checks']['l3_violations'] = l3_result.get('violations', [])
            return result # è¿”å›çµ¦ main.py è™•ç† 422 HTTPException

        # 5. L4: å‘ˆç¾å±¤ (Presentation Layer)
        safe_diagnosis = l3_result.get('safe_diagnosis_payload', {})
        l4_payload = {
            "layer": "L4_PRESENTATION", 
            "input": {
                "safe_diagnosis_payload": safe_diagnosis, 
                "round_count": round_count, 
                "max_rounds": max_rounds,
                "previous_diagnosis": previous_diagnosis if previous_diagnosis else {}
            }
        }
        # ğŸš¨ L4 å¯¦éš› LLM èª¿ç”¨ (ä½¿ç”¨æº«åº¦ 0.1)
        l4_result = await call_llm_with_prompt(self.llm, self.prompts_dir / "l4_presentation_prompt.txt", l4_payload, temperature=0.1)
        result['l4'] = l4_result
        
        # [FIX] å‰ç«¯é˜²å´©æ½°è™•ç†ï¼šç¢ºä¿ diagnosis æ˜¯å­—ä¸²
        presentation = l4_result.get('presentation', "")
        
        if isinstance(presentation, dict):
            # å¦‚æœ L4 å›å‚³çš„æ˜¯çµæ§‹åŒ–ç‰©ä»¶ (ä¾‹å¦‚åŒ…å« title/content)ï¼Œå„ªå…ˆå–å…§å®¹
            if "content" in presentation:
                result['diagnosis'] = presentation["content"]
            elif "message" in presentation:
                result['diagnosis'] = presentation["message"]
            else:
                # å¦å‰‡å°‡æ•´å€‹å­—å…¸è½‰ç‚ºæ˜“è®€çš„å­—ä¸²
                lines = []
                for k, v in presentation.items():
                    # éæ¿¾æ‰éå¿…è¦çš„ metadata
                    if k not in ["type", "status"]:
                        lines.append(f"**{k}**: {v}")
                result['diagnosis'] = "\n\n".join(lines)
        elif isinstance(presentation, list):
            result['diagnosis'] = "\n".join([str(x) for x in presentation])
        else:
            # å·²ç¶“æ˜¯å­—ä¸²æˆ– None
            result['diagnosis'] = str(presentation) if presentation else "è¨ºæ–·ç”Ÿæˆç•°å¸¸ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
        
        # æª¢æŸ¥æ”¶æ–‚ (ä¾æ“š SCBR æ–‡ä»¶ [10.2] çš„æ”¶æ–‚æ¢ä»¶)
        coverage_ratio = l2_raw_result.get('coverage_evaluation', {}).get('coverage_ratio', 0.0)
        # ä¿®æ­£æ”¶æ–‚åˆ¤æ–·é‚è¼¯ï¼Œç´å…¥æœ€å¤§è¼ªæ¬¡æª¢æŸ¥ (å¼·åˆ¶æ”¶æ–‚)
        is_coverage_ok = coverage_ratio >= 0.95
        # é€™è£¡çš„é‚è¼¯å¿…é ˆå’Œ main.py å…§éƒ¨çš„ should_converge é‚è¼¯ä¿æŒä¸€è‡´
        result['converged'] = is_coverage_ok 

        return result