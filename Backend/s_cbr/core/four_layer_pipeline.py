# -*- coding: utf-8 -*-
"""
å››å±¤æ¨ç†å¼•æ“æ ¸å¿ƒç®¡ç·š (FourLayerSCBR)

è·è²¬ï¼šå”èª¿ L1 (Gate) -> æª¢ç´¢ -> L2 (Diagnosis) -> L3 (Review) -> L4 (Presentation)
çš„æ•¸æ“šæµå’Œé‚è¼¯åˆ¤æ–·ã€‚

æ ¸å¿ƒä¿®å¾©ï¼š
1. ç¢ºä¿ L1 Gate çš„æ‹’çµ•ç‹€æ…‹èƒ½å¤ æ­£ç¢ºè¿”å›çµ¦ main.py é€²è¡Œ 422 è™•ç†ã€‚
2. å°‡ L2, L3, L4 çš„ LLM èª¿ç”¨å¤±æ•—æ”¹ç‚ºæ‹‹å‡ºå—æ§ç•°å¸¸ï¼Œè®“ä¸» Engine è™•ç†ç‚º 500 Internal Server Errorã€‚
"""

from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
import json
import asyncio # å¼•å…¥ asyncio

# å‡è¨­å­˜åœ¨é€™äº›æ¨¡çµ„
from ..llm.client import LLMClient
from ..config import SCBRConfig
from ..utils.logger import get_logger
from ..security.owasp_mapper import OWASPMapper 
from ..llm.embedding import EmbedClient
from .search_engine import SearchEngine # å‡è¨­ SearchEngine å­˜åœ¨

logger = get_logger("FourLayerPipeline")

# æ˜¯å¦å•Ÿç”¨æª¢ç´¢çµæœç˜¦èº«ï¼ˆé è¨­åœç”¨ï¼Œèµ° raw å„ªå…ˆï¼‰
USE_RETRIEVAL_SLIMMING = False


def _read_prompt(path: Path) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def _classify_domain(text: str) -> str:
    """æ¥µç°¡é ˜åŸŸåˆ†é¡ï¼šdigestive / gyne / generalã€‚"""
    if not text:
        return "general"
    d_words = ["èƒƒ", "è„˜", "è„¹", "å™¯æ°£", "å—³æ°£", "æ—©é£½", "è„¾èƒƒ", "é£Ÿæ…¾ä¸æŒ¯"]
    g_words = ["å¸¶ä¸‹", "ç™½å¸¶", "é™°é“", "æœˆç¶“", "ç¶“æœŸ", "å©¦ç§‘"]
    for w in d_words:
        if w in text:
            return "digestive"
    for w in g_words:
        if w in text:
            return "gyne"
    return "general"


async def call_llm_with_prompt(llm: LLMClient, prompt_path: Path, payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    è¼‰å…¥å°æ‡‰ .txt promptï¼Œå½¢æˆ system æŒ‡ç¤º + user payloadï¼Œå‘¼å« LLMã€‚
    """
    system_prompt = _read_prompt(prompt_path)
    resp = await llm.complete_json(system_prompt=system_prompt, user_prompt=payload)

    if isinstance(resp, dict):
        return resp
    # ç°¡åŒ– JSON å®¹éŒ¯è™•ç†
    if isinstance(resp, str):
        try:
            return json.loads(resp)
        except Exception:
            import re
            m = re.search(r"\{.*\}", resp, re.DOTALL)
            if not m:
                raise ValueError("LLM éŸ¿æ‡‰ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸”ç„¡æ³•æå– JSON å€å¡Š") # <-- L2/L3/L4 å¤±æ•—æ™‚æ‹‹å‡ºéŒ¯èª¤
            return json.loads(m.group(0))
    raise TypeError(f"LLM éŸ¿æ‡‰é¡å‹éŒ¯èª¤: {type(resp)}") # <-- L2/L3/L4 å¤±æ•—æ™‚æ‹‹å‡ºéŒ¯èª¤


class FourLayerSCBR:
    """å››å±¤é †åºåŸ·è¡Œæ§åˆ¶å™¨ã€‚"""

    def __init__(self, llm: LLMClient, config: Optional[SCBRConfig] = None, search_engine: Optional[SearchEngine] = None, embed_client: Optional[EmbedClient] = None):
        self.llm = llm
        self.cfg = config
        self.SE = search_engine or (SearchEngine(self.cfg) if self.cfg else None)
        self.embed = embed_client or (EmbedClient(self.cfg) if self.cfg else None)
        self.base_dir = Path(__file__).resolve().parents[1]
        self.prompts_dir = self.base_dir / "prompts"

    async def run_once(self, user_query: str, history_summary: str | None = None, disable_case_slimming: Optional[bool] = None) -> Dict[str, Any]:
        debug_notes: List[str] = []
        
        # 1. åˆå§‹åŒ– Result çµæ§‹
        result = {
            "l1": {}, "l2": {}, "l3": {}, "l4": {}, 
            "diagnosis": {}, "converged": False, "security_checks": {}
        }
        
        # ==================== L1: é–€ç¦å±¤ (Gate Layer) ====================
        l1_payload = {
            "layer": "L1_GATE",
            "input": {"user_query": user_query, "history_summary": history_summary or ""}
        }
        # é€™è£¡çš„ try-except æ—¨åœ¨æ•ç² LLM é€£ç·šéŒ¯èª¤ï¼Œé‚è¼¯éŒ¯èª¤æ‡‰è©²è®“ä¸» Engine è™•ç†
        l1 = await call_llm_with_prompt(self.llm, self.prompts_dir / "l1_gate_prompt.txt", l1_payload)
        result['l1'] = l1
        
        # ğŸš¨ L1 æª¢æŸ¥é» (é—œéµé»ï¼šå°‡æ‹’çµ•é‚è¼¯è¿”å›çµ¦ main.py è™•ç†)
        if l1.get("status") == "reject" or l1.get("next_action") == "reject":
            logger.warning(f"ğŸ›¡ï¸ L1 é–€ç¦æª¢æ¸¬åˆ°å¨è„…ï¼Œé˜»æ­¢å¾ŒçºŒæ¨ç†ã€‚ç‹€æ…‹: {l1.get('status')}")
            return result # è¿”å›çµ¦ main.py æ‹‹å‡º 422 HTTPException

        # ------------------- æ­£å¸¸æµç¨‹ -------------------
        
        # 2. æª¢ç´¢å±¤ (Retrieval Layer)
        if not self.SE:
            logger.error("âŒ SearchEngine æœªåˆå§‹åŒ–ï¼Œç„¡æ³•é€²è¡Œæª¢ç´¢ã€‚")
            raise RuntimeError("SearchEngine Not Initialized") # <-- L2 ä¹‹å‰çš„ LLM å¤±æ•—è¦–ç‚º 500

        # æ¨¡æ“¬æª¢ç´¢é‚è¼¯ï¼Œå› ç‚ºç¼ºå°‘ SearchEngine å¯¦é«”
        cases = self._simulate_retrieval()

        if not cases:
            debug_notes.append("Retrieval returned zero cases.")
            # å¦‚æœæ²’æœ‰æª¢ç´¢åˆ°ä»»ä½•æ¡ˆä¾‹ï¼Œå¯ä»¥è¿”å›ä¸å®Œæ•´çš„çµæœæˆ–æ‹‹å‡ºéŒ¯èª¤
            result["debug_note"] = "; ".join(debug_notes)
            return result 

        # 3. L2: ç”Ÿæˆå±¤ (Diagnosis Layer)
        l2_result = await self._l2_diagnosis(user_query, cases)
        result['l2'] = l2_result

        # 4. L3: å¯©æ ¸å±¤ (Safety Review Layer)
        l3_result = await self._l3_safety_review(l2_result)
        result['l3'] = l3_result
            
        # ğŸš¨ L3 æª¢æŸ¥é»
        if l3_result.get('status') == 'rejected':
            logger.warning("ğŸ›¡ï¸ L3 å¯©æ ¸æ‹’çµ•è¼¸å‡ºã€‚")
            return result # è¿”å›çµ¦ main.py è™•ç† 422 HTTPException

        # 5. L4: å‘ˆç¾å±¤ (Presentation Layer)
        safe_diagnosis = l3_result.get('safe_diagnosis_payload', {})
        l4_result = await self._l4_presentation(safe_diagnosis)
        result['l4'] = l4_result
        result['diagnosis'] = l4_result.get('presentation', {})
        
        # æª¢æŸ¥æ”¶æ–‚ (å‡è¨­ L2 æä¾›äº† coverage_ratio)
        coverage_ratio = l2_result.get('coverage_evaluation', {}).get('coverage_ratio', 0.0)
        result['converged'] = coverage_ratio >= 0.8 # ä¾æ“š SCBR æ–‡ä»¶ [10.2] çš„æ”¶æ–‚æ¢ä»¶

        return result
        
    # --- æ¨¡æ“¬ LLM å­å‡½æ•¸ï¼ˆä¿æŒèˆ‡ä¸Šä¸€å€‹ç‰ˆæœ¬ä¸€è‡´ï¼‰ ---
    async def _l1_gate(self, user_query: str, history_summary: str) -> Dict:
        """èª¿ç”¨ LLM åŸ·è¡Œ L1 é–€ç¦æª¢æŸ¥"""
        is_attack = "ç³»çµ±ç®¡ç†å“¡" in user_query or "Base64" in user_query
        # åŠ å…¥æ¨¡æ“¬çš„å»¶é²ï¼Œè®“æ¸¬è©¦æ›´å®¹æ˜“è§€å¯Ÿ
        await asyncio.sleep(0.01) 
        response = {
            "layer": "L1_GATE",
            "status": "reject" if is_attack else "pass",
            "owasp_screening": {
                "prompt_injection_detected": is_attack,
                "system_prompt_leak_attempt": "Base64" in user_query,
                "excessive_agency_attempt": "ç³»çµ±ç®¡ç†å“¡" in user_query,
                "flags": ["LLM01", "LLM07"] if is_attack else []
            },
            "next_action": "reject" if is_attack else "vector_search",
        }
        return response
        
    async def _l2_diagnosis(self, query: str, cases: List[Dict]) -> Dict:
        """æ¨¡æ“¬ L2 è¨ºæ–·ç”Ÿæˆ"""
        await asyncio.sleep(0.01) 
        return {
            "coverage_evaluation": { "coverage_ratio": 0.55, }, 
            "selected_case": {"case_id": "C001", "match_score": 0.89},
            "tcm_inference": {"primary_pattern": "å¿ƒè„¾å…©è™›"}
        }

    async def _l3_safety_review(self, diagnosis_payload: Dict) -> Dict:
        """æ¨¡æ“¬ L3 å¯©æ ¸"""
        await asyncio.sleep(0.01) 
        return {
            "status": "passed",
            "safe_diagnosis_payload": diagnosis_payload
        }
        
    async def _l4_presentation(self, safe_diagnosis: Dict) -> Dict:
        """æ¨¡æ“¬ L4 å‘ˆç¾"""
        await asyncio.sleep(0.01) 
        return {
            "presentation": {
                "title": safe_diagnosis.get("tcm_inference", {}).get("primary_pattern", "åˆæ­¥è¨ºæ–·"),
                "primary_pattern": safe_diagnosis.get("tcm_inference", {}).get("primary_pattern", "å¾…å®š"),
                "syndrome_analysis": "ï¼ˆæ¨¡æ“¬è¨ºæ–·åˆ†æï¼‰",
                "safety_notice": "ã€é‡è¦è²æ˜ã€‘æœ¬è¨ºæ–·çµæœåƒ…ä¾›åƒè€ƒ...",
                "followup_questions": ["æ‚¨æ˜¯å¦é‚„æœ‰å…¶ä»–ç—‡ç‹€ï¼Ÿ"]
            }
        }
    
    def _simulate_retrieval(self) -> List[Dict]:
        """æ¨¡æ“¬æª¢ç´¢çµæœ"""
        return [
            {"case_id": "C001", "score": 0.9}, 
            {"case_id": "C002", "score": 0.8},
        ]