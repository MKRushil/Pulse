# -*- coding: utf-8 -*-
"""
å››å±¤æ¨ç†å¼•æ“æ ¸å¿ƒç®¡ç·š (FourLayerSCBR)

è·è²¬ï¼šå”èª¿ L1 (Gate) -> æª¢ç´¢ -> L2 (Diagnosis) -> L3 (Review) -> L4 (Presentation)
çš„æ•¸æ“šæµå’Œé‚è¼¯åˆ¤æ–·ã€‚

æ ¸å¿ƒä¿®å¾©ï¼š
1. ç¢ºä¿ L1 Gate çš„æ‹’çµ•ç‹€æ…‹èƒ½å¤ æ­£ç¢ºè¿”å›çµ¦ main.py é€²è¡Œ 422 è™•ç†ã€‚
2. å°‡ L2, L3, L4 çš„ LLM èª¿ç”¨å¤±æ•—æ”¹ç‚ºæ‹‹å‡ºå—æ§ç•°å¸¸ï¼Œè®“ä¸» Engine è™•ç†ç‚º 500 Internal Server Errorã€‚
3. ğŸš¨ ä¿®æ­£ï¼šå°‡ L1, L3, L4 çš„æ¨¡æ“¬å‡½å¼æ›¿æ›ç‚ºå¯¦éš›çš„ LLM å‘¼å«ï¼Œä¸¦è¨­ç½®æº«åº¦åƒæ•¸ã€‚
"""

from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
import json
import asyncio 
import re # å¼•å…¥ re è™•ç† JSON å®¹éŒ¯

# å‡è¨­å­˜åœ¨é€™äº›æ¨¡çµ„
from ..llm.client import LLMClient
from ..config import SCBRConfig
from ..utils.logger import get_logger
from ..security.owasp_mapper import OWASPMapper 
from ..llm.embedding import EmbedClient
from .search_engine import SearchEngine # å‡è¨­ SearchEngine å­˜åœ¨

logger = get_logger("FourLayerPipeline")

# æ˜¯å¦å•Ÿç”¨æª¢ç´¢çµæœç˜¦èº«ï¼ˆé è¨­åœç”¨ï¼Œèµ° raw å„ªå…ˆï¼‰
USE_RETRIEVAL_SLIMMING = False


def _read_prompt(path: Path) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def _classify_domain(text: str) -> str:
    """æ¥µç°¡é ˜åŸŸåˆ†é¡ï¼šdigestive / gyne / generalã€‚"""
    if not text:
        return "general"
    d_words = ["èƒƒ", "è„˜", "è„¹", "å™¯æ°£", "å—³æ°£", "æ—©é£½", "è„¾èƒƒ", "é£Ÿæ…¾ä¸æŒ¯"]
    g_words = ["å¸¶ä¸‹", "ç™½å¸¶", "é™°é“", "æœˆç¶“", "ç¶“æœŸ", "å©¦ç§‘"]
    for w in d_words:
        if w in text:
            return "digestive"
    for w in g_words:
        if w in text:
            return "gyne"
    return "general"


async def call_llm_with_prompt(llm: LLMClient, prompt_path: Path, payload: Dict[str, Any], temperature: float = 0.0) -> Dict[str, Any]:
    """
    è¼‰å…¥å°æ‡‰ .txt promptï¼Œå½¢æˆ system æŒ‡ç¤º + user payloadï¼Œå‘¼å« LLMã€‚
    ğŸš¨ ä¿®æ­£ï¼šå°‡ temperature ä½œç‚ºé¡å¤–åƒæ•¸å‚³å…¥ï¼Œä½†ä¸ç›´æ¥å‚³éçµ¦ llm.complete_json()
             å› ç‚º complete_json() é æœŸä¸æ¥å—æ­¤åƒæ•¸ (é™¤éå®ƒå…§éƒ¨èª¿ç”¨ chat_complete)ã€‚
    """
    system_prompt = _read_prompt(prompt_path)
    
    # ğŸš¨ ä¿®æ­£é»ï¼šåªå‚³é LLMClient.complete_json æ¥å—çš„åƒæ•¸
    # å‡è¨­ LLMClient.complete_json å…§éƒ¨æœƒè™•ç† temperature/å…¶å®ƒåƒæ•¸ã€‚
    # å¦‚æœ LLMClient.complete_json å…§éƒ¨æ²’æœ‰è™•ç†ï¼Œé€™æœƒæ˜¯ä¸‹ä¸€å€‹å•é¡Œã€‚
    resp = await llm.complete_json(system_prompt=system_prompt, user_prompt=payload) 

    if isinstance(resp, dict):
        return resp
    # ç°¡åŒ– JSON å®¹éŒ¯è™•ç†
    if isinstance(resp, str):
        try:
            return json.loads(resp)
        except Exception:
            import re
            m = re.search(r"\{.*\}", resp, re.DOTALL)
            if not m:
                raise ValueError("LLM éŸ¿æ‡‰ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œä¸”ç„¡æ³•æå– JSON å€å¡Š") 
            return json.loads(m.group(0))
    raise TypeError(f"LLM éŸ¿æ‡‰é¡å‹éŒ¯èª¤: {type(resp)}")


# ç‚ºäº†åœ¨æ—¥èªŒä¸­æå–åˆ†æ•¸ï¼Œå®šç¾© _score_of å‡½å¼
def _score_of(hit: Dict[str, Any]) -> float:
    """å¾æª¢ç´¢çµæœä¸­æå–åˆ†æ•¸ï¼Œå…¼å®¹ _additional å’Œ score çµæ§‹"""
    try:
        add = hit.get("_additional", {}) if isinstance(hit.get("_additional"), dict) else {}
        # å„ªå…ˆä½¿ç”¨ score/distanceï¼Œå…¶æ¬¡ä½¿ç”¨ _final_score (SearchEngine æœƒæ­£è¦åŒ–)
        return float(add.get("score") or hit.get("_final_score") or 0.0)
    except Exception:
        return 0.0


class FourLayerSCBR:
    """å››å±¤é †åºåŸ·è¡Œæ§åˆ¶å™¨ã€‚"""

    def __init__(self, llm: LLMClient, config: Optional[SCBRConfig] = None, search_engine: Optional[SearchEngine] = None, embed_client: Optional[EmbedClient] = None):
        self.llm = llm
        self.cfg = config
        self.SE = search_engine or (SearchEngine(self.cfg) if self.cfg else None)
        self.embed = embed_client or (EmbedClient(self.cfg) if self.cfg else None)
        self.base_dir = Path(__file__).resolve().parents[1]
        self.prompts_dir = self.base_dir / "prompts"

    async def run_once(
        self, 
        user_query: str, 
        history_summary: str | None = None, 
        disable_case_slimming: Optional[bool] = None, 
        round_count: int = 1, 
        max_rounds: int = 7,
        previous_diagnosis: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        debug_notes: List[str] = []
        
        # 1. åˆå§‹åŒ– Result çµæ§‹
        result = {
            "l1": {}, "l2": {}, "l3": {}, "l4": {}, 
            "diagnosis": {}, "converged": False, "security_checks": {}, "is_forced_convergence": False
        }
        
        # ==================== L1: é–€ç¦å±¤ (Gate Layer) ====================
        l1_payload = {
            "layer": "L1_GATE",
            "input": {"user_query": user_query, "history_summary": history_summary or ""}
        }
        # ğŸš¨ L1 å¯¦éš› LLM èª¿ç”¨ (ä½¿ç”¨æº«åº¦ 0.0)
        l1 = await call_llm_with_prompt(self.llm, self.prompts_dir / "l1_gate_prompt.txt", l1_payload, temperature=0.0)
        result['l1'] = l1
        
        logger.info(f"[L1 FINAL RESULT] L1 ç‹€æ…‹: {l1.get('status', 'N/A')}")
        # L1 Schema å®šç¾©äº† keyword_plan
        logger.info(f"[L1 KEYWORD PLAN]\n{json.dumps(l1.get('keyword_plan', {}), indent=2, ensure_ascii=False)}")

        # èˆŠç‰ˆæ—¥èªŒçš„ L1 BEFORE/AFTER FILTER é‚è¼¯ï¼ˆä¿ç•™ï¼‰
        try:
            raw = getattr(self.llm, "_last_raw_output", None)
            flt = getattr(self.llm, "_last_filtered_output", None)
            is_l1 = getattr(self.llm, "_last_is_l1", False)
            if is_l1 and raw and flt:
                def _pp(s: str) -> str:
                    try:
                        return json.dumps(json.loads(s), ensure_ascii=False, indent=2)
                    except Exception:
                        return s
                logger.info("[L1 BEFORE FILTER]\n%s", _pp(raw))
                logger.info("[L1 AFTER  FILTER]\n%s", _pp(flt))
        except Exception:
            pass
        
        # ğŸš¨ L1 æª¢æŸ¥é» (é—œéµé»ï¼šå°‡æ‹’çµ•é‚è¼¯è¿”å›çµ¦ main.py è™•ç†)
        if l1.get("status") == "reject" or l1.get("next_action") == "reject":
            logger.warning(f"ğŸ›¡ï¸ L1 é–€ç¦æª¢æ¸¬åˆ°å¨è„…ï¼Œé˜»æ­¢å¾ŒçºŒæ¨ç†ã€‚ç‹€æ…‹: {l1.get('status')}")
            result['security_checks']['l1_flags'] = l1.get('owasp_screening', {}).get('flags', [])
            return result # è¿”å›çµ¦ main.py æ‹‹å‡º 422 HTTPException

        # ------------------- æ­£å¸¸æµç¨‹ -------------------
        
        # 2. æª¢ç´¢å±¤ (Retrieval Layer)
        cases: List[Dict] = []
        if l1.get("next_action") == "vector_search":
            if not self.SE or not self.embed:
                logger.error("âŒ SearchEngine æˆ– EmbedClient æœªåˆå§‹åŒ–ï¼Œç„¡æ³•é€²è¡Œæª¢ç´¢ã€‚")
                return result 
            
            # å¾ L1 çµæœä¸­æå–é—œéµå­—ï¼ˆBM25 ç”¨ï¼Œä½†æˆ‘å€‘éµå¾ªèˆŠæ—¥èªŒä½¿ç”¨ full_textï¼‰
            text_query = user_query 
            
            # 1. ç²å–æŸ¥è©¢å‘é‡
            try:
                # å¯¦éš›èª¿ç”¨ embed æœå‹™
                vector = await self.embed.embed(text_query) 
            except Exception as e:
                 logger.warning(f"âš ï¸ å‘é‡ç”Ÿæˆå¤±æ•—ï¼Œå˜—è©¦ç´” BM25: {e}")
                 vector = None
                
            # 2. åŸ·è¡Œæ··åˆæª¢ç´¢
            try:
                # ğŸš¨ é—œéµæª¢ç´¢å‘¼å«ï¼šä½¿ç”¨ hybrid_search (åƒè€ƒ v2.3.md Step 5: alpha=0.55, search_fields=["full_text"])
                cases = await self.SE.hybrid_search(
                    index="TCMCase", 
                    text=text_query, 
                    vector=vector, 
                    alpha=self.cfg.search.hybrid_alpha if vector else 1.0, 
                    limit=self.cfg.search.top_k,
                    # å¾ config è®€å–æœç´¢æ¬„ä½
                    search_fields=self.cfg.search.search_fields 
                )
            except Exception as e:
                logger.error(f"âŒ æª¢ç´¢å¤±æ•—: {e}", exc_info=True)
                # æª¢ç´¢å¤±æ•—ï¼Œå°‡è¿”å›ç©ºåˆ—è¡¨ []

        # ğŸš¨ æ—¥èªŒé» 2ï¼šæª¢ç´¢çµæœæ‘˜è¦
        log_samples = []
        if cases:
            log_samples = [
                # ä½¿ç”¨ _score_of å‡½å¼ï¼Œå…¼å®¹ score/final_score
                {"case_id": c.get("case_id", "N/A"), "score": f"{_score_of(c):.4f}"}
                for c in cases[:3] 
            ]
        logger.info(f"[RETRIEVAL RESULT] æˆåŠŸæ‰¾åˆ° {len(cases)} å€‹æ¡ˆä¾‹. Top 3 ç¯„ä¾‹: {log_samples}")

        if not cases:
            debug_notes.append("Retrieval returned zero cases.")
            result["debug_note"] = "; ".join(debug_notes)
            return result 

        # 3. L2: ç”Ÿæˆå±¤ (Diagnosis Layer)
        l2_payload = {
            "layer": "L2_CASE_ANCHORED_DIAGNOSIS",
            "input": {
                "user_accumulated_query": user_query,
                "retrieved_cases": cases,
                "round_count": round_count,
                "previous_diagnosis": previous_diagnosis if previous_diagnosis else {}
            }
        }
        # ğŸš¨ L2 å¯¦éš› LLM èª¿ç”¨ (ä½¿ç”¨æº«åº¦ 0.1)
        l2_result = await call_llm_with_prompt(self.llm, self.prompts_dir / "l2_case_anchored_diagnosis_prompt.txt", l2_payload, temperature=0.1)
        result['l2'] = l2_result

        # ğŸš¨ [æ—¥èªŒé» 3: L2 æ¡ˆä¾‹éŒ¨å®šæ‘˜è¦]
        selected_case_id = l2_result.get("selected_case", {}).get("case_id", "æœªéŒ¨å®š")
        coverage = l2_result.get("coverage_evaluation", {}).get("coverage_ratio", 0.0)
        primary_pattern = l2_result.get('tcm_inference', {}).get('primary_pattern', 'N/A')
        
        logger.info(
            f"[L2 DIAGNOSIS SUMMARY] éŒ¨å®š ID: {selected_case_id}, è­‰å‹: {primary_pattern}, "
            f"è¦†è“‹åº¦: {coverage:.2f}"
        )

        # 4. L3: å¯©æ ¸å±¤ (Safety Review Layer)
        l3_payload = {"layer": "L3_SAFETY_REVIEW", "input": {"diagnosis_payload": l2_result}}
        # ğŸš¨ L3 å¯¦éš› LLM èª¿ç”¨ (ä½¿ç”¨æº«åº¦ 0.0)
        l3_result = await call_llm_with_prompt(self.llm, self.prompts_dir / "l3_safety_review_prompt.txt", l3_payload, temperature=0.0)
        result['l3'] = l3_result
        
        # ğŸš¨ [æ—¥èªŒé» 4: L3 å®‰å…¨å¯©æ ¸çµæœ]
        logger.info(f"[L3 REVIEW STATUS] å¯©æ ¸çµæœ: {l3_result.get('status', 'N/A')}")
            
        # ğŸš¨ L3 æª¢æŸ¥é»
        if l3_result.get('status') == 'rejected':
            logger.warning("ğŸ›¡ï¸ L3 å¯©æ ¸æ‹’çµ•è¼¸å‡ºã€‚")
            result['security_checks']['l3_violations'] = l3_result.get('violations', [])
            return result # è¿”å›çµ¦ main.py è™•ç† 422 HTTPException

        # 5. L4: å‘ˆç¾å±¤ (Presentation Layer)
        safe_diagnosis = l3_result.get('safe_diagnosis_payload', {})
        l4_payload = {
            "layer": "L4_PRESENTATION", 
            "input": {
                "safe_diagnosis_payload": safe_diagnosis, 
                "round_count": round_count, 
                "max_rounds": max_rounds,
                "previous_diagnosis": previous_diagnosis if previous_diagnosis else {}
            }
        }
        # ğŸš¨ L4 å¯¦éš› LLM èª¿ç”¨ (ä½¿ç”¨æº«åº¦ 0.1)
        l4_result = await call_llm_with_prompt(self.llm, self.prompts_dir / "l4_presentation_prompt.txt", l4_payload, temperature=0.1)
        result['l4'] = l4_result
        result['diagnosis'] = l4_result.get('presentation', {})
        
        # æª¢æŸ¥æ”¶æ–‚ (ä¾æ“š SCBR æ–‡ä»¶ [10.2] çš„æ”¶æ–‚æ¢ä»¶)
        coverage_ratio = l2_result.get('coverage_evaluation', {}).get('coverage_ratio', 0.0)
        # ä¿®æ­£æ”¶æ–‚åˆ¤æ–·é‚è¼¯ï¼Œç´å…¥æœ€å¤§è¼ªæ¬¡æª¢æŸ¥ (å¼·åˆ¶æ”¶æ–‚)
        is_coverage_ok = coverage_ratio >= 0.8
        is_max_round_reached = round_count >= max_rounds
        # é€™è£¡çš„é‚è¼¯å¿…é ˆå’Œ main.py å…§éƒ¨çš„ should_converge é‚è¼¯ä¿æŒä¸€è‡´
        result['converged'] = is_coverage_ok or is_max_round_reached 

        # ğŸš¨ [æ–°å¢] æª¢æŸ¥æ˜¯å¦ç‚ºã€Œä½è¦†è“‹åº¦çš„å¼·åˆ¶æ”¶æ–‚ã€ (ç”¨æ–¼ output_validator å¼·åŒ–è­¦å‘Š)
        if is_max_round_reached and coverage_ratio < 0.75:
            result['is_forced_convergence'] = True
            logger.warning(
                f"âš ï¸ è§¸ç™¼ä½è¦†è“‹åº¦å¼·åˆ¶æ”¶æ–‚ (Round: {round_count}, Coverage: {coverage_ratio:.2f})")
        
        return result