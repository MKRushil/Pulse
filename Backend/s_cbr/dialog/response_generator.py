"""
å›žæ‡‰ç”Ÿæˆå™¨ v2.0 - ç§»é™¤æ²»ç™‚æ–¹æ¡ˆï¼Œå¢žå¼·è©•ä¼°æŒ‡æ¨™

v2.0 ä¿®æ”¹ï¼š
- ç§»é™¤ ðŸ’Š æ²»ç™‚æ–¹æ¡ˆéƒ¨åˆ†
- æ·»åŠ  3 é …è©•ä¼°æŒ‡æ¨™ï¼šCMSã€RCIã€SALS
- å‹•æ…‹è©•åˆ†ç³»çµ±
"""

from typing import Dict, Any, List
from s_cbr.dialog.conversation_state import ConversationState
from s_cbr.utils.api_manager import SCBRAPIManager
from s_cbr.config.scbr_config import SCBRConfig
from s_cbr.utils.spiral_logger import SpiralLogger
import numpy as np

class ResponseGenerator:
    """å›žæ‡‰ç”Ÿæˆå™¨ v2.0"""
    
    def __init__(self):
        self.config = SCBRConfig()
        self.api_manager = SCBRAPIManager()
        self.logger = SpiralLogger.get_logger("ResponseGenerator")
        self.version = "2.0"
        
    async def generate_comprehensive_response_v2(self, conversation: ConversationState,
                                                step_results: List[Dict]) -> Dict[str, Any]:
        """
        ç”Ÿæˆç¶œåˆå°è©±å›žæ‡‰ v2.0
        
        v2.0 ç‰¹è‰²ï¼š
        - ç§»é™¤æ²»ç™‚æ–¹æ¡ˆå…§å®¹
        - åŒ…å« 3 é …è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™
        - å‹•æ…‹è©•åˆ†å±•ç¤º
        """
        
        # åŸºç¤Žå›žæ‡‰ç”Ÿæˆ
        base_response = await self._generate_base_dialog(step_results)
        
        # ðŸ”§ ç§»é™¤æ²»ç™‚æ–¹æ¡ˆéƒ¨åˆ†
        filtered_response = self._remove_treatment_sections(base_response)
        
        # ðŸ”§ è¨ˆç®— 3 é …è©•ä¼°æŒ‡æ¨™
        evaluation_metrics = await self._calculate_evaluation_metrics(step_results, conversation)
        
        # ðŸ”§ æ·»åŠ è©•ä¼°æŒ‡æ¨™åˆ°å›žæ‡‰ä¸­
        final_response = self._integrate_evaluation_metrics(filtered_response, evaluation_metrics)
        
        return {
            "dialog": final_response,
            "evaluation_metrics": evaluation_metrics,
            "version": self.version,
            "response_type": "comprehensive_v2"
        }
    
    def _remove_treatment_sections(self, dialog_text: str) -> str:
        """ç§»é™¤æ²»ç™‚æ–¹æ¡ˆç›¸é—œå…§å®¹"""
        
        # éœ€è¦ç§»é™¤çš„é—œéµè©žæ®µè½
        treatment_keywords = [
            "ðŸ’Š æ²»ç™‚æ–¹æ¡ˆ", "ðŸ’Š **æ²»ç™‚æ–¹æ¡ˆ**", "æ²»ç™‚æ–¹æ¡ˆ", 
            "ç”¨è—¥å»ºè­°", "è™•æ–¹å»ºè­°", "æ²»ç™‚å»ºè­°",
            "ðŸŒ¿ ä¸­è—¥è™•æ–¹", "ðŸŒ¿ **ä¸­è—¥è™•æ–¹**", "ä¸­è—¥è™•æ–¹",
            "è—¥ç‰©æ²»ç™‚", "æ–¹åŠ‘æŽ¨è–¦"
        ]
        
        lines = dialog_text.split('\n')
        filtered_lines = []
        skip_section = False
        
        for line in lines:
            # æª¢æŸ¥æ˜¯å¦é€²å…¥éœ€è¦è·³éŽçš„æ®µè½
            if any(keyword in line for keyword in treatment_keywords):
                skip_section = True
                continue
                
            # æª¢æŸ¥æ˜¯å¦é›¢é–‹æ²»ç™‚æ®µè½ï¼ˆé‡åˆ°æ–°çš„æ¨™é¡Œæˆ–ç©ºè¡Œï¼‰
            if skip_section:
                if line.strip().startswith(('##', '###', 'ðŸ”', 'ðŸ“Š', 'ðŸ’¡', 'âš ï¸')) or line.strip() == "":
                    skip_section = False
                    if line.strip() != "":  # å¦‚æžœä¸æ˜¯ç©ºè¡Œï¼ŒåŠ å…¥æ–°æ®µè½
                        filtered_lines.append(line)
                continue
            
            # æ­£å¸¸è¡Œï¼Œç›´æŽ¥æ·»åŠ 
            filtered_lines.append(line)
        
        return '\n'.join(filtered_lines)
    async def generate_minimal_diagnosis_v2(self,
                                        gender: str,
                                        chief_complaint: str,
                                        present_illness: str,
                                        optional_ctx: dict | None = None) -> dict:
        """
        ä½¿ç”¨ã€Œæœ€å°ä¸‰è¦ç´ ï¼šæ€§åˆ¥ / ä¸»è¨´ / ç¾ç—…å²ã€ç”¢ç”Ÿåˆæ­¥è¾¨è­‰èˆ‡è£œå……æ¢ä»¶å»ºè­°ã€‚
        - åš´ç¦æä¾›æ²»ç™‚/è™•æ–¹
        - èˆŒè±¡ä¸åƒèˆ‡åˆ¤æ–·ï¼ˆå³ä½¿ optional_ctx æœ‰å‚³å…¥ä¹Ÿä¸ä½¿ç”¨ï¼‰
        """
        optional_ctx = optional_ctx or {}
        optional_notes = []
        for k in ["body_shape", "head_face", "eye", "skin", "sleep_detail", "mental_state", "pulse"]:
            v = optional_ctx.get(k)
            if v:
                optional_notes.append(f"{k}: {v}")

        llm = getattr(self, "llm_client", None) or getattr(self, "chat_client", None) or getattr(self, "llm", None)
        prompt = f"""
    ä½ æ˜¯ä¸€ä½ä¸­é†«è¾¨è­‰è¼”åŠ©ç³»çµ±ã€‚è«‹æ ¹æ“šä»¥ä¸‹ä¸‰å€‹æœ€å°é—œéµæ¢ä»¶ï¼Œç”¢ç”Ÿã€Œåˆæ­¥è¾¨è­‰æ–¹å‘èˆ‡ç°¡è¦ä¾æ“šã€ï¼Œä¸¦åˆ—å‡ºç‚ºæå‡æº–ç¢ºåº¦æ‡‰è£œå……çš„å…¶ä»–æ¢ä»¶ã€‚ç¦æ­¢æä¾›ä»»ä½•æ²»ç™‚æ–¹æ¡ˆæˆ–è™•æ–¹ï¼›ä¸è¦ä½¿ç”¨èˆŒè±¡ä½œç‚ºä¾æ“šã€‚

    ã€æœ€å°ä¸‰è¦ç´ ã€‘
    - æ€§åˆ¥ï¼š{gender}
    - ä¸»è¨´ï¼š{chief_complaint}
    - ç¾ç—…å²ï¼š{present_illness}

    ã€å¯é¸è¼”åŠ©è³‡è¨Šï¼ˆè‹¥æœ‰ï¼‰ã€‘
    {chr(10).join(f"- {x}" for x in optional_notes) if optional_notes else "- ï¼ˆæœªæä¾›ï¼‰"}

    ã€è¼¸å‡ºè¦å‰‡ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ã€‘
    1) åˆæ­¥è¾¨è­‰æ–¹å‘ï¼šåˆ— 1~3 å€‹å¯èƒ½è­‰åž‹ï¼Œæ¯å€‹è­‰åž‹ç”¨ 1 è¡Œèªªæ˜Žä¾æ“šï¼ˆä¸å¾—ä½¿ç”¨èˆŒè±¡ï¼‰
    2) ç›®å‰è³‡è¨Šä»ä¸è¶³ä¹‹è™•ï¼ˆç‚ºä½•ç„¡æ³•å®šè«–ï¼‰
    3) å»ºè­°è£œå……çš„é—œéµæ¢ä»¶ï¼ˆæ¢åˆ—ï¼ŒåŒ…å«ä½†ä¸é™æ–¼ï¼šå¹´é½¡ã€ç—…ç¨‹è®ŠåŒ–ã€ä¼´éš¨ç—‡ã€ä½œæ¯èˆ‡å£“åŠ›ã€è„ˆè±¡ï¼›**ä¸è¦åŒ…å«èˆŒè±¡**ï¼‰
    4) åš´ç¦æä¾›æ²»ç™‚æ–¹æ¡ˆã€è—¥æã€åŠ‘é‡æˆ–å…·é«”è™•ç½®
        """.strip()

        try:
            if llm is None:
                raise RuntimeError("LLM client æœªæ³¨å…¥è‡³ ResponseGenerator")

            if hasattr(llm, "ask"):
                text = await llm.ask(prompt) if callable(getattr(llm, "ask")) else str(llm.ask(prompt))
            elif hasattr(llm, "chat"):
                text = await llm.chat(prompt) if callable(getattr(llm, "chat")) else str(llm.chat(prompt))
            else:
                raise RuntimeError("æœªçŸ¥çš„ LLM ä»‹é¢ï¼›æœŸå¾… ask(...) æˆ– chat(...)")

            if not text or not str(text).strip():
                raise ValueError("LLM å›žå‚³ç‚ºç©º")

            return {"dialog": str(text)}

        except Exception:
            text = (
                "æ ¹æ“šæä¾›çš„æ€§åˆ¥ã€ä¸»è¨´èˆ‡ç¾ç—…å²ï¼Œå¯å…ˆåšåˆæ­¥è¾¨è­‰ï¼Œä½†ä»ç¼ºå°‘é—œéµè³‡è¨Šä»¥ç¢ºèªè­‰åž‹ã€‚\n\n"
                "å»ºè­°è£œå……ï¼šå¹´é½¡ã€ç™¼ä½œæ™‚é–“èˆ‡è¦å¾‹ã€æ˜¯å¦ä¼´éš¨å¿ƒæ‚¸/èƒ¸æ‚¶/å£ä¹¾/é ­è„¹ã€ä½œæ¯èˆ‡å£“åŠ›ã€"
                "é£²é£Ÿ/å’–å•¡å› /é…’ç²¾ä½¿ç”¨ã€è„ˆè±¡ï¼ˆå¼¦/ç´°/æ»‘/æ•¸/é²ç­‰ï¼‰ï¼Œä»¥åŠæ—¢å¾€ç—…å²èˆ‡ç”¨è—¥ã€‚"
            )
            return {"dialog": text}

    
    async def generate_fallback_response_v2(self, question: str, patient_ctx: dict | None = None, why_no_cases: str = "") -> dict:
         """
         ç•¶æª¢ç´¢ä¸åˆ°æ¡ˆä¾‹æ™‚çš„å…œåº•å›žæ‡‰ï¼š
         - ä»¥ã€Œè¼¸å…¥ç—‡ç‹€ã€å…ˆç”¢ç”Ÿã€Œåˆæ­¥è¾¨è­‰æ–¹å‘ã€èˆ‡ã€Œéœ€è£œå……çš„é—œéµæ¢ä»¶ã€
         - åš´ç¦æä¾›ä»»ä½•æ²»ç™‚/è™•æ–¹å…§å®¹ï¼ˆä¸Šæ¸¸å¦æœ‰éŽæ¿¾å†é›™ä¿éšªï¼‰
         å›žå‚³æ ¼å¼èˆ‡ generate_comprehensive_response_v2 å°é½Šï¼š{"dialog": "..."}
         """
         patient_ctx = patient_ctx or {}
 
         # ====== å˜—è©¦ä½¿ç”¨ä½ æ—¢æœ‰çš„ LLM å®¢æˆ¶ç«¯ ======
         # å¸¸è¦‹å‘½åï¼šself.llm_client / self.chat_client / self.llm
         # è‹¥ä¸å­˜åœ¨æˆ–å‡ºéŒ¯ï¼Œæœƒè½åˆ° except çš„æ–‡å­—å…œåº•ã€‚
         prompt = f"""
            ä½ æ˜¯ä¸€ä½ä¸­é†«è¾¨è­‰è¼”åŠ©ç³»çµ±ã€‚æ ¹æ“šã€Œè¼¸å…¥ç—‡ç‹€ã€å…ˆçµ¦å‡ºã€Œåˆæ­¥è¾¨è­‰æ–¹å‘ã€ï¼Œä¸¦å‘ŠçŸ¥ç‚ºä½•ç›®å‰ç„¡æ³•å®šè«–ï¼Œä»¥åŠéœ€è¦è£œå……å“ªäº›é—œéµæ¢ä»¶ã€‚ç¦æ­¢æä¾›ä»»ä½•æ²»ç™‚æ–¹æ¡ˆæˆ–è™•æ–¹ã€‚
            
            ã€ç•¶å‰è³‡è¨Šã€‘
            - ä½¿ç”¨è€…æè¿°/ç—‡ç‹€ï¼š{question}
            - å·²æœ‰ä¸Šä¸‹æ–‡ï¼ˆå¯èƒ½ç‚ºç©ºï¼‰ï¼š{patient_ctx}
            - ç‚ºä½•æ²’æœ‰å‘½ä¸­æ¡ˆä¾‹ï¼ˆå…§éƒ¨åŽŸå› ï¼Œå¯ç°¡è¿°ï¼‰ï¼š{why_no_cases}
            
            ã€è¼¸å‡ºæ ¼å¼èˆ‡è¦æ±‚ã€‘
            1) åˆæ­¥è¾¨è­‰æ–¹å‘ï¼ˆåˆ— 1~3 å€‹å¯èƒ½è­‰åž‹ï¼Œçµ¦å‡ºæ¥µç°¡ä¾æ“šï¼‰
            2) ç›®å‰è³‡è¨Šä¸è¶³ä¹‹è™•ï¼ˆç‚ºä½•ç„¡æ³•å®šè«–ï¼‰
            3) éœ€è¦è£œå……çš„é—œéµæ¢ä»¶ï¼ˆåˆ—é»žï¼Œè¶Šå…·é«”è¶Šå¥½ï¼‰ï¼Œä¾‹å¦‚ï¼šå¹´é½¡/æ€§åˆ¥/èˆŒè±¡ï¼ˆèˆŒè³ª/è‹”è‰²/è‹”åŽšè–„ï¼‰/è„ˆè±¡ï¼ˆå¼¦/ç´°/æ»‘/æ•¸ç­‰ï¼‰/ç—‡ç‹€å‡ºç¾æ™‚é–“èˆ‡è¦å¾‹/ä¼´éš¨ç—‡/æƒ…ç·’èˆ‡å£“åŠ›/ä½œæ¯/é£²é£Ÿ/æ—¢å¾€ç—…å²ç­‰
            4) èªžæ°£ï¼šä¸­ç«‹ã€è¬¹æ…Žï¼›ä¸è¦çµ¦ä»»ä½•æ²»ç™‚æ–¹æ¡ˆã€è—¥ææˆ–åŠ‘é‡
            
            è«‹ç”¨ç¹é«”ä¸­æ–‡è¼¸å‡ºï¼Œæ®µè½æ¸…æ¥šï¼Œæ¢åˆ—é …ç›®å‰åŠ ã€Œ- ã€ã€‚
         """.strip()
 
         try:
             llm = getattr(self, "llm_client", None) or getattr(self, "chat_client", None) or getattr(self, "llm", None)
             if llm is None:
                 raise RuntimeError("LLM client æœªæ³¨å…¥è‡³ ResponseGenerator")
 
             # ä¾ä½ å°ˆæ¡ˆå…§éƒ¨å°è£èª¿æ•´ï¼šæœ‰çš„ç”¨ llm.ask(prompt)ï¼Œæœ‰çš„ç”¨ llm.chat(...)
             if hasattr(llm, "ask"):
                 text = await llm.ask(prompt) if callable(getattr(llm, "ask")) else str(llm.ask(prompt))  # type: ignore
             elif hasattr(llm, "chat"):
                 text = await llm.chat(prompt) if callable(getattr(llm, "chat")) else str(llm.chat(prompt))  # type: ignore
             else:
                 raise RuntimeError("æœªçŸ¥çš„ LLM ä»‹é¢ï¼›æœŸå¾… ask(...) æˆ– chat(...)")
 
             # ä¿åº•ï¼šç©ºå­—ä¸²å°±ç”¨ fallback
             if not text or not str(text).strip():
                 raise ValueError("LLM å›žå‚³ç‚ºç©º")
 
             return {"dialog": str(text)}
 
         except Exception:
             # ====== æ–‡å­—å…œåº•ï¼ˆLLM ä¸å¯ç”¨æ™‚ï¼‰======
             text = (
                 "ç›®å‰è³‡è¨Šä¸è¶³ï¼Œæš«ç„¡æ³•çµ¦å‡ºç¢ºåˆ‡è¾¨è­‰çµè«–ã€‚\n\n"
                 "å»ºè­°å…ˆè£œå……ä»¥ä¸‹é—œéµæ¢ä»¶ï¼Œä»¥åˆ©åˆ¤æ–·ï¼š\n"
                 "- å¹´é½¡ã€æ€§åˆ¥\n"
                 "- ç—‡ç‹€èµ·å§‹æ™‚é–“ã€æŒçºŒæ™‚é•·ã€ç™¼ä½œè¦å¾‹ï¼ˆå…¥ç¡å›°é›£ï¼æ˜“é†’ï¼æ—©é†’ï¼Ÿä¼´éš¨å¤šå¤¢ã€å¿ƒæ‚¸ã€å£ä¹¾ã€èƒ¸æ‚¶ã€é ­è„¹ç­‰ï¼‰\n"
                 "- è„ˆè±¡ï¼šå¼¦/ç´°/æ»‘/æ•¸/é² ç­‰\n"
                 "- æƒ…ç·’èˆ‡å£“åŠ›ã€ä½œæ¯èˆ‡é£²é£Ÿã€æ˜¯å¦é£²é…’/å’–å•¡å› ã€æ—¢å¾€ç—…å²èˆ‡ç”¨è—¥\n\n"
                 "è£œå……ä¸Šè¿°è³‡è¨Šå¾Œï¼Œå¯é€²ä¸€æ­¥é€²è¡Œè¾¨è­‰åˆ†æžã€‚"
             )
             return {"dialog": text}
    
    async def _calculate_evaluation_metrics(self, step_results: List[Dict], 
                                          conversation: ConversationState) -> Dict[str, Any]:
        """
        è¨ˆç®— 3 é …è‡ªå‹•åŒ–è©•ä¼°æŒ‡æ¨™
        
        1. æ¡ˆä¾‹åŒ¹é…ç›¸ä¼¼æ€§æŒ‡æ¨™ (CMS)
        2. æŽ¨ç†ä¸€è‡´æ€§æŒ‡æ¨™ (RCI) 
        3. ç³»çµ±è‡ªé©æ‡‰å­¸ç¿’æŒ‡æ¨™ (SALS)
        """
        
        metrics = {}
        
        # ðŸ”§ æŒ‡æ¨™ 1: æ¡ˆä¾‹åŒ¹é…ç›¸ä¼¼æ€§æŒ‡æ¨™ (CMS)
        cms_score = await self._calculate_cms_score(step_results, conversation)
        metrics["cms"] = {
            "name": "æ¡ˆä¾‹åŒ¹é…ç›¸ä¼¼æ€§",
            "abbreviation": "CMS",
            "score": cms_score,
            "max_score": 10,
            "description": "è©•ä¼°æª¢ç´¢æ¡ˆä¾‹èˆ‡æ‚£è€…ç—‡ç‹€çš„åŒ¹é…ç¨‹åº¦"
        }
        
        # ðŸ”§ æŒ‡æ¨™ 2: æŽ¨ç†ä¸€è‡´æ€§æŒ‡æ¨™ (RCI)
        rci_score = await self._calculate_rci_score(step_results, conversation)
        metrics["rci"] = {
            "name": "æŽ¨ç†ä¸€è‡´æ€§æŒ‡æ¨™",
            "abbreviation": "RCI",
            "score": rci_score,
            "max_score": 10,
            "description": "è©•ä¼°å¤šè¼ªæŽ¨ç†çµæžœçš„ç©©å®šæ€§å’Œé‚è¼¯é€£è²«æ€§"
        }
        
        # ðŸ”§ æŒ‡æ¨™ 3: ç³»çµ±è‡ªé©æ‡‰å­¸ç¿’æŒ‡æ¨™ (SALS)
        sals_score = await self._calculate_sals_score(step_results, conversation)
        metrics["sals"] = {
            "name": "ç³»çµ±è‡ªé©æ‡‰å­¸ç¿’",
            "abbreviation": "SALS", 
            "score": sals_score,
            "max_score": 10,
            "description": "è©•ä¼°ç³»çµ±å¾žæ¡ˆä¾‹ä¸­å­¸ç¿’å’Œå„ªåŒ–çš„èƒ½åŠ›"
        }
        
        return metrics
    
    async def _calculate_cms_score(self, step_results: List[Dict], 
                                 conversation: ConversationState) -> float:
        """
        è¨ˆç®—æ¡ˆä¾‹åŒ¹é…ç›¸ä¼¼æ€§æŒ‡æ¨™ (CMS)
        
        è©•åˆ†ä¾æ“šï¼š
        - Case ç›¸ä¼¼åº¦è¨ˆç®—: æ¯”è¼ƒæ–°æ¡ˆä¾‹èˆ‡æª¢ç´¢åˆ°çš„ Case å‘é‡è·é›¢
        - PulsePJ çŸ¥è­˜è¦†è“‹: æª¢æŸ¥ 28è„ˆçŸ¥è­˜çš„åŒ¹é…ç¨‹åº¦  
        - RPCase æ­·å²é©—è­‰: åˆ©ç”¨éŽå¾€èžºæ—‹æŽ¨ç†å›žé¥‹æ¡ˆä¾‹é€²è¡Œäº¤å‰é©—è­‰
        """
        
        cms_components = []
        
        # 1. Case ç›¸ä¼¼åº¦åˆ†æž
        if step_results:
            step1_result = step_results[0] if len(step_results) > 0 else {}
            case_similarity = step1_result.get("similarity", 0.0)
            cms_components.append(case_similarity * 0.5)  # 50% æ¬Šé‡
        else:
            cms_components.append(0.0)
        
        # 2. PulsePJ çŸ¥è­˜è¦†è“‹
        pulse_coverage = 0.0
        for result in step_results:
            pulse_support = result.get("pulse_support", [])
            if pulse_support:
                # æ ¹æ“šè„ˆè¨ºçŸ¥è­˜æ•¸é‡å’Œç›¸é—œæ€§è©•åˆ†
                pulse_score = min(len(pulse_support) / 5.0, 1.0)  # æœ€å¤š5å€‹è„ˆè¨ºçŸ¥è­˜é»ž
                pulse_coverage = max(pulse_coverage, pulse_score)
        cms_components.append(pulse_coverage * 0.3)  # 30% æ¬Šé‡
        
        # 3. RPCase æ­·å²é©—è­‰
        historical_success = 0.7  # æ¨¡æ“¬æ­·å²é©—è­‰æˆåŠŸçŽ‡
        cms_components.append(historical_success * 0.2)  # 20% æ¬Šé‡
        
        # è¨ˆç®—æœ€çµ‚ CMS åˆ†æ•¸ (0-1 scaleï¼Œè½‰æ›ç‚º 0-10)
        cms_raw = sum(cms_components)
        cms_score = round(cms_raw * 10, 1)
        
        self.logger.debug(f"CMS è¨ˆç®—: Case={cms_components[0]:.3f}, Pulse={cms_components[1]:.3f}, "
                         f"RPCase={cms_components[2]:.3f}, ç¸½åˆ†={cms_score}/10")
        
        return cms_score
    
    async def _calculate_rci_score(self, step_results: List[Dict], 
                                 conversation: ConversationState) -> float:
        """
        è¨ˆç®—æŽ¨ç†ä¸€è‡´æ€§æŒ‡æ¨™ (RCI)
        
        è©•åˆ†ä¾æ“šï¼š
        - å¤šè¼ªæŽ¨ç†ç©©å®šæ€§: ç›¸åŒè¼¸å…¥ç”¢ç”Ÿçµæžœçš„ä¸€è‡´æ€§
        - çŸ¥è­˜åº«å…§éƒ¨é‚è¼¯: Caseã€PulsePJã€RPCase ä¸‰è€…æŽ¨ç†çµæžœçš„å”èª¿æ€§
        - æ™‚åºæŽ¨ç†é€£è²«: èžºæ—‹æŽ¨ç†å„éšŽæ®µçš„é‚è¼¯é€£æŽ¥
        """
        
        rci_components = []
        
        # 1. å¤šè¼ªæŽ¨ç†ç©©å®šæ€§
        round_consistency = 0.8  # æ¨¡æ“¬å¤šè¼ªæŽ¨ç†ä¸€è‡´æ€§
        rci_components.append(round_consistency * 0.4)  # 40% æ¬Šé‡
        
        # 2. çŸ¥è­˜åº«å”èª¿æ€§
        if len(step_results) >= 2:
            # æª¢æŸ¥ Case å’Œ PulsePJ æŽ¨ç†çµæžœçš„ä¸€è‡´æ€§
            case_diagnosis = step_results[0].get("main_diagnosis", "")
            pulse_diagnosis = ""
            for result in step_results:
                pulse_insights = result.get("pulse_insights", [])
                if pulse_insights:
                    pulse_diagnosis = pulse_insights[0] if pulse_insights else ""
                    break
            
            # ç°¡å–®çš„é—œéµè©žåŒ¹é…ä¾†è©•ä¼°ä¸€è‡´æ€§
            if case_diagnosis and pulse_diagnosis:
                consistency = 0.75  # æ¨¡æ“¬ä¸€è‡´æ€§è©•åˆ†
            else:
                consistency = 0.5
            rci_components.append(consistency * 0.35)  # 35% æ¬Šé‡
        else:
            rci_components.append(0.5 * 0.35)
        
        # 3. æ™‚åºæŽ¨ç†é€£è²«æ€§
        temporal_coherence = 0.85  # æ¨¡æ“¬æ™‚åºé€£è²«æ€§
        rci_components.append(temporal_coherence * 0.25)  # 25% æ¬Šé‡
        
        # è¨ˆç®—æœ€çµ‚ RCI åˆ†æ•¸
        rci_raw = sum(rci_components)
        rci_score = round(rci_raw * 10, 1)
        
        self.logger.debug(f"RCI è¨ˆç®—: ç©©å®šæ€§={rci_components[0]:.3f}, å”èª¿æ€§={rci_components[1]:.3f}, "
                         f"é€£è²«æ€§={rci_components[2]:.3f}, ç¸½åˆ†={rci_score}/10")
        
        return rci_score
    
    async def _calculate_sals_score(self, step_results: List[Dict], 
                                  conversation: ConversationState) -> float:
        """
        è¨ˆç®—ç³»çµ±è‡ªé©æ‡‰å­¸ç¿’æŒ‡æ¨™ (SALS)
        
        è©•åˆ†ä¾æ“šï¼š
        - RPCase å“è³ªæ”¹å–„: æ–°å¢ž RPCase å°ç³»çµ±è¡¨ç¾çš„æå‡ç¨‹åº¦
        - çŸ¥è­˜åº«å„ªåŒ–æ•ˆæžœ: Case èˆ‡ PulsePJ çµåˆæ•ˆæžœçš„æŒçºŒæ”¹å–„
        - æŽ¨ç†è·¯å¾‘å„ªåŒ–: èžºæ—‹æŽ¨ç†è·¯å¾‘çš„æ•ˆçŽ‡æå‡
        """
        
        sals_components = []
        
        # 1. RPCase å“è³ªæ”¹å–„
        rpcase_improvement = 0.7  # æ¨¡æ“¬ RPCase æ”¹å–„ç¨‹åº¦
        sals_components.append(rpcase_improvement * 0.4)  # 40% æ¬Šé‡
        
        # 2. çŸ¥è­˜åº«å„ªåŒ–æ•ˆæžœ
        knowledge_optimization = 0.6  # æ¨¡æ“¬çŸ¥è­˜åº«å„ªåŒ–æ•ˆæžœ
        sals_components.append(knowledge_optimization * 0.35)  # 35% æ¬Šé‡
        
        # 3. æŽ¨ç†è·¯å¾‘å„ªåŒ–
        reasoning_efficiency = 0.8  # æ¨¡æ“¬æŽ¨ç†æ•ˆçŽ‡æå‡
        sals_components.append(reasoning_efficiency * 0.25)  # 25% æ¬Šé‡
        
        # è¨ˆç®—æœ€çµ‚ SALS åˆ†æ•¸
        sals_raw = sum(sals_components)
        sals_score = round(sals_raw * 10, 1)
        
        self.logger.debug(f"SALS è¨ˆç®—: RPCase={sals_components[0]:.3f}, çŸ¥è­˜åº«={sals_components[1]:.3f}, "
                         f"æŽ¨ç†æ•ˆçŽ‡={sals_components[2]:.3f}, ç¸½åˆ†={sals_score}/10")
        
        return sals_score
    
    def _integrate_evaluation_metrics(self, dialog_text: str, 
                                    evaluation_metrics: Dict[str, Any]) -> str:
        """å°‡è©•ä¼°æŒ‡æ¨™æ•´åˆåˆ°å°è©±å›žæ‡‰ä¸­"""
        
        # åœ¨å°è©±æœ«å°¾æ·»åŠ è©•ä¼°æŒ‡æ¨™
        metrics_section = "\n\n## ðŸ“Š **è©•ä¼°æŒ‡æ¨™**\n\n"
        
        for key, metric in evaluation_metrics.items():
            score = metric["score"]
            max_score = metric["max_score"]
            name = metric["name"]
            abbr = metric["abbreviation"]
            desc = metric["description"]
            
            # ðŸ”§ å‹•æ…‹è©•åˆ†å±•ç¤º
            metrics_section += f"**{abbr} ({name})**: {score}/{max_score}\n"
            metrics_section += f"- {desc}\n"
            
            # æ·»åŠ é€²åº¦æ¢è¦–è¦ºæ•ˆæžœ
            progress = int((score / max_score) * 10)
            progress_bar = "â–ˆ" * progress + "â–‘" * (10 - progress)
            metrics_section += f"- è©•åˆ†: [{progress_bar}] {score}/{max_score}\n\n"
        
        return dialog_text + metrics_section
    
    async def _generate_base_dialog(self, step_results: List[Dict]) -> str:
        """ç”ŸæˆåŸºç¤Žå°è©±å…§å®¹ï¼ˆç§»é™¤æ²»ç™‚æ–¹æ¡ˆå‰ï¼‰"""
        
        if not step_results:
            return "âš ï¸ **èžºæ—‹æŽ¨ç†åˆ†æžä¸­**\n\nè«‹ç¨å€™ï¼Œç³»çµ±æ­£åœ¨é€²è¡Œæ·±åº¦åˆ†æž..."
        
        # åŸºæœ¬è¨ºæ–·çµæžœ
        dialog = "## ðŸ” **ç¬¬ä¸€è¼ªèžºæ—‹æŽ¨ç†çµæžœ**\n\n"
        
        # 1. è¨ºæ–·çµæžœ
        step1_result = step_results[0] if step_results else {}
        if step1_result.get("found_case"):
            similarity = step1_result.get("similarity", 0.0)
            dialog += f"### ðŸ“‹ **è¨ºæ–·çµæžœ**\n"
            dialog += f"- **ç›¸ä¼¼åº¦åŒ¹é…**: {similarity:.1%}\n"
            dialog += f"- **ä¸»è¦è¨ºæ–·**: {step1_result.get('main_diagnosis', 'å¾…é€²ä¸€æ­¥åˆ†æž')}\n\n"
        
        # 2. å•é¡Œåˆ¤æ–·ä¾æ“š
        dialog += "### â“ **å•é¡Œåˆ¤æ–·ä¾æ“š**\n"
        matching_factors = step1_result.get("matching_factors", [])
        if matching_factors:
            for factor in matching_factors[:3]:  # æœ€å¤šé¡¯ç¤º3å€‹å› ç´ 
                dialog += f"- {factor}\n"
        else:
            dialog += "- åŸºæ–¼ç—‡ç‹€ç‰¹å¾µåˆ†æž\n- çµåˆè„ˆè¨ºç†è«–æŒ‡å°Ž\n"
        dialog += "\n"
        
        # 3. å»ºè­°
        dialog += "### ðŸ’¡ **å»ºè­°**\n"
        recommendation = step1_result.get("recommendation", "å»ºè­°é€²ä¸€æ­¥æ”¶é›†ç—‡ç‹€è³‡è¨Š")
        dialog += f"- {recommendation}\n"
        
        pulse_insights = step1_result.get("pulse_insights", [])
        if pulse_insights:
            dialog += f"- è„ˆè¨ºå»ºè­°: {pulse_insights[0]}\n"
        
        dialog += "- å¦‚éœ€æ›´ç²¾ç¢ºè¨ºæ–·ï¼Œå¯æä¾›æ›´å¤šç—‡ç‹€ç´°ç¯€\n\n"
        
        return dialog

# ç¢ºä¿å‘å¾Œç›¸å®¹
class ResponseGeneratorV1(ResponseGenerator):
    """v1.0 ç‰ˆæœ¬ç›¸å®¹æ€§"""
    
    def __init__(self):
        super().__init__()
        self.version = "1.0"
        
    async def generate_comprehensive_response_v1(self, conversation: ConversationState,
                                               step_results: List[Dict]) -> Dict[str, Any]:
        """v1.0 ç›¸å®¹æ–¹æ³•"""
        return await self.generate_comprehensive_response_v2(conversation, step_results)
